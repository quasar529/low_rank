{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/bumjun/qlora/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import transformers\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BertTokenizer,\n",
    "    BertForSequenceClassification,\n",
    "    RobertaTokenizer,\n",
    "    RobertaForSequenceClassification,\n",
    "    EvalPrediction,\n",
    "    PretrainedConfig,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "import wandb\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from tqdm import tqdm, trange\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "# import loralib as lora\n",
    "from peft import (\n",
    "    get_peft_config,\n",
    "    get_peft_model,\n",
    "    get_peft_model_state_dict,\n",
    "    set_peft_model_state_dict,\n",
    "    LoraConfig,\n",
    "    PeftType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptEncoderConfig,\n",
    "    prepare_model_for_kbit_training,\n",
    ")\n",
    "from collections import Counter\n",
    "import glob\n",
    "import time\n",
    "import datasets\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = False  # type: ignore\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_error(original_weight, approx_weight):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    return torch.linalg.norm(original_weight.to(device) - approx_weight.to(device), \"fro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['classifier.bias', 'pooler.dense.bias', 'pooler.dense.weight', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1182722 || all params: 1568096260 || trainable%: 0.075424068672927\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/deberta-v2-xxlarge\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "config = LoraConfig(r=4, lora_alpha=8, target_modules=[\"query_proj\", \"value_proj\"], task_type=\"SEQ_CLS\")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleDict(\n",
       "  (default): Linear(in_features=1536, out_features=4, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_A = model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0037, -0.0002, -0.0136, -0.0114],\n",
       "         [-0.0118, -0.0211, -0.0214,  0.0007],\n",
       "         [-0.0003, -0.0050, -0.0224,  0.0027],\n",
       "         ...,\n",
       "         [-0.0077, -0.0008, -0.0219, -0.0033],\n",
       "         [-0.0131, -0.0245, -0.0117,  0.0163],\n",
       "         [-0.0070, -0.0254, -0.0136,  0.0109]], grad_fn=<TransposeBackward0>),\n",
       " torch.Size([1536, 4]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_A = model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A.default.weight.transpose(0,1)\n",
    "lora_A, lora_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1551, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_A_norm = torch.norm(lora_A)\n",
    "lora_A_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Parameter containing:\n",
       " tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]], requires_grad=True),\n",
       " torch.Size([1536, 4]),\n",
       " tensor(0., grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_B = model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_B.default\n",
    "lora_B.weight, lora_B.weight.shape, torch.norm(lora_B.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### query SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0712,  0.0211, -0.0921,  ..., -0.0111,  0.0479,  0.0247],\n",
       "         [-0.0326,  0.0605, -0.1219,  ...,  0.0445, -0.0812, -0.0392],\n",
       "         [ 0.0090,  0.0134,  0.0068,  ...,  0.0231, -0.0630,  0.0749],\n",
       "         ...,\n",
       "         [ 0.0343,  0.0161, -0.0853,  ..., -0.0633, -0.0336,  0.0295],\n",
       "         [ 0.1448, -0.0611, -0.0674,  ..., -0.0059,  0.0809, -0.0400],\n",
       "         [-0.0075,  0.0023, -0.0746,  ..., -0.0186, -0.0690, -0.0521]]),\n",
       " torch.Size([1536, 1536]),\n",
       " tensor(88.6837))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_original_weight = model.deberta.encoder.layer[0].attention.self.query_proj.weight.data.T\n",
    "q_original_weight, q_original_weight.shape, torch.norm(q_original_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.2236, -0.0308,  0.0417,  0.0626],\n",
       "         [ 0.1297, -0.1951,  0.0092,  0.0092],\n",
       "         [ 0.0193,  0.0311, -0.0281, -0.0416],\n",
       "         ...,\n",
       "         [-0.0423,  0.0485, -0.1052, -0.0067],\n",
       "         [ 0.0470, -0.0709, -0.0392,  0.0321],\n",
       "         [-0.0780,  0.0835, -0.0392, -0.0022]]),\n",
       " torch.Size([1536, 4]),\n",
       " tensor(5.8884))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_u, q_proj_s, q_proj_v = torch.linalg.svd(q_original_weight)\n",
    "new_lora_A = q_proj_u[:, :4] @ torch.diag(q_proj_s[:4]).sqrt()\n",
    "new_lora_A, new_lora_A.shape, torch.norm(new_lora_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, device='cuda:0')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_reconstructed = q_proj_u @ torch.diag(q_proj_s) @ q_proj_v\n",
    "recon_error(q_original_weight, q_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(86.9137, device='cuda:0')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_reconstructed = q_proj_u[:, :4] @ torch.diag(q_proj_s[:4]) @ q_proj_v[:4, :]\n",
    "recon_error(q_original_weight, q_reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0683, -0.0099,  0.0151,  0.0244],\n",
       "        [ 0.0396, -0.0627,  0.0033,  0.0036],\n",
       "        [ 0.0059,  0.0100, -0.0102, -0.0162],\n",
       "        ...,\n",
       "        [-0.0129,  0.0156, -0.0380, -0.0026],\n",
       "        [ 0.0144, -0.0228, -0.0142,  0.0125],\n",
       "        [-0.0238,  0.0268, -0.0142, -0.0009]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_u[:, :4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536, 4])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_u[:, :4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.7285,  9.6862,  7.6498,  6.6093])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_proj_s[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[10.7285,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  9.6862,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  7.6498,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  6.6093]]),\n",
       " torch.Size([4, 4]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(q_proj_s[:4]), torch.diag(q_proj_s[:4]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.2236,  0.1297,  0.0193,  ..., -0.0423,  0.0470, -0.0780]),\n",
       " torch.Size([1536]),\n",
       " tensor(3.2754))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lora_A[:,0],new_lora_A[:,0].shape, torch.norm(new_lora_A[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0037, -0.0118, -0.0003,  ..., -0.0077, -0.0131, -0.0070],\n",
       "        grad_fn=<SelectBackward0>),\n",
       " torch.Size([1536]),\n",
       " tensor(0.5674, grad_fn=<LinalgVectorNormBackward0>))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_A[:, 0], lora_A[:, 0].shape, torch.norm(lora_A[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th col torch.Size([1536]) tensor(0.5674, grad_fn=<LinalgVectorNormBackward0>)\n",
      "1th col torch.Size([1536]) tensor(0.5873, grad_fn=<LinalgVectorNormBackward0>)\n",
      "2th col torch.Size([1536]) tensor(0.5792, grad_fn=<LinalgVectorNormBackward0>)\n",
      "3th col torch.Size([1536]) tensor(0.5762, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    lora_A = model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A.default.weight.T\n",
    "    print(f\"{i}th col\",lora_A[:, i].shape, torch.norm(lora_A[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th col new_lora_A torch.Size([1536]) tensor(3.2754)\n",
      "1th col new_lora_A torch.Size([1536]) tensor(3.1123)\n",
      "2th col new_lora_A torch.Size([1536]) tensor(2.7658)\n",
      "3th col new_lora_A torch.Size([1536]) tensor(2.5709)\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    q_proj_u, q_proj_s, q_proj_v = torch.linalg.svd(q_original_weight)\n",
    "    new_lora_A = q_proj_u[:, :4] @ torch.diag(q_proj_s[:4]).sqrt()\n",
    "    print(f\"{i}th col new_lora_A\", new_lora_A[:, i].shape, torch.norm(new_lora_A[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "x = [0,1,2,3,4]\n",
    "for i in range(5):\n",
    "    new_x = 5+i\n",
    "    x[i]= new_x\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_of_layers = len(model.deberta.encoder.layer)\n",
    "q_new_lora_A_list = []\n",
    "v_new_lora_A_list = []\n",
    "\n",
    "q_new_lora_B_list = []\n",
    "v_new_lora_B_list = []\n",
    "\n",
    "approx_rank = 4\n",
    "for layer_idx in range(len_of_layers):\n",
    "    q_original_weight = model.deberta.encoder.layer[layer_idx].attention.self.query_proj.weight.data.T\n",
    "    v_original_weight = model.deberta.encoder.layer[layer_idx].attention.self.value_proj.weight.data.T\n",
    "\n",
    "    q_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.query_proj.lora_A.default.weight.T\n",
    "    v_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.value_proj.lora_A.default.weight.T\n",
    "\n",
    "    q_proj_u, q_proj_s, q_proj_v = torch.linalg.svd(q_original_weight)\n",
    "    q_new_lora_A = q_proj_u[:, :approx_rank] @ torch.diag(q_proj_s[:approx_rank]).sqrt()\n",
    "    q_new_lora_B = torch.diag(q_proj_s[:approx_rank]).sqrt() @ q_proj_v[:approx_rank, :]\n",
    "    \n",
    "    v_proj_u, v_proj_s, v_proj_v = torch.linalg.svd(v_original_weight)\n",
    "    v_new_lora_A = v_proj_u[:, :approx_rank] @ torch.diag(v_proj_s[:approx_rank]).sqrt()\n",
    "    v_new_lora_B = torch.diag(v_proj_s[:approx_rank]).sqrt() @ v_proj_v[:approx_rank, :]\n",
    "    \n",
    "    q_new_lora_B_list.append(q_new_lora_B)\n",
    "    v_new_lora_B_list.append(v_new_lora_B)\n",
    "\n",
    "    for i in range(4):\n",
    "        print(f\"Before Scale, q {i}th col Norm\", torch.norm(q_new_lora_A[:, i]))\n",
    "\n",
    "        q_og_lora_A_icol_norm = torch.norm(q_og_lora_A[:, i])\n",
    "        q_new_lora_A_icol_norm = torch.norm(q_new_lora_A[:, i])\n",
    "\n",
    "        q_scale = q_og_lora_A_icol_norm / q_new_lora_A_icol_norm\n",
    "\n",
    "        q_new_lora_A[:, i] = q_new_lora_A[:, i] * q_scale\n",
    "\n",
    "        print(f\"After Scale, q {i}th col Norm\", torch.norm(q_new_lora_A[:, i]))\n",
    "        print(\"####################\")\n",
    "\n",
    "        print(f\"Before Scale, v {i}th col Norm\", torch.norm(v_new_lora_A[:, i]))\n",
    "\n",
    "        v_og_lora_A_icol_norm = torch.norm(v_og_lora_A[:, i])\n",
    "        v_new_lora_A_icol_norm = torch.norm(v_new_lora_A[:, i])\n",
    "\n",
    "        v_scale = v_og_lora_A_icol_norm / v_new_lora_A_icol_norm\n",
    "\n",
    "        v_new_lora_A[:, i] = v_new_lora_A[:, i] * v_scale\n",
    "\n",
    "        print(f\"After Scale, v {i}th col Norm\", torch.norm(v_new_lora_A[:, i]))\n",
    "        print(\"####################\")\n",
    "\n",
    "    q_new_lora_A_list.append(q_new_lora_A)\n",
    "    v_new_lora_A_list.append(v_new_lora_A)\n",
    "\n",
    "    model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.lora_A.default.weight.data = (\n",
    "        q_new_lora_A_list[layer_idx].transpose(0, 1).contiguous()\n",
    "    )\n",
    "    model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.lora_A.default.weight.data = (\n",
    "        v_new_lora_A_list[layer_idx].transpose(0, 1).contiguous()\n",
    "    )\n",
    "    \n",
    "    model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.lora_B.default.weight.data = (\n",
    "        q_new_lora_B_list[layer_idx].transpose(0, 1).contiguous()\n",
    "    )\n",
    "    model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.lora_B.default.weight.data = (\n",
    "        v_new_lora_B_list[layer_idx].transpose(0, 1).contiguous()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Scale, q 0th col Norm tensor(3.2754)\n",
      "After Scale, q 0th col Norm tensor(0.5823, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0830)\n",
      "After Scale, v 0th col Norm tensor(0.5780, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(3.1123, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5741, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9221, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5697, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.7658, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5851, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.9160, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5672, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.5709, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5755, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8722, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5772, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.7274)\n",
      "After Scale, q 0th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0011)\n",
      "After Scale, v 0th col Norm tensor(0.5817, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.3932, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5884, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9277, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5723, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3170, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5767, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8841, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5945, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2348, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5835, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8331, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5784, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9403)\n",
      "After Scale, q 0th col Norm tensor(0.5832, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0408)\n",
      "After Scale, v 0th col Norm tensor(0.5703, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4288, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5786, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(2.0072, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5734, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3038, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5741, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.9679, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5844, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2439, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5794, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.9358, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5717, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0741)\n",
      "After Scale, q 0th col Norm tensor(0.5732, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0850)\n",
      "After Scale, v 0th col Norm tensor(0.5960, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4699, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5830, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(2.0752, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5847, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3309, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5682, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(2.0130, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5750, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3086, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5776, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(2.0107, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5696, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.8918)\n",
      "After Scale, q 0th col Norm tensor(0.5765, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9788)\n",
      "After Scale, v 0th col Norm tensor(0.5792, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5720, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9444, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5749, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3424, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5731, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.9122, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5749, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3075, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5948, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.9023, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0407)\n",
      "After Scale, q 0th col Norm tensor(0.5750, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0163)\n",
      "After Scale, v 0th col Norm tensor(0.5790, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5450, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5775, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(2.0037, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5806, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3736, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5830, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.9756, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5725, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3111, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5994, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.9667, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5713, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9064)\n",
      "After Scale, q 0th col Norm tensor(0.5797, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9741)\n",
      "After Scale, v 0th col Norm tensor(0.5738, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5365, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5802, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9338, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5720, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4089, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5788, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8988, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5817, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2958, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5764, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8905, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5680, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1607)\n",
      "After Scale, q 0th col Norm tensor(0.5868, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9488)\n",
      "After Scale, v 0th col Norm tensor(0.5660, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5938, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5735, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5660, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3900, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5614, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8861, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5843, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3430, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5780, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8741, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5700, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0960)\n",
      "After Scale, q 0th col Norm tensor(0.5784, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8925)\n",
      "After Scale, v 0th col Norm tensor(0.5789, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5855, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5806, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8453, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5646, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4220, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5770, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8297, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5892, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3458, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5706, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8157, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5748, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0624)\n",
      "After Scale, q 0th col Norm tensor(0.5814, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8364)\n",
      "After Scale, v 0th col Norm tensor(0.5702, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5112, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5666, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7825, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5757, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3653, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5796, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7542, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5722, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3556, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5827, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7466, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5730, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0136)\n",
      "After Scale, q 0th col Norm tensor(0.5661, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7608)\n",
      "After Scale, v 0th col Norm tensor(0.5819, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.3976, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5731, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7192, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5705, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3635, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5761, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7014, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5647, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2662, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5775, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6975, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5789, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9946)\n",
      "After Scale, q 0th col Norm tensor(0.5697, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.6533)\n",
      "After Scale, v 0th col Norm tensor(0.5803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8534, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5749, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.6288, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5710, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4948, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5748, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6197, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5823, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.4425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5818, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6095, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5733, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0471)\n",
      "After Scale, q 0th col Norm tensor(0.5765, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0520)\n",
      "After Scale, v 0th col Norm tensor(0.5843, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8709, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7964, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5748, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.6469, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5854, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6738, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5698, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3719, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5675, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6186, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5782, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.6912)\n",
      "After Scale, q 0th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7994)\n",
      "After Scale, v 0th col Norm tensor(0.5722, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.2055, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5811, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7123, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5857, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.1798, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5673, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6797, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5672, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.0779, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5718, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6344, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5752, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0725)\n",
      "After Scale, q 0th col Norm tensor(0.5592, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9530)\n",
      "After Scale, v 0th col Norm tensor(0.5707, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4894, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5822, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8648, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5776, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3708, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5833, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5843, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2621, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5740, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8202, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5839, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1196)\n",
      "After Scale, q 0th col Norm tensor(0.5872, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9776)\n",
      "After Scale, v 0th col Norm tensor(0.5674, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5201, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5823, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9533, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5666, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3593, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5783, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.9400, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5842, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3367, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5703, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.9073, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5827, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9832)\n",
      "After Scale, q 0th col Norm tensor(0.5770, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9328)\n",
      "After Scale, v 0th col Norm tensor(0.5829, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5012, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5775, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8785, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5769, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3815, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5782, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5790, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3218, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5745, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7690, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5722, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0535)\n",
      "After Scale, q 0th col Norm tensor(0.5809, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9506)\n",
      "After Scale, v 0th col Norm tensor(0.5690, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5685, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5845, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9325, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5723, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4341, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5777, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8835, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5818, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3260, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5809, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8638, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5834, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.8972)\n",
      "After Scale, q 0th col Norm tensor(0.5768, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9289)\n",
      "After Scale, v 0th col Norm tensor(0.5681, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5271, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5721, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8970, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5770, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4338, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5645, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8773, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5783, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2976, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8111, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5814, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1522)\n",
      "After Scale, q 0th col Norm tensor(0.5669, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8965)\n",
      "After Scale, v 0th col Norm tensor(0.5800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5873, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5668, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8814, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5798, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4169, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8267, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5759, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5775, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8114, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5828, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0936)\n",
      "After Scale, q 0th col Norm tensor(0.5714, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8215)\n",
      "After Scale, v 0th col Norm tensor(0.5638, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5840, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7916, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5833, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4425, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5700, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7815, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5831, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3598, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5734, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7597, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5774, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0776)\n",
      "After Scale, q 0th col Norm tensor(0.5738, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7862)\n",
      "After Scale, v 0th col Norm tensor(0.5743, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5212, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5837, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7477, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5730, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4035, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5751, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7289, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5662, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3628, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5677, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7231, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5866, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0492)\n",
      "After Scale, q 0th col Norm tensor(0.5706, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7733)\n",
      "After Scale, v 0th col Norm tensor(0.5814, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4176, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5887, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7344, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3496, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5860, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7015, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5771, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2671, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5805, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6931, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5846, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9450)\n",
      "After Scale, q 0th col Norm tensor(0.5782, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.6534)\n",
      "After Scale, v 0th col Norm tensor(0.5857, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8392, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5692, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.6306, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5759, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.5007, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5704, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6205, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5714, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.4438, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5821, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6131, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5661, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0412)\n",
      "After Scale, q 0th col Norm tensor(0.5756, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0354)\n",
      "After Scale, v 0th col Norm tensor(0.5732, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8576, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5792, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7962, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5754, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.6406, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5754, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6704, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5848, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3796, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5797, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6199, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5773, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.6832)\n",
      "After Scale, q 0th col Norm tensor(0.5768, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8011)\n",
      "After Scale, v 0th col Norm tensor(0.5791, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.2135, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5901, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.6954, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5864, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.1647, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6724, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5889, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.0750, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5733, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6135, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5839, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0625)\n",
      "After Scale, q 0th col Norm tensor(0.5722, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9794)\n",
      "After Scale, v 0th col Norm tensor(0.5783, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4885, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5681, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8532, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5643, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3805, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5868, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8356, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5735, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2740, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5719, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8054, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5789, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1114)\n",
      "After Scale, q 0th col Norm tensor(0.5870, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9700)\n",
      "After Scale, v 0th col Norm tensor(0.5797, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5203, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5804, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9554, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5746, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3634, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5699, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.9134, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5733, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3458, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5835, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.9028, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9791)\n",
      "After Scale, q 0th col Norm tensor(0.5880, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9373)\n",
      "After Scale, v 0th col Norm tensor(0.5689, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5037, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5640, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8758, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3929, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5647, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7668, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5928, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3371, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5895, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7537, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5827, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0429)\n",
      "After Scale, q 0th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9463)\n",
      "After Scale, v 0th col Norm tensor(0.5790, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5609, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5901, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9231, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4373, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5801, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8741, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5750, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3232, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5807, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8408, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5718, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.8912)\n",
      "After Scale, q 0th col Norm tensor(0.5844, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9274)\n",
      "After Scale, v 0th col Norm tensor(0.5773, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5192, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5662, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9024, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5772, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4362, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5861, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8775, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5719, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3050, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5794, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7988, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5757, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1349)\n",
      "After Scale, q 0th col Norm tensor(0.5923, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8989)\n",
      "After Scale, v 0th col Norm tensor(0.5719, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5776, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5708, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8711, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5708, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4348, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5769, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8139, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5777, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3198, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5851, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8006, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5751, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0654)\n",
      "After Scale, q 0th col Norm tensor(0.5748, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8168)\n",
      "After Scale, v 0th col Norm tensor(0.5694, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5702, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5839, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7857, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5819, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4547, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5780, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7677, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5662, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3552, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5670, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7481, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5753, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0342)\n",
      "After Scale, q 0th col Norm tensor(0.5688, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7632)\n",
      "After Scale, v 0th col Norm tensor(0.5811, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5090, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7298, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5779, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3980, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5721, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7152, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5758, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3637, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5823, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7019, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5803, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0312)\n",
      "After Scale, q 0th col Norm tensor(0.5856, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7371)\n",
      "After Scale, v 0th col Norm tensor(0.5678, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4257, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5862, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7036, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5907, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3334, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5764, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6830, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5621, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2701, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5818, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6733, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5771, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9505)\n",
      "After Scale, q 0th col Norm tensor(0.5888, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.6393)\n",
      "After Scale, v 0th col Norm tensor(0.5684, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8367, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5869, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.6254, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5859, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4836, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6168, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5719, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.4008, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5790, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6127, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5736, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0318)\n",
      "After Scale, q 0th col Norm tensor(0.5722, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(2.0485)\n",
      "After Scale, v 0th col Norm tensor(0.5794, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8401, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5883, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7966, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5731, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.6452, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5825, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6702, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5731, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3654, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5763, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6191, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5692, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.6869)\n",
      "After Scale, q 0th col Norm tensor(0.5834, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7746)\n",
      "After Scale, v 0th col Norm tensor(0.5725, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.2029, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5860, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.6771, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5877, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.1564, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5680, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6479, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5689, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.0784, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6054, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5864, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0530)\n",
      "After Scale, q 0th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9356)\n",
      "After Scale, v 0th col Norm tensor(0.5898, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4767, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5840, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8355, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5755, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3749, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5834, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8131, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5854, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2615, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5732, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7821, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5853, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1329)\n",
      "After Scale, q 0th col Norm tensor(0.5859, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9430)\n",
      "After Scale, v 0th col Norm tensor(0.5781, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4833, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5781, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.9111, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5726, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3730, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5700, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8811, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5796, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3494, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5723, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.8612, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5888, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9930)\n",
      "After Scale, q 0th col Norm tensor(0.5785, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9023)\n",
      "After Scale, v 0th col Norm tensor(0.5820, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4474, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5730, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8385, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5791, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4028, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5759, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7388, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3293, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5728, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7236, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5817, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0552)\n",
      "After Scale, q 0th col Norm tensor(0.5898, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.9132)\n",
      "After Scale, v 0th col Norm tensor(0.5692, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5419, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8859, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5724, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4315, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5857, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8534, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5736, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3121, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7973, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5766, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(2.9097)\n",
      "After Scale, q 0th col Norm tensor(0.5810, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8794)\n",
      "After Scale, v 0th col Norm tensor(0.5675, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4727, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5712, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8673, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5675, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4401, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5752, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.8493, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5711, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2935, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7513, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5670, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1697)\n",
      "After Scale, q 0th col Norm tensor(0.5824, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.8566)\n",
      "After Scale, v 0th col Norm tensor(0.5721, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5924, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.8268, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5757, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4376, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7820, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5898, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3558, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5907, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7744, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5717, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.1089)\n",
      "After Scale, q 0th col Norm tensor(0.5716, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7797)\n",
      "After Scale, v 0th col Norm tensor(0.5814, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.5728, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5769, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7601, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5815, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4644, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5827, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.7522, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5781, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.4014, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5764, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.7265, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5777, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0483)\n",
      "After Scale, q 0th col Norm tensor(0.5736, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7499)\n",
      "After Scale, v 0th col Norm tensor(0.5756, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4992, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5761, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7095, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5788, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.4197, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5728, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6887, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5909, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.3858, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5707, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5815, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(3.0182)\n",
      "After Scale, q 0th col Norm tensor(0.5830, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.7719)\n",
      "After Scale, v 0th col Norm tensor(0.5800, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.4593, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5778, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.7197, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5736, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.3484, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5763, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6854, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5804, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.2945, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5757, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6826, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5784, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 0th col Norm tensor(4.0768)\n",
      "After Scale, q 0th col Norm tensor(0.5875, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 0th col Norm tensor(1.6519)\n",
      "After Scale, v 0th col Norm tensor(0.5808, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 1th col Norm tensor(2.8870, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 1th col Norm tensor(0.5788, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 1th col Norm tensor(1.6469, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 1th col Norm tensor(0.5868, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 2th col Norm tensor(2.6832, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 2th col Norm tensor(0.5668, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 2th col Norm tensor(1.6427, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 2th col Norm tensor(0.5696, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, q 3th col Norm tensor(2.4321, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, q 3th col Norm tensor(0.5815, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n",
      "Before Scale, v 3th col Norm tensor(1.6396, grad_fn=<LinalgVectorNormBackward0>)\n",
      "After Scale, v 3th col Norm tensor(0.5724, grad_fn=<LinalgVectorNormBackward0>)\n",
      "####################\n"
     ]
    }
   ],
   "source": [
    "len_of_layers = len(model.deberta.encoder.layer)\n",
    "q_new_lora_A_list = []\n",
    "v_new_lora_A_list = []\n",
    "\n",
    "for layer_idx in range(len_of_layers):\n",
    "    q_original_weight = model.deberta.encoder.layer[layer_idx].attention.self.query_proj.weight.data.T\n",
    "    v_original_weight = model.deberta.encoder.layer[layer_idx].attention.self.value_proj.weight.data.T\n",
    "\n",
    "    q_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.query_proj.lora_A.default.weight.T\n",
    "    v_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.value_proj.lora_A.default.weight.T\n",
    "\n",
    "    q_proj_u, q_proj_s, q_proj_v = torch.linalg.svd(q_original_weight)\n",
    "    q_new_lora_A = q_proj_u[:, :4] @ torch.diag(q_proj_s[:4]).sqrt()\n",
    "\n",
    "    v_proj_u, v_proj_s, v_proj_v = torch.linalg.svd(v_original_weight)\n",
    "    v_new_lora_A = v_proj_u[:, :4] @ torch.diag(v_proj_s[:4]).sqrt()\n",
    "\n",
    "    for i in range(4):\n",
    "        print(f\"Before Scale, q {i}th col Norm\", torch.norm(q_new_lora_A[:, i]))\n",
    "\n",
    "        q_og_lora_A_icol_norm = torch.norm(q_og_lora_A[:, i])\n",
    "        q_new_lora_A_icol_norm = torch.norm(q_new_lora_A[:, i])\n",
    "\n",
    "        q_scale = q_og_lora_A_icol_norm / q_new_lora_A_icol_norm\n",
    "\n",
    "        q_new_lora_A[:, i] = q_new_lora_A[:, i] * q_scale\n",
    "\n",
    "        print(f\"After Scale, q {i}th col Norm\", torch.norm(q_new_lora_A[:, i]))\n",
    "        print(\"####################\")\n",
    "\n",
    "        print(f\"Before Scale, v {i}th col Norm\", torch.norm(v_new_lora_A[:, i]))\n",
    "\n",
    "        v_og_lora_A_icol_norm = torch.norm(v_og_lora_A[:, i])\n",
    "        v_new_lora_A_icol_norm = torch.norm(v_new_lora_A[:, i])\n",
    "\n",
    "        v_scale = v_og_lora_A_icol_norm / v_new_lora_A_icol_norm\n",
    "\n",
    "        v_new_lora_A[:, i] = v_new_lora_A[:, i] * v_scale\n",
    "\n",
    "        print(f\"After Scale, v {i}th col Norm\", torch.norm(v_new_lora_A[:, i]))\n",
    "        print(\"####################\")\n",
    "\n",
    "    q_new_lora_A_list.append(q_new_lora_A)\n",
    "    v_new_lora_A_list.append(v_new_lora_A)\n",
    "\n",
    "    model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.lora_A.default.weight.data = (\n",
    "        q_new_lora_A_list[layer_idx].transpose(0, 1).contiguous()\n",
    "    )\n",
    "    model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.lora_A.default.weight.data = (\n",
    "        v_new_lora_A_list[layer_idx].transpose(0, 1).contiguous()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Layer  lora_A Norm  new_lora_A Norm\n",
      "0       0     0.567429         3.275432\n",
      "1       1     0.585682         2.727432\n",
      "2       2     0.579598         2.940313\n",
      "3       3     0.576057         3.074098\n",
      "4       4     0.575257         2.891766\n",
      "5       5     0.579585         3.040698\n",
      "6       6     0.578364         2.906408\n",
      "7       7     0.573760         3.160725\n",
      "8       8     0.573446         3.096008\n",
      "9       9     0.565393         3.062362\n",
      "10     10     0.582618         3.013615\n",
      "11     11     0.569206         2.994594\n",
      "12     12     0.568341         3.047105\n",
      "13     13     0.581365         2.691216\n",
      "14     14     0.566967         3.072535\n",
      "15     15     0.581771         3.119625\n",
      "16     16     0.574085         2.983188\n",
      "17     17     0.574927         3.053514\n",
      "18     18     0.575363         2.897182\n",
      "19     19     0.586855         3.152239\n",
      "20     20     0.578613         3.093604\n",
      "21     21     0.570879         3.077572\n",
      "22     22     0.577624         3.049182\n",
      "23     23     0.573886         2.944951\n",
      "24     24     0.579352         3.041238\n",
      "25     25     0.584444         2.683238\n",
      "26     26     0.568893         3.062517\n",
      "27     27     0.576324         3.111404\n",
      "28     28     0.576997         2.979095\n",
      "29     29     0.574366         3.042852\n",
      "30     30     0.575966         2.891166\n",
      "31     31     0.583495         3.134865\n",
      "32     32     0.582322         3.065402\n",
      "33     33     0.571108         3.034192\n",
      "34     34     0.576950         3.031219\n",
      "35     35     0.576118         2.950520\n",
      "36     36     0.584723         3.031766\n",
      "37     37     0.574917         2.686852\n",
      "38     38     0.573221         3.053014\n",
      "39     39     0.585378         3.132854\n",
      "40     40     0.577499         2.992962\n",
      "41     41     0.568450         3.055184\n",
      "42     42     0.574061         2.909659\n",
      "43     43     0.581406         3.169681\n",
      "44     44     0.575938         3.108903\n",
      "45     45     0.572011         3.048301\n",
      "46     46     0.573805         3.018180\n",
      "47     47     0.570426         4.076821\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+kAAAIjCAYAAAB/OVoZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/SrBM8AAAACXBIWXMAAA9hAAAPYQGoP6dpAACdaklEQVR4nOzdd3xT9f7H8Xe6Wzqg7L1l76GADCeiiKiogCxF3AO9Lvx5VVzg3gMUBRkO4IpergNRpmwZggxB9p5t6W6T8/vj26QtnSlpk7av5+ORR5KTk5xv1jnn8x2fr82yLEsAAAAAAMDr/LxdAAAAAAAAYBCkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CIJ0AAAAAAB8BEE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDQCnWp08f9enTp0S2ZbPZ9Nxzz7nuP/fcc7LZbDp58mSJbL9BgwYaNWpUiWwrq5J+n2XFsWPHNGjQIFWuXFk2m01vv/12sW7v3N8niq4k9yslYe/evbLZbJo6daq3iwIAhUKQDqBUmzp1qmw2m0JCQnTo0KEcj/fp00etW7f2QsncN2rUKNlsNtclPDxcjRo10qBBgzR37lw5HA6PbGfFihV67rnnFBMT45HX8yRfLltxK02/1cJ4+OGH9fPPP2vcuHGaPn26rrrqKm8XySfY7XbVqlVLNptNP/74o7eLAx8TExOjkJAQ2Ww2bdu2zdvFAeAlAd4uAAB4QkpKiiZOnKj33nvP20U5L8HBwfr0008lSUlJSdq3b5/++9//atCgQerTp4++++47RUZGutZfsGCB29tYsWKFxo8fr1GjRqlixYqFfl5SUpICAor3sJFf2Xbs2CE/P+qWS4vffvtN1113nR599FFvF8Wn/Pbbbzpy5IgaNGigmTNnql+/ft4uEnzI7NmzZbPZVKNGDc2cOVMvvviit4sEwAs42wFQJrRv316ffPKJDh8+XGzbsCxLSUlJxfb6khQQEKBhw4Zp2LBhGjNmjF588UVt2rRJEyZM0OLFizVmzJhs6wcFBSkoKKjYyuNwOJScnCxJCgkJKfYgPT/BwcEKDAz02vaRXUJCQr6PHz9+3K1KoIIkJyd7rDdJQQp6b+djxowZ6tixox5++GHNmzevWLeF4lHcv4+rr75aQ4YM0axZs4ptOwB8G0E6gDLhqaeekt1u18SJEwtcNz09XS+88IIaN26s4OBgNWjQQE899ZRSUlKyrdegQQP1799fP//8szp37qzQ0FBNmjRJixcvls1m0zfffKPx48erdu3aioiI0KBBgxQbG6uUlBSNHTtW1apVU3h4uG677bYcr+2uJ598UldeeaVmz56tv//+27U8t7Gj7733nlq1aqWwsDBVqlRJnTt3dp3sPffcc3rsscckSQ0bNnR1rd+7d68kM673/vvv18yZM9WqVSsFBwfrp59+cj2W25jfkydP6uabb1ZkZKQqV66shx56yBXYS/mPB836mgWVLbcx6bt379ZNN92k6OhohYWF6aKLLtL//ve/bOtk/b5eeukl1alTRyEhIbrsssu0a9euPD9zd99n79691a5du1yf26xZM/Xt27fQ28rLn3/+qVGjRqlRo0YKCQlRjRo1dPvtt+vUqVOudRYtWiSbzaZvv/02x/NnzZolm82mlStXupZt375dgwYNUnR0tEJCQtS5c2d9//332Z7nHFayZMkS3XvvvapWrZrq1KmTaxmd61qWpQ8++MD1PTq585199dVXevrpp1W7dm2FhYUpLi7Orc9rw4YN6tevnyIjIxUeHq7LLrtMq1atKvR727dvn+699141a9ZMoaGhqly5sm666SbXb9JdSUlJ+vbbbzV48GDdfPPNSkpK0nfffVeo554+fVqPPvqo2rRpo/DwcEVGRqpfv37atGlTtvXc/b1PnjxZjRs3VmhoqLp27aply5YV+v049xfz5s1T69atFRwcrFatWrn2GVkdOnRIt99+u6pXr+5a77PPPnM9blmWqlSpokceecS1zOFwqGLFivL39882BOaVV15RQECA4uPjC13W3Pz222/q2bOnKlSooIoVK+q6667L0cXcmZNi69atGjp0qCpVqqSLL75YUuH+j+7Yv3+/li1bpsGDB2vw4MHas2ePVqxYcV7vEUDpRHd3AGVCw4YNNWLECH3yySd68sknVatWrTzXveOOOzRt2jQNGjRI//rXv7R69WpNmDBB27ZtyxHY7NixQ0OGDNFdd92lMWPGqFmzZq7HJkyYoNDQUD355JPatWuX3nvvPQUGBsrPz09nzpzRc889p1WrVmnq1Klq2LChnnnmmfN6j8OHD9eCBQv0yy+/6IILLsh1nU8++UQPPvigBg0a5Aoi//zzT61evVpDhw7VDTfcoL///ltffvml3nrrLVWpUkWSVLVqVddr/Pbbb/rmm290//33q0qVKmrQoEG+5br55pvVoEEDTZgwQatWrdK7776rM2fO6IsvvnDr/RWmbFkdO3ZM3bt3V2Jioh588EFVrlxZ06ZN04ABAzRnzhxdf/312dafOHGi/Pz89Oijjyo2Nlavvvqqbr31Vq1evbpQ5SvofQ4fPlxjxozRli1bso0tX7t2rf7++289/fTTbn0eufnll1+0e/du3XbbbapRo4b++usvTZ48WX/99ZdWrVolm82mPn36qG7dupo5c2aOz2DmzJlq3LixunXrJkn666+/1KNHD9WuXVtPPvmkKlSooG+++UYDBw7U3Llzczz/3nvvVdWqVfXMM8/k2ZrYq1cvTZ8+XcOHD9cVV1yhESNGuB5z9zt74YUXFBQUpEcffVQpKSlu9Rr566+/1LNnT0VGRurxxx9XYGCgJk2apD59+mjJkiW68MILC3xva9eu1YoVKzR48GDVqVNHe/fu1UcffaQ+ffpo69atCgsLK3R5JOn7779XfHy8Bg8erBo1aqhPnz6aOXOmhg4dWuBzd+/erXnz5ummm25Sw4YNdezYMU2aNEm9e/fW1q1bc+zzCvN7nzJliu666y51795dY8eO1e7duzVgwABFR0erbt26hXpPy5cv13/+8x/de++9ioiI0Lvvvqsbb7xR+/fvV+XKlSWZ7/2iiy5yBfVVq1bVjz/+qNGjRysuLk5jx46VzWZTjx49tHTpUtdr//nnn4qNjZWfn59+//13XXPNNZKkZcuWqUOHDgoPDy9UGXOzcOFC9evXT40aNdJzzz2npKQkvffee+rRo4fWr1+fY7930003qWnTpnr55ZdlWZakwv0f3fHll1+qQoUK6t+/v0JDQ9W4cWPNnDlT3bt3L/L7BFBKWQBQin3++eeWJGvt2rXWP//8YwUEBFgPPvig6/HevXtbrVq1ct3fuHGjJcm64447sr3Oo48+akmyfvvtN9ey+vXrW5Ksn376Kdu6ixYtsiRZrVu3tlJTU13LhwwZYtlsNqtfv37Z1u/WrZtVv379At/LyJEjrQoVKuT5+IYNGyxJ1sMPP5zt/fXu3dt1/7rrrsv2fnPz2muvWZKsPXv25HhMkuXn52f99ddfuT727LPPuu4/++yzliRrwIAB2da79957LUnWpk2bLMuyrD179liSrM8//7zA18yvbPXr17dGjhzpuj927FhLkrVs2TLXsrNnz1oNGza0GjRoYNntdsuyMr+vFi1aWCkpKa5133nnHUuStXnz5hzbyqqw7zMmJsYKCQmxnnjiiWzrPfjgg1aFChWs+Pj4fLdz7m81N4mJiTmWffnll5Yka+nSpa5l48aNs4KDg62YmBjXsuPHj1sBAQHZPu/LLrvMatOmjZWcnOxa5nA4rO7du1tNmzZ1LXP+zy6++GIrPT093zI6SbLuu+++bMvc/c4aNWqU63vOa3tZ39vAgQOtoKAg659//nEtO3z4sBUREWH16tWrUO8tt22vXLnSkmR98cUXhSpXVv3797d69Ojhuj958mQrICDAOn78eIHPTU5Odn0+Tnv27LGCg4Ot559/3rWssL/31NRUq1q1alb79u2zrTd58mRLUrb9Sl4kWUFBQdauXbtcyzZt2mRJst577z3XstGjR1s1a9a0Tp48me35gwcPtqKiolyf82uvvWb5+/tbcXFxlmVZ1rvvvmvVr1/f6tq1q+t/ZbfbrYoVK2bbDxYkt31Q+/btrWrVqlmnTp3KVnY/Pz9rxIgRrmXO//+QIUNyvG5h/4+F1aZNG+vWW2913X/qqaesKlWqWGlpaW6/FoDSje7uAMqMRo0aafjw4Zo8ebKOHDmS6zo//PCDJGXrUilJ//rXvyQpR7fbhg0b5tlNecSIEdnGSF944YWyLEu33357tvUuvPBCHThwQOnp6e69oXM4W43Onj2b5zoVK1bUwYMHtXbt2iJvp3fv3mrZsmWh17/vvvuy3X/ggQckZX7WxeWHH35Q165dXV1PJfMZ3Xnnndq7d6+2bt2abf3bbrstW0tsz549JZkWysIo6H1GRUXpuuuu05dffulqabPb7fr66681cOBAVahQwc13mFNoaKjrdnJysk6ePKmLLrpIkrR+/XrXYyNGjFBKSormzJnjWvb1118rPT1dw4YNk2S6T//222+6+eabdfbsWZ08eVInT57UqVOn1LdvX+3cuTPHjAljxoyRv79/kcvv7nc2cuTIbO+5sOx2uxYsWKCBAweqUaNGruU1a9bU0KFDtXz58hxd53N7b1m3nZaWplOnTqlJkyaqWLFits+7ME6dOqWff/5ZQ4YMcS278cYbXV3TCxIcHOxKnGi323Xq1CmFh4erWbNmuZaloN/7unXrdPz4cd19993Z1hs1apSioqIK/b4uv/xyNW7c2HW/bdu2ioyMdG3HsizNnTtX1157rSzLcv3OTp48qb59+yo2NtZV/p49e8put7u6eC9btkw9e/ZUz549Xd3wt2zZopiYGNf7KYojR45o48aNGjVqlKKjo7OV/Yorrsh133X33XfnWFbY/2Nh/Pnnn9q8eXO238eQIUN08uRJ/fzzz269FoDSjyAdQJny9NNPKz09Pc+x6fv27ZOfn5+aNGmSbXmNGjVUsWJF7du3L9vyhg0b5rmtevXqZbvvPLE9t5toVFSUHA6HYmNjC/0+cuMcfxkREZHnOk888YTCw8PVtWtXNW3aVPfdd59+//13t7aT33vOTdOmTbPdb9y4sfz8/Io8brew9u3bl234gVOLFi1cj2d17vdVqVIlSdKZM2cKtb3CvM8RI0a4xpVKpkvtsWPHNHz48EJtoyCnT5/WQw89pOrVqys0NFRVq1Z1fV9Zf1/NmzdXly5dNHPmTNeymTNn6qKLLnL99nft2iXLsvTvf/9bVatWzXZ59tlnJZnkb1m5+9s4l7vfWVG3d+LECSUmJua5LYfDoQMHDhS4raSkJD3zzDOqW7eugoODVaVKFVWtWlUxMTFu/5+//vprpaWlqUOHDtq1a5d27dql06dP68ILL8z2PeXF4XDorbfeUtOmTbOVxdkl/FwF/d6dn/W5v+vAwMBsFRsFOXc7zm05t3PixAnFxMRo8uTJOX5nt912m6TM31nHjh0VFhbm+v84g/RevXpp3bp1Sk5Odj2WtaLHXc73ntfv4+TJkzmGc+T2+yjs/7EwZsyYoQoVKqhRo0au30dISIhrFgAA5Qtj0gGUKY0aNdKwYcM0efJkPfnkk3muV9ixgvm14uXVopjXcmfralFt2bJFknJUMGTVokUL7dixQ/Pnz9dPP/2kuXPn6sMPP9Qzzzyj8ePHF2o7RWm5zOrczzavz9put5/Xdtzl6e8lt/fVt29fVa9eXTNmzFCvXr00Y8YM1ahRQ5dffnmRtnGum2++WStWrNBjjz2m9u3bKzw8XA6HQ1dddVWOzOcjRozQQw89pIMHDyolJUWrVq3S+++/73rcuf6jjz6aZ2+Rc39r5/vbcFdJbi+3bT3wwAP6/PPPNXbsWHXr1k1RUVGy2WwaPHiw25nmnYFWjx49cn189+7d+QbHL7/8sv7973/r9ttv1wsvvKDo6Gj5+flp7NixuZaluPZD7m7HWbZhw4Zp5MiRua7btm1bSaaC4MILL9TSpUu1a9cuHT16VD179lT16tWVlpam1atXa9myZWrevHmeuSqKS26/D3f+j/mxLEtffvmlEhIScu3FdPz4ccXHx5/XGHwApQtBOoAy5+mnn9aMGTP0yiuv5Hisfv36cjgc2rlzp6v1TjKJjWJiYlS/fv2SLKpbpk+fLpvNpiuuuCLf9SpUqKBbbrlFt9xyi1JTU3XDDTfopZde0rhx4xQSEuJ2MqOC7Ny5M1sr065du+RwOFyJl5wteFmzM0s5W02lwleeSOa73LFjR47l27dvdz3uSQW9T8kELEOHDtXUqVP1yiuvaN68eefdRdzpzJkz+vXXXzV+/PhsSQh37tyZ6/qDBw/WI488oi+//FJJSUkKDAzULbfc4nrcGRAGBgZ6rBKhICX1nVWtWlVhYWF5bsvPz69QidHmzJmjkSNH6o033nAtS05OzvFbLogzS/f999+v3r17Z3vM4XBo+PDhmjVrVr7JBefMmaNLLrlEU6ZMybY8JibGlWTRHc7PeufOnbr00ktdy9PS0rRnz548ZypwV9WqVRURESG73V6o31nPnj31yiuvaOHChapSpYqaN28um82mVq1aadmyZVq2bJn69+9/XmVyvve8fh9VqlQpcHiKu//H/CxZskQHDx7U888/n+245NzOnXfeqXnz5rmGqgAo++juDqDMady4sYYNG6ZJkybp6NGj2R67+uqrJUlvv/12tuVvvvmmJLmyB/uaiRMnasGCBbrllltydE/N6typf4KCgtSyZUtZlqW0tDRJcp18uhto5OWDDz7Idv+9996TJPXr10+SFBkZqSpVqmTL2ixJH374YY7XcqdsV199tdasWZNtOrGEhARNnjxZDRo0cGtcfWEU9D6dhg8frjNnzuiuu+5SfHy8x06snYH+uS2h5/6WnapUqaJ+/fppxowZmjlzpq666qpswVy1atXUp08fTZo0KdccDidOnPBIubMqqe/M399fV155pb777rtswxGOHTumWbNm6eKLL1ZkZGShXufcz/u9995zuxeIsxX98ccf16BBg7Jdbr75ZvXu3bvALs25lWX27Nk58gYUVufOnVW1alV9/PHHSk1NdS2fOnWqx/YNkin3jTfeqLlz57p6A2V17u+sZ8+eSklJ0dtvv62LL77YVXHXs2dPTZ8+XYcPHz6v8eiSyU3Qvn17TZs2Ldt73bJlixYsWOA6ThT0vqTC/x/z4+zq/thjj+X4fYwZM0ZNmzalyztQztCSDqBM+r//+z9Nnz5dO3bsUKtWrVzL27Vrp5EjR2ry5MmKiYlR7969tWbNGk2bNk0DBw7UJZdc4sVSmzncZ8yYIcm02O3bt0/ff/+9/vzzT11yySWaPHlyvs+/8sorVaNGDfXo0UPVq1fXtm3b9P777+uaa65xjWXv1KmTJPMZDR48WIGBgbr22muLnNhsz549GjBggK666iqtXLlSM2bM0NChQ7O1xN1xxx2aOHGi7rjjDnXu3FlLly7NNt+7kztle/LJJ/Xll1+qX79+evDBBxUdHa1p06Zpz549mjt3rivJlqcU5n1KUocOHdS6dWvNnj1bLVq0UMeOHQu9jRMnTujFF1/Msbxhw4a69dZb1atXL7366qtKS0tT7dq1tWDBAu3ZsyfP1xsxYoQGDRokyUxndq4PPvhAF198sdq0aaMxY8aoUaNGOnbsmFauXKmDBw/mmIP7fJXkd/biiy/ql19+0cUXX6x7771XAQEBmjRpklJSUvTqq68W6jX69++v6dOnKyoqSi1bttTKlSu1cOFC19RihTVz5ky1b98+z9b7AQMG6IEHHtD69evz/L30799fzz//vG677TZ1795dmzdv1syZM90aP55VYGCgXnzxRd1111269NJLdcstt2jPnj36/PPPi/yaeZk4caIWLVqkCy+8UGPGjFHLli11+vRprV+/XgsXLtTp06dd63br1k0BAQHasWOH7rzzTtfyXr166aOPPpKk8w7SJem1115Tv3791K1bN40ePdo1BVtUVJSee+65Ap8fGRnp9v8xNykpKZo7d66uuOIKhYSE5LrOgAED9M477+j48eOqVq2aW68PoJTyQkZ5APCYrFOwnWvkyJGWpBzTWqWlpVnjx4+3GjZsaAUGBlp169a1xo0bl20aKssyU35dc801OV7XOcXR7NmzC1UW5xQ+J06cyPe9OMvrvISFhVkNGjSwbrzxRmvOnDk5pl+yrJxTsE2aNMnq1auXVblyZSs4ONhq3Lix9dhjj1mxsbHZnvfCCy9YtWvXtvz8/LJNeaZcps1yUh5TsG3dutUaNGiQFRERYVWqVMm6//77raSkpGzPTUxMtEaPHm1FRUVZERER1s0332wdP348x2vmV7Zzp2CzLMv6559/rEGDBlkVK1a0QkJCrK5du1rz58/Ptk5e31d+U8Nl5c77dHr11VctSdbLL7+c72tn1bt372zff9bLZZddZlmWZR08eNC6/vrrrYoVK1pRUVHWTTfdZB0+fDjXz9GyLCslJcWqVKmSFRUVlWdZ//nnH2vEiBFWjRo1rMDAQKt27dpW//79rTlz5rjWye9/lpe8fkvn850VtL1zP4P169dbffv2tcLDw62wsDDrkksusVasWJFtnfze25kzZ6zbbrvNqlKlihUeHm717dvX2r59e66/xbz88ccfliTr3//+d57r7N27N8f0iudKTk62/vWvf1k1a9a0QkNDrR49elgrV67MsQ9w9/f+4YcfWg0bNrSCg4Otzp07W0uXLs3xmnnJ6zvO7fM5duyYdd9991l169a1AgMDrRo1aliXXXaZNXny5BzP79KliyXJWr16tWvZwYMHLUlW3bp1CyzXufJ67wsXLrR69OhhhYaGWpGRkda1115rbd26Nds6+e2/3f0/5mbu3LmWJGvKlCl5rrN48WJLkvXOO+8U6jUBlH42y/JwBhEAAMq5d955Rw8//LD27t2ba/brkpKenq5atWrp2muvzTGWGQAA+CbGpAMA4EGWZWnKlCnq3bu3VwN0SZo3b55OnDihESNGeLUcAACg8BiTDgCAByQkJOj777/XokWLtHnzZn333XdeK8vq1av1559/6oUXXlCHDh1yZBSH55ybnPJcoaGhioqKKqHSlC+pqanZxrPnJioqqsSnDnSy2+0FJmAMDw9najUAORCkAwDgASdOnNDQoUNVsWJFPfXUUxowYIDXyvLRRx9pxowZat++vaZOneq1cpQHNWvWzPfxkSNH8h0UkxUrVhSY7PPzzz/XqFGjSqZA5zhw4EC2aRtz8+yzzxYqUR2A8oUx6QAAAEW0cOHCfB+vVauWx6cDhHHmzBn98ccf+a7TqlWrAitSiktycrKWL1+e7zqNGjXyeDZ9AKUfQToAAAAAAD6CxHEAAAAAAPiIcjcm3eFw6PDhw4qIiJDNZvN2cQAAAAAAZZxlWTp79qxq1aolP7/828rLXZB++PBh1a1b19vFAAAAAACUMwcOHFCdOnXyXafcBekRERGSzIcTGRnp5dIAAAAAAMq6uLg41a1b1xWP5qfcBenOLu6RkZEE6QAAAACAElOYIdckjgMAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BHlbkx6YViWpfT0dNntdm8XBchVYGCg/P39vV0MAAAAAB5GkH6O1NRUHTlyRImJid4uCpAnm82mOnXqKDw83NtFAQAAAOBBBOlZOBwO7dmzR/7+/qpVq5aCgoIKlX0PKEmWZenEiRM6ePCgmjZtSos6AAAAUIYQpGeRmpoqh8OhunXrKiwszNvFAfJUtWpV7d27V2lpaQTpAAAAQBlC4rhc+PnxscC30cMDAAAAKJuIRgEAAAAA8BEE6QAAAAAA+AiC9DKiT58+Gjt2rLeLAQAAAAA4DwTpKBYTJkyQv7+/XnvttUKtb7PZFBISon379mVbPnDgQI0aNaoYSggAAAAAvocgHZIku90uh8Phsdf77LPP9Pjjj+uzzz4r9HNsNpueeeYZj5VBMtOVpaene/Q1AQAAAKC4EKQXwLIsJaame+ViWVaRy33mzBmNGDFClSpVUlhYmPr166edO3e6Hp86daoqVqyo77//Xi1btlRwcLD279+vtWvX6oorrlCVKlUUFRWl3r17a/369W5te8mSJUpKStLzzz+vuLg4rVixolDPu//++zVjxgxt2bIlz3VSUlL04IMPqlq1agoJCdHFF1+stWvXuh5fvHixbDabfvzxR3Xq1EnBwcFavny5+vTpowceeEBjx45VpUqVVL16dX3yySdKSEjQbbfdpoiICDVp0kQ//vijW+8VAAAAADyJedILkJRmV8tnfvbKtrc+31dhQUX7ikaNGqWdO3fq+++/V2RkpJ544gldffXV2rp1qwIDAyVJiYmJeuWVV/Tpp5+qcuXKqlatmnbv3q2RI0fqvffek2VZeuONN3T11Vdr586dioiIKNS2p0yZoiFDhigwMFBDhgzRlClT1L179wKf16NHD/3999968sknNX/+/FzXefzxxzV37lxNmzZN9evX16uvvqq+fftq165dio6Odq335JNP6vXXX1ejRo1UqVIlSdK0adP0+OOPa82aNfr66691zz336Ntvv9X111+vp556Sm+99ZaGDx+u/fv3KywsrFDvFQAAAAA8iZb0MsgZnH/66afq2bOn2rVrp5kzZ+rQoUOaN2+ea720tDR9+OGH6t69u5o1a6awsDBdeumlGjZsmJo3b64WLVpo8uTJSkxM1JIlSwq17bi4OM2ZM0fDhg2TJA0bNkzffPON4uPjC/X8CRMm6KefftKyZctyPJaQkKCPPvpIr732mvr166eWLVvqk08+UWhoqKZMmZJt3eeff15XXHGFGjdu7Are27Vrp6efflpNmzbVuHHjFBISoipVqmjMmDFq2rSpnnnmGZ06dUp//vlnocoKAAAAAJ5GS3oBQgP9tfX5vl7bdlFs27ZNAQEBuvDCC13LKleurGbNmmnbtm2uZUFBQWrbtm225x47dkxPP/20Fi9erOPHj8tutysxMVH79+8v1La//PJLNW7cWO3atZMktW/fXvXr19fXX3+t0aNHF/j8li1basSIEXryySf1+++/Z3vsn3/+UVpamnr06OFaFhgYqK5du2Z7X5LUuXPnHK+d9b36+/urcuXKatOmjWtZ9erVJUnHjx8vxDsFAAAA4HXb/isFhEh1ukihFb1dGo8gSC+AzWYrcpdzXxcaGiqbzZZt2ciRI3Xq1Cm98847ql+/voKDg9WtWzelpqYW6jWnTJmiv/76SwEBmZ+Zw+HQZ599VqggXZLGjx+vCy64IFurv7sqVKiQY5mzm7+TzWbLtsz5WXgygR4AAACAYvTTU1Lsfun2n6V6F3m7NB5Bd/cyqEWLFkpPT9fq1atdy06dOqUdO3aoZcuW+T73999/14MPPqirr75arVq1UnBwsE6ePFmo7W7evFnr1q3T4sWLtXHjRtdl8eLFWrlypbZv316o16lbt67uv/9+PfXUU7Lb7a7ljRs3VlBQULYW9rS0NK1du7bA9wUAAACgjElLlmIPmNvRjb1bFg8iSC+DmjZtquuuu05jxozR8uXLtWnTJg0bNky1a9fWddddV+Bzp0+frm3btmn16tW69dZbFRoaWqjtTpkyRV27dlWvXr3UunVr16VXr17q0qVLjnHj+Rk3bpwOHz6shQsXupZVqFBB99xzjx577DH99NNP2rp1q8aMGaPExMRCt9IDAAAAKCNi9kmypKAIqUIVb5fGYwjSy6jPP/9cnTp1Uv/+/dWtWzdZlqUffvghR5fvc02ZMkVnzpxRx44dNXz4cNd0ZwVJTU3VjBkzdOONN+b6+I033qgvvvhCaWlphSp/dHS0nnjiCSUnJ2dbPnHiRN14440aPny4OnbsqF27dunnn392ZXAHAAAAUE6c+sdcV24knTOMtzSzWeczGXcpFBcXp6ioKMXGxioyMjLbY8nJydqzZ48aNmyokJAQL5UQKBi/VQAAAJR7K96XFvyf1Op66aap3i5NvvKLQ89FSzoAAAAAoPQ5vdtcRzfybjk8zKeC9IkTJ8pms2ns2LH5rjd79mw1b95cISEhatOmjX744YeSKWA5N3PmTIWHh+d6adWqlbeLBwAAAKA8OZ3R3b0MJY2TfGgKtrVr12rSpEk55u0+14oVKzRkyBBNmDBB/fv316xZszRw4ECtX79erVu3LqHSlk8DBgzINvd6VgWNdQcAAAAAjyqjLek+EaTHx8fr1ltv1SeffKIXX3wx33XfeecdXXXVVXrsscckSS+88IJ++eUXvf/++/r4449LorjlVkREhCIiIrxdDAAAAADlXXqKFHvQ3C5jQbpPdHe/7777dM011+jyyy8vcN2VK1fmWK9v375auXJlruunpKQoLi4u2wUAAAAAUIqd2SdZDikoXAoveDaq0sTrLelfffWV1q9fr7Vr1xZq/aNHj6p69erZllWvXl1Hjx7Ndf0JEyZo/Pjx511OAAAAAICPcHV1b1impl+TvNySfuDAAT300EOaOXNmsU0jNW7cOMXGxrouBw4cKJbtAAAAAABKSBlNGid5uSX9jz/+0PHjx9WxY0fXMrvdrqVLl+r9999XSkqK/P39sz2nRo0aOnbsWLZlx44dU40aNXLdRnBwsIKDgz1feAAAAACAd5TRpHGSl1vSL7vsMm3evFkbN250XTp37qxbb71VGzduzBGgS1K3bt3066+/Zlv2yy+/qFu3biVVbAAAAACAN5XhIN2rLekRERE5pk2rUKGCKleu7Fo+YsQI1a5dWxMmTJAkPfTQQ+rdu7feeOMNXXPNNfrqq6+0bt06TZ48ucTLj9zt3btXDRs21IYNG9S+fXtvFwcAAABAWXMqo7t75bLX3d0nsrvnZ//+/Tpy5Ijrfvfu3TVr1ixNnjxZ7dq105w5czRv3jzmSEe+mjdvruDg4DwTDGY1depU2Ww2XXXVVdmWx8TEyGazafHixcVUSgAAAAAFSk+VYjNyjdGSXvzODYByC4huuukm3XTTTSVTIHhFamqqgoKCPPJay5cvV1JSkgYNGqRp06bpiSeeKPA5AQEBWrhwoRYtWqRLLrnEI+WQPPu+AAAAgHIpZr+Zfi2wghReveD1Sxmfb0n3OsuSUhO8c7GsQhezT58+evDBB/X4448rOjpaNWrU0HPPPZdtnZiYGN1xxx2qWrWqIiMjdemll2rTpk2SpNjYWPn7+2vdunWSJIfDoejoaF100UWu58+YMUN169Yt0se4ZMkSde3aVcHBwapZs6aefPJJpaenZyv//fffr7Fjx6pKlSrq27evJOnNN99UmzZtVKFCBdWtW1f33nuv4uPj3dr2lClTNHToUA0fPlyfffZZoZ5ToUIF3X777XryySfzXW/z5s269NJLFRoaqsqVK+vOO+/MVr5Ro0Zp4MCBeumll1SrVi01a9ZMe/fulc1m0zfffKOePXsqNDRUXbp00d9//621a9eqc+fOCg8PV79+/XTixAm33isAAABQ5rkyuzcqc9OvST7Yku5z0hKll2t5Z9tPHZaCKhR69WnTpumRRx7R6tWrtXLlSo0aNUo9evTQFVdcIcn0QAgNDdWPP/6oqKgoTZo0SZdddpn+/vtvRUdHq3379lq8eLE6d+6szZs3y2azacOGDYqPj1d4eLiWLFmi3r17u/02Dh06pKuvvlqjRo3SF198oe3bt2vMmDEKCQnJVpEwbdo03XPPPfr9999dy/z8/PTuu++qYcOG2r17t+699149/vjj+vDDDwu17bNnz2r27NlavXq1mjdvrtjYWC1btkw9e/Ys8LnPPfecmjRpojlz5mjQoEE5Hk9ISFDfvn3VrVs3rV27VsePH9cdd9yh+++/X1OnTnWt9+uvvyoyMlK//PJLtuc/++yzevvtt1WvXj3dfvvtGjp0qCIiIvTOO+8oLCxMN998s5555hl99NFHhXqvAAAAQLmQdY70MoiW9DKkbdu2evbZZ9W0aVONGDFCnTt3dmXCX758udasWaPZs2erc+fOatq0qV5//XVVrFhRc+bMkWRas53DCxYvXqwrrrhCLVq00PLly13LihKkf/jhh6pbt67ef/99NW/eXAMHDtT48eP1xhtvyOFwuNZr2rSpXn31VTVr1kzNmjWTJI0dO1aXXHKJGjRooEsvvVQvvviivvnmm0Jv+6uvvlLTpk3VqlUr+fv7a/DgwZoyZUqhnlurVi099NBD+r//+79srf5Os2bNUnJysr744gu1bt1al156qd5//31Nnz492zSBFSpU0KeffqpWrVqpVatWruWPPvqo+vbtqxYtWuihhx7SH3/8oX//+9/q0aOHOnTooNGjR2vRokWFfq8AAABAueAM0stg0jiJlvSCBYaZFm1vbdsNbdu2zXa/Zs2aOn78uCRp06ZNio+PV+XKlbOtk5SUpH/+Md1FevfurSlTpshut2vJkiW68sorVaNGDS1evFht27bVrl271KdPH7ffxrZt29StWzfZsnRF6dGjh+Lj43Xw4EHVq1dPktSpU6ccz124cKEmTJig7du3Ky4uTunp6UpOTlZiYqLCwgr+fD777DMNGzbMdX/YsGHq3bu33nvvPUVERBT4/CeeeEKTJk3SZ599pptvvjnH+2rXrp0qVMjs7dCjRw85HA7t2LFD1aub8TFt2rTJdRx61u8r67pZlzm/PwAAAAAZTmXp7l4GEaQXxGZzq8u5NwUGBma7b7PZXC3V8fHxqlmzZq6J+CpWrChJ6tWrl86ePav169dr6dKlevnll1WjRg1NnDhR7dq1U61atdS0adNiK3/WYFcyU7n1799f99xzj1566SVFR0dr+fLlGj16tFJTUwsM0rdu3apVq1ZpzZo12ZLF2e12ffXVVxozZkyBZapYsaLGjRun8ePHq3///h55X05Zvy9nBca5y7L2NAAAAACgMj1HukR393KjY8eOOnr0qAICAtSkSZNslypVqkgyAWnbtm31/vvvKzAwUM2bN1evXr20YcMGzZ8/v0hd3SWpRYsWWrlypawsifB+//13RUREqE6dOnk+748//pDD4dAbb7yhiy66SBdccIEOHy58r4YpU6aoV69e2rRpkzZu3Oi6PPLII4Xu8i5JDzzwgPz8/PTOO+/keF+bNm1SQkJCtvfl5+fn6q4PAAAAwIPsaSa7uyRFl83u7gTp5cTll1+ubt26aeDAgVqwYIH27t2rFStW6P/+7/9cGd0lMy595syZroA8OjpaLVq00Ndff13kIP3ee+/VgQMH9MADD2j79u367rvv9Oyzz+qRRx6Rn1/eP8EmTZooLS1N7733nnbv3q3p06fr448/LtQ209LSNH36dA0ZMkStW7fOdrnjjju0evVq/fXXX4V6rZCQEI0fP17vvvtutuW33nqrQkJCNHLkSG3ZskWLFi3SAw88oOHDh7u6rwMAAADwoJj9kmWXAkKliBreLk2xIEgvJ2w2m3744Qf16tVLt912my644AINHjxY+/btyxZQ9u7dW3a7PdvY8z59+uRY5o7atWvrhx9+0Jo1a9SuXTvdfffdGj16tJ5++ul8n9euXTu9+eabeuWVV9S6dWvNnDlTEyZMKNQ2v//+e506dUrXX399jsdatGihFi1auNWaPnLkSDVqlL07TVhYmH7++WedPn1aXbp00aBBg3TZZZfp/fffL/TrAgAAAHBD1q7uZXD6NUmyWZYbk3GXAXFxcYqKilJsbKwiIyOzPZacnKw9e/aoYcOGCgkJ8VIJgYLxWwUAAEC5tOpj6acnpBbXSrfM8HZpCi2/OPRctKQDAAAAAEqHMp40TiJIRxG8/PLLCg8Pz/XSr1+/EitHv3798izHyy+/XGLlAAAAAFBCykGQzhRscNvdd9+dY85wp9DQ0BIrx6effqqkpKRcH4uOji6xcgAAAAAoIaedc6SXzczuEkE6iiA6OtonguDatWt7uwgAAAAASkq26dfKbks63d1zUc5y6aEU4jcKAACAcidmv+RIz5h+raa3S1NsCNKzCAwMlCQlJiZ6uSRA/lJTUyVJ/v7+Xi4JAAAAUEJO7zHX0Q0lv7IbytLdPQt/f39VrFhRx48fl2TmwbaV0bn3UHo5HA6dOHFCYWFhCgjgLwwAAIByohwkjZMI0nOoUaOGJLkCdcAX+fn5qV69elQiAQAAoPxwJY0jSC9XbDabatasqWrVqiktLc3bxQFyFRQUJL8y3MUHAAAAyIGW9PLN39+f8b4AAAAA4CvKSZBOUxwAAAAAwLfZ06Uze83tymV3jnSJIB0AAAAA4OtiD2RMvxYiRdTydmmKFUE6AAAAAMC3Obu6Vyrb069JBOkAAAAAAF9XTsajSwTpAAAAAABf5wrSG3q3HCWAIB0AAAAA4NtOZcyRXsaTxkkE6QAAAAAAX0d3dwAAAAAAfIDDnjn9GkE6AAAAAABeFHtAcqRJ/sFSZB1vl6bYEaQDAAAAAHyXa/q1BmV++jWJIB0AAAAA4MucQXo5SBonEaQDAAAAAHzZqfKTNE4iSAcAAAAA+LJyNEe6RJAOAAAAAPBlpzPmSI+muzsAAAAAAN5TzqZfkwjSAQAAAAC+Ku6QZE+V/IOkqLI//ZpEkA4AAAAA8FWnMrq6V2og+fl7tSglhSAdAAAAAOCbTpevzO4SQToAAAAAwFe5gvTykTROIkgHAAAAAPiqcjb9mkSQDgAAAADwVXR3BwAAAADABzgc0uk95nZlursDAAAAAOA9cYcke4rkFyhFlo/p1ySCdAAAAACAL3J2da9UX/IP8G5ZShBBOgAAAADA95zOmCO9HGV2lwjSAQAAAAC+qBwmjZMI0gEAAAAAvsiZNI4gHQAAAAAALzuV0d29MkE6AAAAAADe43BIZ2hJBwAAAADA+84ekdKTJb8AKaqet0tTorwapH/00Udq27atIiMjFRkZqW7duunHH3/Mc/2pU6fKZrNlu4SEhJRgiQEAAAAAxc6Z2b1i+Zp+TZK8+m7r1KmjiRMnqmnTprIsS9OmTdN1112nDRs2qFWrVrk+JzIyUjt27HDdt9lsJVVcAAAAAEBJKKeZ3SUvB+nXXntttvsvvfSSPvroI61atSrPIN1ms6lGjRolUTwAAAAAgDe4ksaVrznSJR8ak2632/XVV18pISFB3bp1y3O9+Ph41a9fX3Xr1tV1112nv/76K9/XTUlJUVxcXLYLAAAAAMCHleOWdK8H6Zs3b1Z4eLiCg4N1991369tvv1XLli1zXbdZs2b67LPP9N1332nGjBlyOBzq3r27Dh48mOfrT5gwQVFRUa5L3bp1i+utAAAAAAA8oZzOkS5JNsuyLG8WIDU1Vfv371dsbKzmzJmjTz/9VEuWLMkzUM8qLS1NLVq00JAhQ/TCCy/kuk5KSopSUlJc9+Pi4lS3bl3FxsYqMjLSY+8DAAAAAOABDof0ci0pPUl6YH2Z6PIeFxenqKioQsWhXk+TFxQUpCZNmkiSOnXqpLVr1+qdd97RpEmTCnxuYGCgOnTooF27duW5TnBwsIKDgz1WXgAAAABAMYo/agJ0m79UsXxNvyb5QHf3czkcjmwt3/mx2+3avHmzatasWcylAgAAAACUCOd49Er1Jf9A75bFC7zakj5u3Dj169dP9erV09mzZzVr1iwtXrxYP//8syRpxIgRql27tiZMmCBJev7553XRRRepSZMmiomJ0WuvvaZ9+/bpjjvu8ObbAAAAAAB4ijOzezkcjy55OUg/fvy4RowYoSNHjigqKkpt27bVzz//rCuuuEKStH//fvn5ZTb2nzlzRmPGjNHRo0dVqVIlderUSStWrCjU+HUAAAAAQClQjjO7Sz6QOK6kuTNgHwAAAABQwr4eLm37XrrqFemiu71dGo9wJw71uTHpAAAAAIByrJy3pBOkAwAAAAB8g2URpHu7AAAAAAAASJLOHpXSEsvt9GsSQToAAAAAwFc4W9Er1pUCgrxbFi8hSAcAAAAA+AZXV/fG3i2HFxGkAwAAAAB8w+nyPUe6RJAOAAAAAPAV5TxpnESQDgAAAADwFc4gvTLd3QEAAAAA8B7Lkk7Rkk6QDgAAAADwvvjjUlqCZPOTKtb3dmm8hiAdAAAAAOB9zqRxUeV3+jWJIB0AAAAA4AtIGieJIB0AAAAA4AsI0iURpAMAAAAAfMGpjO7u5Tizu0SQDgAAAADwBbSkSyJIBwAAAAB4m2VJp/eY29G0pAMAAAAA4D0JJ6TUs5JsUqXyO/2aRJAOAAAAAPA2Z1f3qLpSQLB3y+JlAd4uAPJwere0bb4UWUtqM8jbpQEAAACA4uMM0iuX7/HoEi3pvmvXr9Iv/5bWfe7tkgAAAABA8XJmdi/nSeMkgnTf1fhSc31gtZRy1rtlAQAAAIDiRGZ3F4J0X1W5sVSpgeRIk/Yu93ZpAAAAAKD4nHa2pJfvzO4SQbpvc7am//Obd8sBAAAAAMUl2/RrtKQTpPsygnQAAAAAZV3iKSklTmb6tQbeLo3XEaT7soa9JJu/dGqXdGaft0sDAAAAAJ7nTBoXVUcKDPFuWXwAQbovC4mS6nQxt2lNBwAAAFAWuZLGNfRuOXwEQbqvo8s7AAAAgLLMFaSTNE4iSPd9ziB9zxLJnu7dsgAAAACAp51mjvSsCNJ9Xe2Optt7cqx0eIO3SwMAAAAAnsUc6dkQpPs6P3+pUR9z+59fvVoUAAAAAPAoy5JOZQTplenuLhGklw6MSwcAAABQFiWellJizW2mX5NEkF46OIP0g+ukpBivFgUAAABACVr5oTTjRmnDTCk10dul8TxnV/fIOlJgqHfL4iMI0kuDivWkyk0lyy7tXebt0qA4xRyQ5twu/f6OqVUEAABA+XXkT+nnp6RdC6Xv7pXebC79+IR0fLu3S+Y5rqRxTL/mRJBeWjhb03cxLr3Msizpvw9KW+ZKvzwjvdlS+u9Y6fg2b5cMKBuObpZm3yatnWKScQIA4MssS/rpSUmWVLO9abhLjpVWfyx9eKH0WT/pz9lSeoq3S3p+SBqXA0F6aeEal/6r+cOi7Nn+P5N3wD9Iqt5GSk+S/vhc+vAiadoAafsPksPu7VICpZM9TZozWvrrP9L/HpFebyZ9e7e0bwX7VACAb9o6T9r3uxQQIt0yQ3pwo3TrXKnZNZLNT9q/QvrPHdIbzaUFT0un/vF2iYvmNEnjzkWQXlo0uFjyC5Ri9mf+kFF2pCVJP48zt7s/KN29TBr1g9RigNkJ71kifTVEereDtOJ9chMA7lozWTq5QwqNlqq2MJVgm76UPu8nvd/ZDDGJP+7tUgKl18F1pgvugbXeLglKoxM7pB0/UmmaVVqStODf5naPsVLFumbWp6aXS0NmSWO3SH3GSZG1paTT0or3pPc6StOulf76VkpP9Wrx3XKKOdLPZbOs8vVviIuLU1RUlGJjYxUZGent4rhnan8zJv3q16WuY7xdGnjS4onS4gkmYcb9a6SgCpmPxRyQ1n4q/TFVSo4xywIrSO2HSF3vkqpe4I0SA6VH/HHpvU5SSpw04D2pw3ATUGz4Qto8V0pLMOv5BUjN+kkdRkhNLjMnQwAKlhQjfdBVij9m7tfrLvV4SGp6peRHexDycWafOf/Z9JUkS2o/TLr2Hck/wNsl874lr0qLXjJB+P3rpKCw3Nezp0s7F5jelzt/kZQR2lWoao53nUb6fsb0ifXNOe49K6TqrbxdmmLjThxKkF6aLHtD+vV5qdnV0pAvvV0aeMqZfebkJj1Zummq1Or63NdLTZQ2fyOtniQd35q5vPGl0oV3S02u4GSovLKnS/tXmiETh/6QLh4rNb/G26XyHfPulTbOlGp1lO74Nfv/JOWsaXFY/4V0MEsLYGRtqf2tUodhUqX6JV9moDT536PS2k9MT5WUs5IjzSyv2kLq8aDUepAUEOTdMsK3xJ+Qlr1ucoQ4fy+ySbKkFtdKN06RAoK9WULvij0ovdfZ9Pq6cYrUZlDhnndmnzmebZieWWkmm6l47nSbdMFVvlcBknhaejUjYdxTR/KujCgDCNLzUaqD9MMbpMl9pKBw6Ym9kn+gt0sET/jqVmn7fKlBT2nkfyWbLf/1Lcv0qFg9yQRlzhrT6EamZb39UCmklP224b7URGn3ImnbfOnvH6WkM5mP2fyka96UOt/mvfL5igNrpSmXm9t3/CrV6Zz3use2mhObTV+ZroOSJJvUqI/UcYSp+CjPJ43lVXqK9M8iadv30smd0hXjpfrdvV0q33HoD+mTyyRZ0ojvpSpNpVUfSes+l1LPmnUia0sX3Wta9IIjvFpceFlynOmWvfKDzF5MjfpIlz0jxR02M9zYU6WGvaXBM8vv72XuHdLm2VLdi6Tbfyr43PBc9jRpxw/mf7h7UebyiFpSx+HmmBZVx7NlLqqD66RPLzNl+1fZTpZMkJ6PUh2kOxzS602kxFPSbT9yklAW7PpVmnGDZPOX7vldqtbCveef2Wu6wq//IjNbdVC4aQHseqdUpYnHiwwvSjwt/f2TqZzZ9aupYXcKjTa9bOwp5sAuSZf8n9TrMfcP7mWFwyF9eqmp4Gx/qzTww8I9Lz3FfMbrv8h+chMaLbUbbE5u3P2vonRJTTTTHW37Xvr7ZzNUwsk/WLphUt69nsoTe7r0ySXS0T+ltrdIN0zOfCw51gQIqz6S4o+aZcFRUpfRpvdXRHXvlBnekZZszleWvZFZCVqro3T5syZId9q9WPpyqAnga3eSbp0jhUV7o8Tes3+V9FlfSTbpzkVSrQ7n93qn/pHWT5M2zDAxhGQq85teKXW+XWpyuXeHd/35jfSfMaaxatR875WjBBCk56NUB+mSyU68ZY7U81Hpsn97uzQ4H+mp0kfdpVM7TQvDVROK/lqpCab1b/UkkxzLqWpzqW5XqU5Xc125KV3iS5uY/Saz//b5GZnIs2T4r1hPat7ftPDWvch0YbMsadHL0tJXzTpd7pD6vVo+x1f/Mc1MaxgcKT3whxRezf3XOLNX2jDTdJePO5S5vFYHM24uopYUWdO0FEbUlCJrSWGVy2/FSGmWctYE5Fu/MwF6WmLmY+E1TBfc2IOm54okXfmS1O2+8v1dr/pY+ukJKSTKjJnN7T+WniL9+bX0+7vmeCeZWUzaDTGJUqlMLtvs6SZJ5+KJUtxBs6xyU9Ny3uLa3P8/B/+QZt5oeohVbSEN/9bsZ8sDh8NUfB3ZaMaTX/e+5147PUXa9l+T42jvsszlUXVN5XOH4d75nBdNkJZMNGUY8F7Jb78EEaTno9QH6RtmSt/da2of71xU8PrwXb+/K/3yb5PY44E/zEnO+bIsUwu9epJpcdU5f++QilKdLhmBexdTS03XeN9iWdKxv0xL7vb5poUqq+ptpBYZgXn11nkHCKsnSz8+LsmSWl4n3fBJ+eqqnXTGJItLPCX1fdkEU+fDYTdTJK6fZjIQO9LzXtc/KDNgd16fezu8BmN0fUHSGfN9bv3efL/2LHMNR9WTWg4ws2zU6WIqOB12M2fxmowW4wvvNr+v8lgJFndYer+r6dLe/y3TIpcfh8NUcCx/Wzq4JmOhzezPeozNfygKSh/LMgHhby9IJ/82yyJrm2zk7YYUPC76+DZp+vXS2SOmQnrEd+Uj8/eGGdJ390lBEdKD64tWuVwYJ/42wfqmWZnD5Wz+Jnlq59ukRpeWXKPO3DEm59Llz0kXP1wy2/QSgvR8lPogPe6w9GYLSTbp8d2lrwuQPV3as9ic8HgiKC2tzh41AURqvHTdh1KHWz2/jfgT5kTowBqTEOvQ+uzdoyVJNtMa6Arcu5o5Kstzy5C37F9tutZun29ab51sfiZTcvNrpOZXu5ehdct/pP/caZLyNOgpDZ5VfiplfnhcWjPJ9Ca5e7lnc3jEHzfDDeIOmn1y3BHp7GFzO+FE4V+nQlVz0lq9lVSjjVSjrVSjdfneN5aEhJPmf7b1ezO9ZdYKl+jGmYF5rQ657wsty4yp/SWjN1vz/tKNn0qBoSVTfl/xzUgzh3OdLtLtC9w7od+30kx76OyVIEn1e2RmhOcYVLrtXiItfE46vN7cD42Wev7L9OwKDCn865zZK30xUDqzRwqvLg37j9lHllXJcebcMOG4dMULJulicUtLNr2H/vjcJKB1qljf5JDoMLz4KgqcPrlMOrROunm62f+WYQTp+Sj1QbokfXCRdGKbNOhzqfUN3i5N4aUmmIQgf/8kVWspjV5QfhOC/OdO0/2vKCc3RWVPk45uNgH7gTUmgI/Zn3O90OjMlva6XU2vjeDw4i9febb+C+n7BzLvB4SYrP3NrzGZWCtUKfpr715skhOmxptA8Na5ZX8s6LG/pI97mqEBI77LPt6xuKWnmvG3cUdM9/izRzIC+cOZt88eMYmR8lKxvlSzbUbQ3sZcImsTuJyPuCMZgfl30r7fJcuR+VjVFubEsOV15thU2M95y1zp27vNd1mnqzTkK6lC5eIpv6/Z+Ys0c5BpebtrifmNFsXx7dKKd82YVGeG72otzRCwhr1MCyq/+9Lj0Hrp1/HmuCOZ6WK73Sd1v7/olY9nj5ncPce2mNe4dY45NymLfnnGVF5FN5buXVXyva2ObzOt6xu/lFIy8hz5BZhzkc63Sw16Fc/56isNTGv+3b+X7UoYEaTnq0wE6T//n7TyfTM10HUfeLs0hZNwUpp1i6kpc7qgn8ncWd66Ce5bKX1+lSSbNOY3qXZH75Xl7NHMgP3AWpNgK2t3T8mchNXvbk5gm/cvP+PCSoplmZrz0/+YgLz9rWaqlKAKntvG4Y3mhDrhhGmJH/5t2e02aFnS1P7SvuWmNfSW6d4uUU6WZbrhxx2WYvZJR7eYCrSjf0qxB3J/Tmh0ZsBes525rtzU96bS8UUbZkjf3a9sw39qtjO/j5bXmWzkRbX3d+mrISZJWnRjadicsvvfckpLkj640Px2u90v9X3p/F8z9pC0+iNp3dTMjPCSFFbFHCNrdcy8Dq96/tuDZ53cabq1b/3O3PcLNAkCe/7LM62wSWfMOeSB1VJgmHTLDHOcLE72dDM1bkk1Upz6x/yvHGnS0G+kC/qWzHZzk5popib94/PsU5NGN5I6jTLnKefTeJBVtunXDnv23McHEaTno0wE6bsWSjNuNC0rD//l+7XMp3eb8p7eLYVWki59WvrpKRMMXvywGYNSXjjs0uTe5oS840hpwLveLlF26akmUHAF7muyJ8uSzdRgtxhgWp4q1vNaUc+LZZkDQ1i09/8/e5ZJ0/qbrPz/2lF8JwSn/jGtEWf2mm7Wt86RarUvnm1JppfGivfN/urS/5Na31h828pqy1zTYycgVLp/Ten7jSaeNi1GR/7MCNw3Sye2Z08Y6BQQYloda7QxXbPb3lzmT3DcZk+T3mptejfU6iC1usHsu9wZNlKQ49tNJVjsARNUDv1GqtPJc6/va359wcxvHVlbum+1Z3vEJcVI6z4z3eiP/ZV77oeouua7dAbttdozRMSbVn1kGo8suySbmQGjzzipUn3Pbic1Qfp6uPTPr6YS4MZPimeGhbjDJunoH1NN5dugKaYlubh9OcRMmdbkcnN89va5idPRzRlj17/OrEDzDzJJ/9oPPf/hq4f+kD651ORs+dd2jxTZlxGk56NMBOmpiaZriD1Fum+NVLWZt0uUt0PrpVk3mxa8ivXMeKIqTTOnW5BMQqu2N3unfPZ0M8VS7U4lM75/7afS//5ldmgPrPdcTWRxOr3HJH/Z9n32GlVJqtk+Y/zmdaUjQ++Jv83sCJtnm0qjK543YyC9yTkXaklU2pw9ZjLmHt1sKgUGz/R8V/BjW6Xf35Y2z8kMLG3+0k2fm1bL4pSaIL3fxVQs9XlK6vNE8W6vpKQlmyFORzdnBu/HtpghDFlFNzZTYHk7AVfsQdPi1eI677f0b5svfX2rqZh6eGvxdR89e1SaeZOp5AwINb/3Zv2KZ1uS+U1smWt+Cxc/XHJDWE7skD7qYVr7bplhTtSLS1qy+Z0fWm/GNh9an5GALJfT1ioXZG9tr9HGvbHPKJrkWOmN5mYmhAuuki57Vqresvi2l54qfXunaeW1+Un93zbjps+XZZn8FGs/NbOpZK0U9Qsw56nFObzUOR2vX4B0zwrfPK9PTTD7nHWfmV6XWUU3MhVnNdubSrOa7QofuP85W/rPHSYnxW0/eLrUPocgPR9lIkiXTCKN3YukqyZKF93j7dLkbudC6ZsRZq7LGm1NzWDWE4mFz0nL3zJzzt72Y8m3PKSnSnNuM+MUo+pJw+ZKVS8ovu0lnpbe62i6bV39utR1TPFtq7jEHspMuLR/RfZxndVaZrawuzOus7jFHjQHls2zzQltVmGVzYm7t07mEk+bExx7SsbQhxL4DyTHSV8NNdOv+AdJ10/yzMnHvpUmOP/7p8xlDXubA/W2703Lx+CZxduF79fnzRy8FeuZCsyynMjL4TDJlI5mBO0bvzTJ62z+potp78c9myyvUGWym5PchePNft8XKkpm3iTtXGAq4654vni3lXJWmj3K9B6x+UlXv2YSZXlSzH5zkvzHtMy5pqtcII36X/End8o6lOSCq8wY/JLezyfHSUc2ZQTtf0iHNkixueRW8Qswx6Hqrcx+wC/Q/B/8AjKuA00FUq7Lg3I+VqGKOY/xheNawknT2luzrbdLkjGLyGMmr8O9K0vm83HYpf89Ylp3pfOrbE+KMdPDrZ2SOT2gZALGzrebKRk3f2P+zwM/Mr0EPM2eZiq+Tu44/+l4S8rhjaYr/D+/5Z7bSDKVxrXaZwTuHczvNbfAffEr0uKXS9cQ3vNAkJ6PMhOkO6fvanKFGQPnazbMNImwLLvU6BIzLvTcLnEOhwkW/v7RTEd05yIzNVFJSE81J1M7/pe5LLSSNORrqd6FxbPN/441O7XqraU7l3i/hel8xZ8wAfu276U9S93PkFycEk6a7pKb55rKBCe/AKnxZabr9a/jTYvr9ZOK58BbGKs+MtM5VW8j3b2s5D6ntGTTGrH1O0k2E0wUpdLI4TAB0PK3pAOrMhbazHffY6xp1XLYTa+ZLXNNhdzQr6XGl3jwzWQ49Y/04UUmidctM820TuVJ0hnph8dMZZRkToxumFxyLTLHtpp9fta8I+HVpbFbvDfVXMwB6e02kizTc6ly4+Lfpj1Nmv+wtCEjF8LFD0uXPnN+yZYsy+xj10w23WGdlaNRdc1+9+wRM4vByPnFO15745fSvLtNT4H7Vnu+O3NRJZzM3tp+eL17sywUVv0e0uXjpbpdPP/ahZEcZ2YVWPm+abkeOlu64ErvlEUyv8sPLzLDcfq9Jl14Z8lue+FzpmJYMv+zy54t/DH0yCZTobh5jvksJdO7rN1gqfPozN4ADrv034cy/s826dp3PNNyn9Wqj6WfnjCNBg/8Yc5FS5OEU2ZO9yMbTfB+eGPuFWdSZuDubHWv2U764VGTSPmyZ6Wej5RUqb2m1ATpH330kT766CPt3btXktSqVSs988wz6tcv7y5is2fP1r///W/t3btXTZs21SuvvKKrr7660NssM0H60S3Sxz3MwfLJfb4z/7FlSUtflxa9aO63HSwNeC/vk7SUs9KnV5iunLU6mBb14m79Sk8xU8f8/aMJGq57X1r9samVDwgxWfObF/43VSiHN0qT+0iypFE/SA16ePb1va2guYZbXGsuNdoU35jrlLNmbvHNc0wZXN3VbObkqs2NUsuBmcMalr5uEt3U7mRasUuaZUkfdjO/fW/0rHDYzTzqaz8193s9Jl3yf4U7ybGnmaD793ek41vNMv8gc4LT/aGcQx/saaZSbPt8k/Rn2FyTjNCTZt1iWvEbX2qG1fhCi5c3bJkrzX9ESo4x+7PLx0td7yy+GSTSU8x/aflbpgt0UIR0+bOmR8PZI9KNU6Q2g4pn2wVZNEFaMtFMPzhqfslt17KkJa+a1iFJanOTaSFy9zidEi/9+ZW05hMTCDk17G2+02b9TPK2z68xvSiqtjDvsziGUSWelt7vbBIe+vpcxpZlelAdXi+d2mX2P/Y08/u0p2dcZ12Wy2OO9OzLT/6deVxr3t8EFMXZ8y6r9FTTe2Lpq+bzdyrp3/W59v4uTb3a7NP/td07eQGWv2WCdUnqdJt0zRt5JyNOSzaV92s/zT50r1pLk+Su7S2551dwOExvAeex0pPH64RT0nsdzLCB/m+Z1vuyIFvgvkE6vCnvwN0vwPzfbpomtRpYgoX0jlITpP/3v/+Vv7+/mjZtKsuyNG3aNL322mvasGGDWrVqlWP9FStWqFevXpowYYL69++vWbNm6ZVXXtH69evVunXhUvaXmSDdsqQ3mknxx6QR30uNenu7ROZA9sOjprVYki5+RLrsmYJPlk/vMUkjkk6bFs4bpxTfCXZ6ikk8svNncwI7eJbJEJqaIM2+zSy3+Zkdvad2lg6H9Flfk4itzU1mPt2yLOWs6SK27XszTY+zltoprLKZYqpiPXOpVD/7fXcqadKSpV2/mMD8759MJlanmu1NcNDqBimqds7nxp+Q3mppWl7v+K3kh1scWCNNucJUtP1ruxRasWS3L2VUqr0mLcrIztxxpHTNm3n38khNNC0KK97LzEIeFCF1vs1008sv8396iuk5s2uhec6I7zz3mf+9QJp1U8Z4vpUld/Lsq+IOS9/dZyqrJJN34LoPc/8fnI99K6X/PpgxTlhSs6vNCWxUbWnxRGnxBKleN+n2n/J/neLgsJtW9LhD3qso2DDTfD6OdBNQ3TKjcP/zU/+YwHzjTCklziwLrCC1HyJ1GSNVa55z/anXmEqRaq2kkd97PlD//kFp/TRTEXD3spIfSuFtMQfMb3rTLNOTweZnuuf2GVd8PQAdDumv/5hhPDH7zLLKTUxG/f/9y1RE37XMe93eZ99myuftJLh/TDU9FWWZ4/31k7I3DJ3eY85L10/PHCLiF2hypHS5Q6p3UcHnnJYlLXja9GKQpCtflLo/kP9zCmP+I9K6KaaH5V1Ly/ZsRwmnpCMbTKPVkY3ZA3ebn3T/upLp7eRlpSZIz010dLRee+01jR49Osdjt9xyixISEjR/fmbN4UUXXaT27dvr448/LtTrl5kgXTLzs2760nQrvWK8d8uSmijNHW264hWl++yeZdL0geZk5tJ/S70e9XwZ05Klb4ab7rkBIWY8XdZut/Z0af7YzG6KvR6XLnnq/CsMnF0EAytID6wruS79viA10WRi3fZfE7A7D5D5qVAtS/CeEbhXdAbydc2Y271LTVf2bf/NnMtTMlNStRkktR5UuER2/7nLtFS1HSzdMKno77Mo5t0nbZwhtRsiXV+4/VexWfe5GeNnOUwr0Y2fZq8sSTxtgoY1kzJbcipUNfkwOo8ufAVDWpIZI7x3mWl1GTn//E8w01NMl8vTu81J05Uvnt/rlRWWZVp+FvxbSk8yn/c1b3omWE2ONePO100x9ytUM/v8ltdl7i/PHpXeamX26d4IJP7+2SQtDY02lWDe6m2261fTcyv1rAlwb51t9mPncjhMBdaaSebaKbqxaTVvPyT/lsqTu0ygHn/UnPCP+N5zc7bvXy19ltGt+rYfPd8LpjQ5vs1kt3cOlQsIkS68y/Qs8GQ35X9+k3551uSckMzQkT7jpA7DTSWqM0BuP0wa6IVxvPHHpTdbmp4Gdy013Za9act/pP/cacrT5HLppqnSvhVmH7jzF7mSDUbWMZXKHUe4n8PBskzvu2VvmPuXPC31fqzoZT66RZrU0xx3R86XGvYs+muVVs7APTjSzBxUDpTKIN1ut2v27NkaOXKkNmzYoJYtc2aHrFevnh555BGNHTvWtezZZ5/VvHnztGnTplxfNyUlRSkpmV1v4+LiVLdu3bIRpDszpNdoI9293HvlSDglfXmL6T4UEGJO8IuS8XXtFBMoSKaF25NTXqQlmwy/uxaalsuhX+We1dqyTG35konmfodhUv93ij5+PDnOzIGdcNz3uwiWhKQY0wJ7Zp9JNhKz37QQxOw3y7LOj5uXwAomKZVTRC3Tlb31IHOi4E6linPqD/8gk0CupObfTY4zPWHSEqXbfpLqdyuZ7eZn6/cm07w9RarXXRrypckevvJD01Lh/MwrNTDBcPtbizY0JSXeZLE9sNr0qhj1Q86WQXcse9PkFwivbmriQ0r5ft3TTu40J6+H15v7rW4wPYWKOpvF9v9J/3vUdK+WTNBw5Qu5ByiulrYRZthTSXJOZ+SpebzPx5E/TeVU/FEzzdDQbzIrLZJiTIv5mk9MIkBJkk1qeqUZ49vo0sIPVTi5MyNQP2byXIz8/vxnLbGnSZN6S8f/KjeJnQpl/yrTzXr/SnM/JMr0HrzwrvMbsnd4o3nd3YvM/aAI6eKHTE+lrNMrHlgrTbm85I9dTs7hYnW6SHcsLHj9krBroekpmZZozkWz9qxrfJlpNW965fnnA1ryWuaQzp6PmmmF3W3MsSxp2rWmwrrlddLNX5xfmVBqlKogffPmzerWrZuSk5MVHh6uWbNm5TnGPCgoSNOmTdOQIUNcyz788EONHz9ex44dy/U5zz33nMaPz9nKXCaC9Pjj0utNze1HdxZ/ZtfcnN6TMQf6P1JIRZMUqt5FRX+9/z0qrf3EBGJ3/GIys56vtCTTzfaf38zYqaFfSw175f+crC2LTa80tbJFmX/45/8z3aMqNzHTavhK7gBfZFlmbPu5wbszgI/Zl9l1PjTajF1qPch0pz2f8bafXGqC9eLqwZGbdZ+Z5FJVmpkETL4yfnrvchPcpMSZOZDjj2UmBKzeRrp4rBnXf74nOcmx0rQBpstbeHXTOleUbm5xh6X3OpsKBG8mAPR19jTT+rPkVdNFNqKmycXR5PLCv8bZY2Zc5tbvzP3oRiaJUn770n0rpc+vyhjSsa3kEiLFHTZzo1t235mmNOaACdRPbDOBV7+J0sF1JmGSc78WEmUqPbqMNp9vUZz42wTqCcdNBf6I8wzUnUlqQ6NNJZinWufLAssyw6wWjjffq2Qqjfs8aSox3dlPnt4j/faimSJUMt2xu9xhcoXk9Zk7j10lPYuCwy69085UuA/82PTy8BX7V5uhT8mx5py0wzAzdNHT3aid/wvJVARe+aJ7x/Gt35uenf7B0v1rfScJI4pdqQrSU1NTtX//fsXGxmrOnDn69NNPtWTJklxb0osSpJfplnRJ+vhiM/2ON+YaP7zRnHQkHM+YwmzO+Z8M2dNMK9uepaab85hF5ze2LjVR+mqItHuxCdBvnS01uLhwz93+gzTndtNVtFZH81x3ynJ8u0nu50iXbp0rNXXjhBg5WZbpdh1/TKrS1HNjIjd9JX17lwlKH/qzZLLuT+ptAtQrX5K631/823PH0c2m4i0+Y5/aoKcZUtPkMs9WJiSeNtM5Hf/LdEG8/Ufzn3fHnNHmpLbuhdLtP/tOZYevOvSHGeLhnGqoyxgzfVFQWN7PsSwzBGjB0+bE1+Yv9XhQ6v1EwS2GlmWOUce2lOxv3dnSVa+7+V35iqQY6ethpvUsq2otTZf2tjcXrTL4XCd2ZATqJ8y0YSO+K1qgHnNA+qCrqUS47gMT8CAnh91Utix6OTNXR5ULTE6e5v3z3y8lnDR5QdZOMV21JanNzdKl/2d6LeVn8xwzzDC8ujR2c8k1Auz4yfSeDK0kPbLd9+ajP7PXHMeaXF68iYid089JZl/a79XCNRikJUsfdDENEL0eMy3xKDfcCdKLKd1r4QUFBalJkybq1KmTJkyYoHbt2umdd97Jdd0aNWrkCMaPHTumGjVq5Pn6wcHBioyMzHYpUxpfZq6dCYJKyq6FmbX11dtIoxd4prXCP9BkeKzU0OzAvhlhMpsWRWqiOZDsXmxa5ofNLXyALpkM7yO/Nweiw+tNkq/Tuwv3XMsymbMd6SaZEgH6+bPZTItC9ZaeTVrUcqAUVsUkmNrxg+deNy9HNpkA3T/IjEf3NTXamO6LvZ+QRi802YObXu75ADgsWhoxz+QRiDtouv7FHS788/f+ntHqZDMnRwToBavdyYwf7ZoxVdLaT8yYyEN/5L7+qX/M9/L9AyZAr9leunOxGbpTmJNfmy0zN8naT8246+LmcEjrM7qOenqqpPMVWtEch9oNMZUdLQaY+c3vWWHGyXoiQJfMsXjkf81+7eif0vTrTS8ld/34hAnQ63WX2g31TNnKIj9/qf1Q09PgypfMOcPJv02FzJQrzL7qXKkJpjLpnfZmdhlHmpmZ4q6l0o2fFBygS6abdERNU6H617eefld5c+aiaH+r7wXokvnsWlxb/DMFXXin6U0km9mXzn/IVNgUZOX75vw2ohZDIJEvrwfp53I4HNlavrPq1q2bfv3112zLfvnlF3Xr5gPjOb2l8aXm+p/fTGBYEjbOMtMdpcabqWBu+yH/rM7uCos2Sd2CIqR9v5uM8e6+t9QEkzRoz1Iz92VRp3yq21Ua/YvpKXB6tzTlSjOdREG2fS/tWWK6MvV92f3touQEhmSezK+ZXPzb+2OauW7e33e7jlasZ5ImFvd8wOHVTEVYpQam9WPaADOMpyD2dFMJJkmdRpl5V1E4QWEmyduw/5gT/FO7zDSYiyaYnkxSRvf4N6WPuptW34BQ053zjl/dTwDX5iYpOMqMt/7n14LXP1+7fzMZg0OiTBDjawKCTaLIp49Lt0w3FcfFUcFUrUVGoF7ZVApOv8G05BfW9v+Z5Gh+AVL/N4tvCr+yJDDE9BZ5aJMZqxwQanL1TL3a9Do8usX8t9ZOkd7tYHp7pJ41uVSGz5OGf+teAjb/jC7xkrTqw5I5BzyzNyMRm8rOdGHno9MoaeBHJjv5+i+kefeY41Ne4g6bfatkEj57qmIOZZJX97rjxo3T0qVLtXfvXm3evFnjxo3T4sWLdeutt0qSRowYoXHjxrnWf+ihh/TTTz/pjTfe0Pbt2/Xcc89p3bp1uv9+H+suWpLqXWQOBPHHpGN/Fe+2nHOgz7vHtBC3uVm6dU7xJGqq1lwa9Jkkm5n2xZ3gKSU+M4N0UIQ5GT2fxFxVmprx8TXamO6Dn1+TPfvuuVITzVh0SerxkBTdsOjbRsnofHtG1vhl0rGtxbed1ARp82xz29da+bwlspYZNxtZx3TD/mKg6Qqfnz8+N12oQyqaXAJwX5PLTAtu6xvN2O0lE00l5NbvpcmXmGR86clSo0uke1eaZIFFGQoSVCGzm/SaTzz7HnLjrARrO7j4W9LOR0kMq6ne0gTqodGmN9iMG0yPiIKkxEs/ZFSCdX/ABPwovJAo6bJ/Sw9tzDy27Fxghn683dbku4k/Zionb5wijVmcfaYZd3S6zSRJO7LJJLMrbus+l2SZ/UI5mC6rUNoPMQmTbf5m2MPc0ZkVnudaON7kUKl7oanABPLh1SD9+PHjGjFihJo1a6bLLrtMa9eu1c8//6wrrrhCkrR//34dOXLEtX737t01a9YsTZ48We3atdOcOXM0b968Qs+RXiYFBGd24S7OLu8Ou5mX87cXzP0eY3PORelpF1yZObXcT+OkfxYV/JyUs9LMQaYFPjjS1EzXu/D8yxJRw2Shbtjb7GBn3WKmVsvN8rfMuLSounRlKi2i6mTOJrC2GAOJv+aZpGwV60sNCkheWJ5Uqm9a1MOrmzHq+QUTCadMgiXJjOXz1d4IpUFYtKkMvXGKCSwOrzfJjI5tNl12B35s9qHnW9HYJWNK1Z0LCj9kqCjij2cOWaESzKieMW96aLQZ1jD9BjO7RH6WvGKGoFSsZ6YiRdFE1JD6v2USg7W6XpJlZkUIq2yG6Ny31kyJeD69FCpUltreYm6v+tAjxc5TekrmFLXO/zSM1jeaDO1+gdLWeWbaxfRzegUfWGumfJWkqyYyRAsF8nriuJJWpuZJd1r1kfTTk6Zmc8S84tnGogkZ05LZpH6vmGlGSoJlmfng//zKnESOWZR37W1ynGlBP7AqM0Cv09mz5UlPlb67N7M19LJnzLQrzp3t6T3SBxeaaaxu/sI3u1sid3uWSdP6mwSDj2wr/Nzf7phypZl6rCQzyZcmx7ebrqGJp0xLw7D/SMHh2df570NmSrjqraU7l5RMi2R5EHtI+u4+M/VT60HmJNKT0zrNuNH0QCrOKdGWv2Wmr/KlaaF8xZE/pS8GmLHpdbqaIWC59YI79pf0cU/Tu2LoN9IFfUu+rGXV4Q0m4W7rGz3bA/HYVumjbqbL9YMbiy9T+J+zpf/cYcZSj93Mvjc3O3+RvrrVnAM2uVy6ZYbp0eNwmCnzDv3hvbnt4RNKVeI4eIBzXPq+FaartacdWCMtfdXcvu79kgvQJRP8XvuOOelKjpW+HJx7C1tynDkJPLDKjH8cMc/zAbpkeg5cP1nq/qC5/+vz0g+PZSYL+fkps3Nu2NskBULp0eBiqWoLkyhpUx69JM7H8W0mQLf5kyU5L9Wam7GZIVHms/pysJlC0enwhszuzP1e5STRk6Jqm4rNx/dIg6Z4ft5lZ7K6DTOK5zjlcGT+NjqN8vzrl3Y122b8typKB9eYHmcpZ7Ov43CYqSEtu0m8RYDuWbU6mASBnh4iWL2l1KiPmTK2OHuCORPGdRrFvjcvTa+Qbv3GVPbvWmhyI6UmmG7wh/4wOZIue8bbpUQpQZBeFlS5wEwfZU+R9q/w7GunnJX+M8bs/Nvc5J3gIjBEumWmeY8n/zbTomXNoJkca7rHHlxjTu5HzDNZjIuLn5905QumpcmZ1XP2SGnbfNPV0i+AbNOlUdZM1Gs+8XwmamfG6QuuMt0gkbuabaVh35p8EnuXmQzJ6Snm+/jhcUmWaelt0MPbJS17bLbzm1M7P00uN8M8kmMy54L2pL3LTHK64MiMrsXIoVZ7c3x0VoLNOCdQ3zDdLA8Kl656xVulRFFcdK+5/uMLk1PA0479Je1faSqZO47w/OuXJY36mJ4qQREmefH0G0wPH8lMuRZR3ZulQylCkF4W2GyZSUcKM27bHT8+abJ5RtWVrn7ds6/tjojq0uBZJkneroXSLxk1kUkxZnqZg2tNC8GI76XaHUumTBfdY8Zz+gdJ2/4rfW0SHurCu02LIEqftreYnhin//Fsjoe05MzWecbKFqxOJ9Ma4fy/z7ld2jjTVMQFVjCVZChd/PwzM1Gvmez5TNR/TDXXbW4iY3J+anUwLerBUabn2cybTVCXcDLzuHrJU6ZnBUqPJldI0Y2llNji6Qm27jNz3fwaz87mU1bV724qxJz/s/ijZmrhi+7xdslQihCklxXFMV/61u+kjTMk2UySuOIYo+uOWu2lgRmJUVa+b8biTx9ouhCFVjLJcUp6KqbWN2SMm83ovlahmplfGqVTcLjUIaOyxZPTsW2fb8aCRtY2LYooWP3u0pAvzTSG2+ebubolM5Y/spZ3y4ai6TDMZKI+utkMo/KUhJPmNyJRCVYYtTtKI741x639K0yX3B8fN70cqreRupbgkDZ4hp+faSCQMuZd92BPsJR4adPX5jYJ4wqvTufMpI2SdNUEk+wZKCSC9LKiUR9JNun4VjMP4/mKOyx9nzHu+uKHfadraesbMrPN/vSkGaMaGm2mmXFnflFPathTuv0n0wp787TimZIOJcfZ2ufJTNTOVr4Ow0yLIgqn8SVmLmm/QEmWaSnqdp+3S4WiCos22awlz46d3fSlZE81rcTeOg6UNrU7mRwEwZFmNpQtcyXZpGvfZrxxadV+qGm5PbUr/2li3bX5GzOfe+UmJt8OCq9We+m+NSbpcbN+3i4NShmC9LIiLNqcoEjn3+Xd4TAZ1ZNjpJrtpT7jCnpGyeozTmre39wOq2wC9BptvFum6q2kGyab1j+UbpUbZ7R2W9LaKef/eqf+MeNlZSNhXFFc0NcE6g17m/8YLRGlW5eMvA9/zTNTpp0vy8pMGNeRVnS31OmcOXZWMknNiiPhKkpGcLjUcbi5vfojz7ymleU42Pl2cu0URXjVkhuGiTKFIL0saeKhLu+rPpT2LDHjQW/8tHjnQi8KPz9Trmvflcb8JtVo7e0SoaxxZaKefv6ZqJ0J45pcZuYdhvua9TPdBgkgSr9a7c0UYI60zOD6fOxbIZ3aaXIVOFvpUXh1u0qjF0hXvmQuKN26jjFTsf3zm5lR5HwdWCMd22KGqbQbcv6vB6DQCNLLEudUbLsXFX080tHN0q/jze2+L0lVmnqmbJ4WGGrGHlZq4O2SoCxqcrn5bSXHmq5+RWVPkzbOMrdp5QMM5ywK6z4z/5HzsT4j0G9zoxQccX6vVV5Vbyl1v18KCvN2SXC+KjUwyd0kMzb9fDmnXWs9qPhmfgCQK4L0sqROFzN1SuIp6egm95+flizNHWPG9l3Qz3RtAsojP//MbrlrPil6JuodP0oJx01CQcajAUbL66QKVaWzh6Xt/yv66ySeNt3mJeZGB5wuzMggvulr8x8pqoRT0l/fmttdOB8EShpBelniHyg17GVuF6XL+8LnpBPbzMnTgPcYe4TyrcOtZsjHsS1mftiicLbytR9q/p8ATF4BZ8+StZ8W/XX+/Eayp5iM5LUY8wlIMrlxarSV0pMyk5YWxcYZptGmZnuTaBBAiSJIL2ucXd7dTR6369fMRCPXfWgSXQDlWWglqe3N5nZRpmOL2W/+V5LUcYTnygWUBZ1vk2z+Jqnisa3uP9+yMgOQTiOpVAacbDbponvN7TWfFG1IicMhrfvc3GbaNcArCNLLGmeQvn+VmduyMBJOSfMyukd1GSNdcGXxlA0obZxjZ7f91/2pDTfMkGRJDXqajPEAMkXVyRw7W5TW9INrTc+vgNDMyjQARusbzDCrs4elbd+7//zdv0ln9pgp3Vrf6PnyASgQQXpZE91IqljfZM7du7zg9S1L+u+DUvwxqUoz6coXir+MQGlRo41Ur7vkSHev26DDnhGki7GyQF6clWCbvjJJGt3h/D+2vkEKifJosYBSLyA4swV8VRGmY1v7mbluP0QKquC5cgEoNIL0ssZmy9LlvRDj0td/IW2fL/kFSjd+YrKmA8h0YcZ0bOs+l9JTC/ecXb9KcYdMl/nm/YuvbEBp1qCnVLW5lJZgAvXCSo6VtvzH3GbWBCB3nW+X/INMr5OD6wr/vJgD0t8/Zr4GAK8gSC+LCjtf+ql/pJ+eNLcvfVqq2a54ywWURs37SxE1TZb2rd8V7jnOhHHthkiBIcVXNqA0s9mkLneY22smF37q0D+/MUmxqrYw83wDyCm8mpk6TXKvNX39NMlyZFSiNSuesgEoEEF6WdSgp0nIc2qnSV6VG3ua9J8xUlqiWb/7AyVbRqC08A/MbE0oTAK5s0fN1GsSrXxAQdoNloIipFO7pD2LC17fsqQ/MirBSBgH5O+iu8311nmFy6tiTzM9LCUSxgFeRpBeFoVWlOp0Nrfzak1f8qp06A8zlu/6j8280ABy13GkGRJycI10eEP+626cKVl2qe6FUrXmJVM+oLQKjjDjXiVpTSESyB1eLx3bLPkHS21vKd6yAaVdzXZS/R4mr0phEjRun29yFIVXZ6gW4GUE6WVVfuPS96+Slr1ubvd/y2TZBZC3iOpSq4Hmdn6BhMOR2QpBKzpQOF0yEsj9/WPevb+cnK3oLa+TwqKLt1xAWXBRxuw96z6XUhPzX3ftFHPdcYTpRQbAawjSy6rGGePSdy82maadkuNMN3fLIbUdzNQaQGF1zUggt3m2mbYwN3uXSmf2SsGRmUE9gPxVvUBq2Nscl9Z9lvd6KWelzXPMbWZNAAqn2dVSxXpS0mlp8zd5r3fib2nvMsnmx/8L8AEE6WVVrQ6mK3tybPbuuT8+bloqKtaTrn7Ne+UDSps6XaSa7SV7irThi9zXcbbytbmJaWsAdzgrwf6YJqUl577OlrkmE3zlplL97iVXNqA08/OXut5lbq/62OR1yI2zguyCq+hhCfgAgvSyyj/AtExIZjooyUxZs+lLU0t6/WQpJNJ75QNKG5stM5BYOyV7DxXJtK5vn29ud6KrO+CWC66SIuuY1r6/vs19Hefc6CSMA9zTcbgUFC6d2GZ6WJ4rNUHaOMvc7kzCOMAXEKSXZVnHpccelOaPNfcvfkSq381rxQJKrdY3SKHRUuwB6e+fsj+26UvJnmpa25nOEHCPf4DUJZ9ZFI5sMr3C/ALN1IYACi8kSmp/q7md23RsW+ZKKbFSpQaZ544AvIogvSxz7mgPrpXmjDZd32t1lPo86d1yAaVVYKhJqCNlDyQsK3NudFrRgaLpOFLyDzIZ3A/+kf0x51CSFtdKFaqUfNmA0u7CuyTZpJ0/S6f+yf6YM2Fc59slP0IDwBfwTyzLKtWXKjcx00EdWCUFhkk3fELGTuB8dBlthozsXiyd2GGW7V8lnfzb/MdaD/Jq8YBSq0IVqdUN5vbaTzKXpyaYhI0SlWBAUVVuLF3Q19xe/XHm8kN/SEc2mmkN2w/zStEA5ESQXtZl7bZ01QSpShPvlQUoCyrWM9lyJWlNRiDhbEVvfQO5HoDz0TVjOrYtc6WEk+b2X99KKXFSpYZSg17eKxtQ2jmnY9swU0qKMbfXZiSMazVQqlDZG6UCkAuC9LKuzU2SbKZ1gnmbAc9wBhKbvpRiDkh/zTP3O47yVomAsqF2JzM7iT1VWp8xi4Kzq3vHEXTFBc5Hw95StZZmloQNM6SkM6ZCTCJhHOBjONqVdXW7So/vlgZ9RjZcwFMa9paqXCClxktf3yqlJ5kTnzqdvV0yoHTLOovCus+ko5ulg2skv4DMxFcAisZmky6829xeM8kE6ulJUvXW5nwRgM8gSC8PwqIJ0AFPyhpIHNlkrjsyLRTgEa2yzKIwN6PXSrN+UkR175YLKAva3mz+XzH7pd9eMss6387xC/AxBOkAUBTtBktBEea2f7A58QFw/gJDMmdROLHNXHca5bXiAGVKYKjU+TZzOz3JzJ/O8QvwOQTpAFAUwRFSh4zut62uNz1WAHhG59slZbTsRdWTGjF3M+AxXe4wQ0gkqe0t5ngGwKcQpANAUV32rHTNm1K/V7xdEqBsqVRfan6Nud1pJAnjAE+KrCV1u1+KrCN1u8/bpQGQC5tlWZa3C1GS4uLiFBUVpdjYWEVGMlUSAAA+KfG0tGuhGaPuH+Dt0gAAcF7ciUM56gEAAN8TFs1YWQBAuVSkID05OVl//vmnjh8/LofDke2xAQMGeKRgAAAAAACUN24H6T/99JNGjBihkydP5njMZrPJbrd7pGAAAAAAAJQ3bmdieeCBB3TTTTfpyJEjcjgc2S4E6AAAAAAAFJ3bQfqxY8f0yCOPqHr16sVRHgAAAAAAyi23g/RBgwZp8eLFxVAUAAAAAADKN7enYEtMTNRNN92kqlWrqk2bNgoMDMz2+IMPPujRAnoaU7ABAAAAAEpSsU7B9uWXX2rBggUKCQnR4sWLZbPZXI/ZbDafD9IBAAAAAPBVbgfp//d//6fx48frySeflJ+f273lAQAAAABAHtyOslNTU3XLLbcQoAMAAAAA4GFuR9ojR47U119/XRxlAQAAAACgXHO7u7vdbterr76qn3/+WW3bts2ROO7NN9/0WOEAAAAAAChP3A7SN2/erA4dOkiStmzZku2xrEnkAAAAAACAe9wK0u12u8aPH682bdqoUqVKxVUmAAAAAADKJbfGpPv7++vKK69UTExMMRUHAAAAAIDyy+3Eca1bt9bu3buLoywAAAAAAJRrbgfpL774oh599FHNnz9fR44cUVxcXLaLOyZMmKAuXbooIiJC1apV08CBA7Vjx458nzN16lTZbLZsl5CQEHffBgAAAAAAPsftxHFXX321JGnAgAHZEsVZliWbzSa73V7o11qyZInuu+8+denSRenp6Xrqqad05ZVXauvWrapQoUKez4uMjMwWzJOwDgAAAABQFrgdpC9atMhjG//pp5+y3Z86daqqVaumP/74Q7169crzeTabTTVq1PBYOQAAAAAA8AVuB+m9e/cujnJIkmJjYyVJ0dHR+a4XHx+v+vXry+FwqGPHjnr55ZfVqlWrXNdNSUlRSkqK6767XfIBAAAAACgpNsuyLHefFBMToylTpmjbtm2SpFatWun2229XVFRUkQvicDg0YMAAxcTEaPny5Xmut3LlSu3cuVNt27ZVbGysXn/9dS1dulR//fWX6tSpk2P95557TuPHj8+xPDY2VpGRkUUuLwAAAAAAhREXF6eoqKhCxaFuB+nr1q1T3759FRoaqq5du0qS1q5dq6SkJC1YsEAdO3YsUqHvuece/fjjj1q+fHmuwXZe0tLS1KJFCw0ZMkQvvPBCjsdza0mvW7cuQToAAAAAoES4E6S73d394Ycf1oABA/TJJ58oIMA8PT09XXfccYfGjh2rpUuXul3g+++/X/Pnz9fSpUvdCtAlKTAwUB06dNCuXbtyfTw4OFjBwcFulwkAAAAAgJLm9hRs69at0xNPPOEK0CUpICBAjz/+uNatW+fWa1mWpfvvv1/ffvutfvvtNzVs2NDd4shut2vz5s2qWbOm288FAAAAAMCXuB2kR0ZGav/+/TmWHzhwQBEREW691n333acZM2Zo1qxZioiI0NGjR3X06FElJSW51hkxYoTGjRvnuv/8889rwYIF2r17t9avX69hw4Zp3759uuOOO9x9KwAAAAAA+BS3u7vfcsstGj16tF5//XV1795dkvT777/rscce05AhQ9x6rY8++kiS1KdPn2zLP//8c40aNUqStH//fvn5ZdYlnDlzRmPGjNHRo0dVqVIlderUSStWrFDLli3dfSsAAAAAAPgUtxPHpaam6rHHHtPHH3+s9PR0SWZc+D333KOJEyf6/PhvdwbsAwAAAABwvoo1u7tTYmKi/vnnH0lS48aNFRYWVpSXKXEE6QAAAACAklSs2d2dwsLC1KZNm6I+HQAAAAAAnKPQQfrtt99e4Do2m01Tpkw5rwIBAAAAAFBeFTpIP3PmTJ6P2e12LVy4UCkpKQTpAAAAAAAUUaGD9G+//TbX5d99952eeuopBQcH65lnnvFYwQAAAAAAKG/cnifd6ffff1fPnj01dOhQ9e/fX7t379aTTz7pybIBAAAAAFCuuB2kb926Vddee6369OmjCy64QDt27NArr7yiSpUqFUf5AAAAAAAoNwodpB84cEC33Xab2rVrp4CAAP3555+aMmWK6tSpU5zlAwAAAACg3Cj0mPRmzZrJZrPpkUceUY8ePbRz507t3Lkzx3oDBgzwaAEBAAAAACgvbJZlWYVZ0c+v4EZ3m80mu91+3oUqTu5MIg8AAAAAwPlyJw4tdEu6w+E474IBAAAAAIC8FTm7OwAAAAAA8CyCdAAAAAAAfARBOgAAAAAAPoIgHQAAAAAAH+HRIN3XM7sDAAAAAODLPBKk//3333r88cdVp04dT7wcAAAAAADlUpGD9MTERH3++efq2bOnWrZsqaVLl+qRRx7xZNkAAAAAAChXCj1PutOqVav06aefavbs2apXr562bdumRYsWqWfPnsVRPgAAAAAAyo1Ct6S/8cYbatWqlQYNGqRKlSpp6dKl2rx5s2w2mypXrlycZQQAAAAAoFwodEv6E088oSeeeELPP/+8/P39i7NMAAAAAACUS4VuSX/hhRc0e/ZsNWzYUE888YS2bNlSnOUCAAAAAKDcKXSQPm7cOP3999+aPn26jh49qgsvvFDt2rWTZVk6c+ZMcZYRAAAAAIBywe3s7r1799a0adN09OhR3XvvverUqZN69+6t7t2768033yyOMgIAAAAAUC7YLMuyzvdFNm/erClTpmjWrFk6fvy4J8pVbOLi4hQVFaXY2FhFRkZ6uzgAAAAAgDLOnTi0yPOkZ9WmTRu9/fbbOnToULZlBw4c8MTLAwAAAABQLngkSHcKDAx03d67d6/S0tI8+fIAAAAAAJRpHg3SAQAAAABA0RGkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CIJ0AAAAAAB8RLEF6ZMmTVL16tWL6+UBAAAAAChzAorypISEBC1ZskT79+9XampqtscefPBBSdLQoUPPv3QAAAAAAJQjbgfpGzZs0NVXX63ExEQlJCQoOjpaJ0+eVFhYmKpVq+YK0gEAAAAAgHvc7u7+8MMP69prr9WZM2cUGhqqVatWad++ferUqZNef/314igjAAAAAADlgttB+saNG/Wvf/1Lfn5+8vf3V0pKiurWratXX31VTz31VHGUEQAAAACAcsHtID0wMFB+fuZp1apV0/79+yVJUVFROnDggGdLBwAAAABAOeL2mPQOHTpo7dq1atq0qXr37q1nnnlGJ0+e1PTp09W6deviKCMAAAAAAOWC2y3pL7/8smrWrClJeumll1SpUiXdc889OnHihCZPnuzxAgIAAAAAUF641ZJuWZaqVavmajGvVq2afvrpp2IpGAAAAAAA5Y1bLemWZalJkyaMPQcAAAAAoBi4FaT7+fmpadOmOnXqVHGVBwAAAACAcsvtMekTJ07UY489pi1bthRHeQAAAAAAKLdslmVZ7jyhUqVKSkxMVHp6uoKCghQaGprt8dOnT3u0gJ4WFxenqKgoxcbGKjIy0tvFAQAAAACUce7EoW5Pwfb2228XtVwAAAAAACAfbgfpI0eOLI5yAAAAAABQ7rkdpEuS3W7XvHnztG3bNklSq1atNGDAAPn7+3u0cAAAAAAAlCduB+m7du3S1VdfrUOHDqlZs2aSpAkTJqhu3br63//+p8aNG3u8kAAAAAAAlAduZ3d/8MEH1bhxYx04cEDr16/X+vXrtX//fjVs2FAPPvigW681YcIEdenSRREREapWrZoGDhyoHTt2FPi82bNnq3nz5goJCVGbNm30ww8/uPs2AAAAAADwOW4H6UuWLNGrr76q6Oho17LKlStr4sSJWrJkiduvdd9992nVqlX65ZdflJaWpiuvvFIJCQl5PmfFihUaMmSIRo8erQ0bNmjgwIEaOHAgU8IBAAAAAEo9t6dgi46O1vz589W9e/dsy3///Xdde+215zUF24kTJ1StWjUtWbJEvXr1ynWdW265RQkJCZo/f75r2UUXXaT27dvr448/LnAbTMEGAAAAAChJ7sShbrek9+/fX3feeadWr14ty7JkWZZWrVqlu+++WwMGDChyoSUpNjZWkrK10p9r5cqVuvzyy7Mt69u3r1auXJnr+ikpKYqLi8t2AQAAAADAF7kdpL/77rtq3LixunXrppCQEIWEhKhHjx5q0qSJ3nnnnSIXxOFwaOzYserRo4dat26d53pHjx5V9erVsy2rXr26jh49muv6EyZMUFRUlOtSt27dIpcRAAAAAIDi5HZ294oVK+q7777Tzp07tX37dklSixYt1KRJk/MqyH333actW7Zo+fLl5/U65xo3bpweeeQR1/24uDgCdQAAAACATyrSPOmS1LRpUzVt2tQjhbj//vs1f/58LV26VHXq1Ml33Ro1aujYsWPZlh07dkw1atTIdf3g4GAFBwd7pJwAAAAAABSnQgXpWVuiC/Lmm28Wel3LsvTAAw/o22+/1eLFi9WwYcMCn9OtWzf9+uuvGjt2rGvZL7/8om7duhV6uwAAAAAA+KJCBekbNmwo1IvZbDa3Nn7fffdp1qxZ+u677xQREeEaVx4VFaXQ0FBJ0ogRI1S7dm1NmDBBkvTQQw+pd+/eeuONN3TNNdfoq6++0rp16zR58mS3tg0AAAAAgK9xewo2j248j6D+888/16hRoyRJffr0UYMGDTR16lTX47Nnz9bTTz+tvXv3qmnTpnr11Vd19dVXF2qbTMEGAAAAAChJ7sShXg3SvYEgHQAAAABQkop1nnQAAAAAAFA8CNIBAAAAAPARBOkAAAAAAPgIgnQAAAAAAHwEQToAAAAAAD6CIB0AAAAAAB9BkA4AAAAAgI8gSAcAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BEE6QAAAAAA+AiCdAAAAAAAfARBOgAAAAAAPoIgHQAAAAAAH0GQDgAAAACAjyBIBwAAAADARxCkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CIJ0AAAAAAB8BEE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDAAAAAOAjCNIBAAAAAPARBOkAAAAAAPgIgnQAAAAAAHwEQToAAAAAAD6CIB0AAAAAAB9BkA4AAAAAgI8gSAcAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BEE6QAAAAAA+AiCdAAAAAAAfARBOgAAAAAAPoIgHQAAAAAAH0GQDgAAAACAjyBIBwAAAADARxCkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CIJ0AAAAAAB8BEE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDAAAAAOAjvBqkL126VNdee61q1aolm82mefPm5bv+4sWLZbPZclyOHj1aMgUGAAAAAKAYeTVIT0hIULt27fTBBx+49bwdO3boyJEjrku1atWKqYQAAAAAAJScAG9uvF+/furXr5/bz6tWrZoqVqzo+QIBAAAAAOBFpXJMevv27VWzZk1dccUV+v333/NdNyUlRXFxcdkuAAAAAAD4olIVpNesWVMff/yx5s6dq7lz56pu3brq06eP1q9fn+dzJkyYoKioKNelbt26JVhiAAAAAAAKz2ZZluXtQkiSzWbTt99+q4EDB7r1vN69e6tevXqaPn16ro+npKQoJSXFdT8uLk5169ZVbGysIiMjz6fIAAAAAAAUKC4uTlFRUYWKQ706Jt0TunbtquXLl+f5eHBwsIKDg0uwRAAAAAAAFE2p6u6em40bN6pmzZreLgYAAAAAAOfNqy3p8fHx2rVrl+v+nj17tHHjRkVHR6tevXoaN26cDh06pC+++EKS9Pbbb6thw4Zq1aqVkpOT9emnn+q3337TggULvPUWAAAAAADwGK8G6evWrdMll1ziuv/II49IkkaOHKmpU6fqyJEj2r9/v+vx1NRU/etf/9KhQ4cUFhamtm3bauHChdleAwAAAACA0spnEseVFHcG7AMAAAAAcL7ciUNL/Zh0AAAAAADKCoJ0AAAAAAB8BEE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDAAAAAOAjCNIBAAAAAPARBOkAAAAAAPgIgnQAAAAAAHwEQToAAAAAAD6CIB0AAAAAAB9BkA4AAAAAgI8gSAcAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BEE6QAAAAAA+AiCdAAAAAAAfARBOgAAAAAAPoIgHQAAAAAAH0GQDgAAAACAjyBIBwAAAADARxCkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CIJ0AAAAAAB8BEE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDAAAAAOAjCNIBAAAAAPARBOkAAAAAAPgIgnQAAAAAAHwEQToAAAAAAD6CIB0AAAAAAB9BkA4AAAAAgI8gSAcAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BEE6QAAAAAA+AiCdAAAAAAAfARBOgAAAAAAPoIgHQAAAAAAH0GQDgAAAACAjyBIBwAAAADARxCkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CK8G6UuXLtW1116rWrVqyWazad68eQU+Z/HixerYsaOCg4PVpEkTTZ06tdjLCQAAAABASfBqkJ6QkKB27drpgw8+KNT6e/bs0TXXXKNLLrlEGzdu1NixY3XHHXfo559/LuaSAgAAAABQ/AK8ufF+/fqpX79+hV7/448/VsOGDfXGG29Iklq0aKHly5frrbfeUt++fYurmAAAAAAAlIhSNSZ95cqVuvzyy7Mt69u3r1auXJnnc1JSUhQXF5ftAgAAAACALypVQfrRo0dVvXr1bMuqV6+uuLg4JSUl5fqcCRMmKCoqynWpW7duSRQVAAAAAAC3laogvSjGjRun2NhY1+XAgQPeLhIAAAAAALny6ph0d9WoUUPHjh3LtuzYsWOKjIxUaGhors8JDg5WcHBwSRQPAAAAAIDzUqpa0rt166Zff/0127JffvlF3bp181KJAAAAAADwHK8G6fHx8dq4caM2btwoyUyxtnHjRu3fv1+S6ao+YsQI1/p33323du/erccff1zbt2/Xhx9+qG+++UYPP/ywN4oPAAAAAIBHeTVIX7dunTp06KAOHTpIkh555BF16NBBzzzzjCTpyJEjroBdkho2bKj//e9/+uWXX9SuXTu98cYb+vTTT5l+DQAAAABQJtgsy7K8XYiSFBcXp6ioKMXGxioyMtLbxQEAAAAAlHHuxKGlakw6AAAAAABlGUE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDAAAAAOAjCNIBAAAAAPARBOkAAAAAAPgIgnQAAAAAAHwEQToAAAAAAD6CIB0AAAAAAB9BkA4AAAAAgI8gSAcAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BEE6QAAAAAA+AiCdAAAAAAAfARBOgAAAAAAPoIgHQAAAAAAH0GQDgAAAACAjyBIBwAAAADARxCkAwAAAADgIwjSAQAAAADwEQTpAAAAAAD4CIJ0AAAAAAB8BEE6AAAAAAA+giAdAAAAAAAfQZAOAAAAAICPIEgHAAAAAMBHEKQDAAAAAOAjCNIBAAAAAPARBOkAAAAAAPgIgnQAAAAAAHwEQToAAAAAAD6CIB0AAAAAAB9BkA4AAAAAgI8gSAcAAAAAwEcQpAMAAAAA4CMI0gEAAAAA8BEE6QAAAAAA+AiCdAAAAAAAfESAtwsAz0mzO3Q0NlkHzyTpUEySDp1J0qGYRB2JTZbNZlNYoL9Cg/wVEuivsCB/hWbcz+06LGO90KDMdUMC/RXk7yc/P5u336pH2R2WJMnPJtlsZeu9eVtSql2HY5N0JCZZJ+KTFR4cqCrhQaoSHqyqEcEKCfT3dhGLxLIsnU1J18mzKYpNSlNUaKCqRYYoPNg7u1SHw9KphFQdP5usMwlpCg70U4WgAIUHByg8JEAVgv0VHFA6P+uCWJal+JR0xSSm6XRCqs4kmsvphDTFJKbqdEJqtsfOJqcrNMhf4cEBigjJ+IwyPqeIkEBFZNx2LctyPyI4UBWC/RXgT/12QSzLUkq6w1zS7EpOcygl3Vwnp9uVkuZQcpo983Z69nVS0uxKSXfI7rBUJTxY1SODVT0yJOMSrOgKQeyvS5BlWUpMtSshJV3xKelKTLUrPiXddT8hxTyWnGaXn59NNpvkZ7PJL+PaluW281ib/fGMx/zMdYCfnyqGBapiWKAqhQWpUliQQoPK5j7MKc3u0NnkdMUnpysuOc3cTknX2eS0jGuzPD45XanpDkWGBioyJFBRoQGKCnPeNpfIjGtfOsam2x06nZiqMwlpOpWQYvbJCak6lWD206cTUpWcZldYUJZ9cHCAKgQHKDzYX+EZ+9+IELOsQlCA63ZgGdwn2x2WElLTlZhiz/jPpSvNbik4wM91Lh8S4GeuA/3lX8bOzUGQXqokpdpN8B2TpINnEjOC8CTX9bG4ZGXEm8UqwM+mQH8/BfrbFBTgryB/mwID/DKW+Zn7GbcDA8657++noIDMA7ZNmQdnSa7btiy3lbGeObBn3pbNpjS7QylpDqXa7UrNOCFMzbg4b6fYzQlfqt2Ryzr2HJ+Z60TCL+tJReFOOoID/BQU4KfgQH8FB/hluWTcD8y8HZT1scDM20EBfsrc12Z+Ls57zhNT5yqux1zrOD8zmYNdcOaBLDw4QEEBnjmYpdkdOhaXrCOxyTock6TDMck6EpuU7faZxLR8XyM8OECVM4L2Kq7rYFWJCFaVCkHmOuOx8OCAYj0pT0136HRCqk7Gp+hkfIpOxafqVIK5Phlvljvvn4pPVardkeM1woL8VTUiWNUigjOuQ1Q143bW5ZUrBBfqgJpud+hkvAm+j8el6PjZFB0/m6xjcSk6cTbZ3I8z5U0v4M8f6G9zndiYEx9/128i89qcCIUH+ys0KEA2SQ7LkiVzkm5ZkuVcZmUsk6kkMMvNMmVZx2FJ5lHJylLErK/nfMyS5VrHyvIES1K6w1JsUprOJGQJvBNTFZOYqjR7Cez4sggN9HedRFYI9s92suj6LIOcn6ezoiRzueuzDglQkL+fUu0OJaXalZBqV1KqCXgSU+1KTDXBkHksPduyxBS7EtPsSswImJLT7Qr081NggE1BWfa/wa7bNgX5+7seD3LtnzP302bfY3MF1kkZwXVyxnVSmt0E3OmmTM6AOynVBNRmPfO8lHRHtu/b04L8/VQ1Ilg1okKyBfA1IkNULTJYNTLuV8in4syyLCWl2bMFmQnnBKAJGYFpQqq5n5TqyPzdZ/kNO5fJyro86+OZ6zj/I/5+5jjqn3FMNdfmfoCfnwL8bPL3tynQL+tjflnWMfftliWHw5LDsmTPuHZY5iTfLFf2dawsyzOWpdktJabmDLoTMn6PCanpxfp9FkZwgJ8qhQW5AvfoCpm3XcF8hUBVzAjqK4UFqkJwgFLSHRm/V/PbTErNuE6zKznL7ezrODJ+/2Z5qt3hOi+xZZyjZD0vUC7nLs7jtblvjs12y1J8crrOpphg2wTe6YpPSVNyWs5jyvkKCvAzQXtIQI4APirUfD7+Gec7/jbJ389523kOZJN/RsWJf5bl/lkqVPz9bEp3WK5986lzgm/n7dik/M8HzkdwgF+2Y1l4SIAiMypendcRIQGKDM24zrgfERKoyFBzPzjAz+1zDGdlZFJq9v2ic1/p3Ic699vOYDvH/yvLviYx1fwH3f09BPmbc8yQQGfDWkYAH+CvkKDMgD400JxzBvmb89BA/8zzUeftrI85z1dd62ZZ38/PJofD7Hec+xW7cz/kkOu28+LcR7nWzVgmyRUjBPhnOYZlxBrO41lgxv4w6+2y1nCYlc2yvL3bLVlxcXGKiopSbGysIiMjvV2cPG06EKP/bjrsCsoPnUnSqYTUAp8XFOCn2hVDMy+VQlUzKkQ2m811QEpMtbsOPomp6UpKc2QctNIzrh1KSk3PdjAr6ZNgFJ+gjINZtktIZrAWkfV2cICCA/10Mj5VR2KSTECeEYifOJtSqEqhCkH+qlUxVNUigxWfnK6T8ak6EZ+i1HT3DkDBAX6qEh6syuFBCvT3y/eEKevyvE6YLEs6k5iaEYSnKC453e3PMjzYnAjEJqUpIdVe6Of52aTK4VmD+WBVCgtSTGKajmUJyE8lpBT6xNhmkypXMCeuqekOxacU7UBfWoUE+ik6LEgVs5y8m+sgRYcFqlIFc+IeERJgArOMVipnC1V8Srpr2dmMk+Zzl6W4+ZstDJtNXg9+ipvNJnOimHHSGByQeR2c0QrkXBaSUZlp1veXzSadOJuiY3HJOhqXouNxyYU6FjpFBAeoWkbLe3KaI1tLsC8EnqWNzSaFBzkrpEwlVFhQZmuns+XWWVGQWVmX9b45gXcusywrx/qp6Q7FJKXqTKKpmCuoErIsCcvS0yfCFUyanjzhGbcD/f10NjldsUlpiktKM9fJ5tq5zBc/MptNqhhq9s2VKwSrUoVARVcIdh27woL8lZBq9s8JqWa/66w8O5ut54bnj29B/n6uz9oZzAcH+GepfMysvHSeQ5fE8TXAz1nBbnpxOXsbJaXZ3T6PKoucFZjOoH7G6AvVspbvxnfuxKG0pPuo3Sfj9enyPTmWRwQHqHalzAD83OsqFYKLpVYpzW52CP/f3v3HRlHnfxx/7W67/bm7pfToD9oqiQYPFdAiWM8cp/boGc9A0JwXf1AQPY31TmwiCX8IifFS1BynGBQTIxg4T048MGciButRf6FgSQ2ix4mi1EBb8Gvb7WK72535/rG7s7u0/Dpod7Z9PpLJznxmtv0svDud9+f9mWlowFAobCoUNhQMGwqFDYUGzPh6dAkOmCdsGwpG3xcaMKzqmhEtMRgJVQer8mYOrsYlVe9MMzrK50qoSsdH+NyxyrRriLaE4zNdTjkcsi4QjKQLiJNcYMTajHgfw6aZVKHvj72GEqr6Vnt8OuegfSFjUEUxNpaWWHVM3FasipOwzzBN/RQMyx9NNH4KRZLI4ICh/xuIjHCfK7fLqRJftkp92ZpYkKPSgmyV+nKS1r3ZgyvgsSnKsQr1MX+kGnw0YfuHQHw9EK3UxQathovL6dD4PLfGJ1T2Y9uRin/kwmJ89DVx+mWgf0BH/ZHk+mi04p28HXn9IRAZ3Dga3T6TPhXluzXBk60JnixN8MZes6y2Ym+2NXhxooGwEa8GJlTJ4pXC+EVPcns4PhCiUwx+JMyIic+QiQ+IWPui/Uk+PtbLUxwTbXM6HfLlJCbekarZSE6FDQ7Ekzx/9CIylsQnVSBPuLjsTaiYxNpjP4+JSaI7w6lcd6Qyn+N2Kc/tir5mJL3muiOzIHIyXcqLznrIznAqbJjWjKFg9FwbCg/VZkTbzEFtA2HTSpIjUyoj58wct8tKtGPrWZlO61aoE6s3WQlVnEyX47zOgukfCFuJe0dPLIGPDG61d/epw9+nju4+BaLnP//RAX19NHDSr+dwyJr5kJeQgA61nuN2WTOoIu91JP+cOCPbSmpPngkWC+qwYWrAMDUQjkzrD4VNhQ0j+moqZBgKh6PHGJH/m9jxkddIRcoZq4A6HFZlM7ECalU+HbLWXc7I/4nLqei205pdE0u489zxymRuNCHPyXSN+G0Gibe0RG5nidzK8mMgYd3aF5lO3XU8mDRw6nBEZsDE49Vp3d4Xi92kbXc8nnMyXdYMtMigQnyGkXX9Yl3HJM4SMq3jY7/THQ5Zt9XEkvB4Qh75tz4ft9PE/s3iSXs8oU9M5nv7B6LVz9g1zYkVTyW1nazd6XCoMJpoJy7jT9guyHWf12nZobCh4/1h+ftD1u+v2OCqvy9k3TbQ81PImrWQ3BaSvz8yWBcMG/ohWvX/X2S6HPGqdTRuIlXryGtkQMs16GcqMuPRZQ12nXjucbtOXuE3jGglP2EWU2zwoC9hPd4eWawZpeH4zNL474XI9XpwIBxtN5OPDcePHzBM63wSOw/Fzz8JiyN5hkbsvBNrk6SQYVq/iyJLPI8YiP0eCw+eoRWr0scGTEbTXVBU0m3qqw6/Nu1uS0rAy8flypeTmequIU3FkrXE6mBsPTZKnTh6HdkX0vFgWOPz3Srz5ai0IEcTo8l3aUH2sA0KneinYDiaxPdbVZXYhVDyAM8QU61PmHqaOP26ICeaeEeTcm925rB/noFwZFr9icn8j8dDKsjJjCff0dfCvPN7UQN7iN1v2BcMK9vtUm4m97qfb739A2rv7lNnT59+PB5SbsIAR2IlODvDNaqnTI5V/dHpx7GZGjzDAEMxoudif0IC7++LDGz0D4StQcjEgcikwcnoVHLO3yMjMqA5OJGPrVcW5trqWQwnOps8lCQdAAAAAIBhdDZ5KMM+AAAAAADYBEk6AAAAAAA2YYskfc2aNbrwwguVnZ2tWbNmadeuXSc9dv369dEHEsWX7OzsEewtAAAAAADDI+VJ+qZNm9TQ0KAVK1Zoz549mjZtmmpra9XZ2XnS93i9Xh05csRavvvuuxHsMQAAAAAAwyPlSfqqVat07733atGiRZoyZYrWrl2r3NxcvfTSSyd9j8PhUElJibUUFxePYI8BAAAAABgeKU3Sg8GgWlpaVFNTY7U5nU7V1NRo586dJ31fb2+vLrjgAlVUVGju3Lnat2/fSY/t7+9XT09P0gIAAAAAgB2lNEk/duyYwuHwoEp4cXGx2tvbh3zP5MmT9dJLL+mNN97Qxo0bZRiGrrnmGn3//fdDHt/Y2Cifz2ctFRUV5/1zAAAAAABwPqR8uvvZqq6u1oIFCzR9+nTNnj1b//znP/Wzn/1ML7zwwpDHL1u2TN3d3dbS1tY2wj0GAAAAAODMZKTymxcVFcnlcqmjoyOpvaOjQyUlJWf0NTIzM3XFFVfowIEDQ+7PyspSVlbWOfcVAAAAAIDhltJKutvtVlVVlZqamqw2wzDU1NSk6urqM/oa4XBYe/fuVWlp6XB1EwAAAACAEZHSSrokNTQ0qK6uTjNmzNDMmTP19NNPKxAIaNGiRZKkBQsWaOLEiWpsbJQkPfbYY7r66qt10UUXqaurS0899ZS+++473XPPPan8GAAAAAAAnLOUJ+m33Xabjh49quXLl6u9vV3Tp0/Xtm3brIfJHTp0SE5nvOD/448/6t5771V7e7vGjRunqqoqffTRR5oyZUqqPgIAAAAAAOeFwzRNM9WdGEk9PT3y+Xzq7u6W1+tNdXcAAAAAAKPc2eShafd0dwAAAAAARiuSdAAAAAAAbIIkHQAAAAAAmyBJBwAAAADAJkjSAQAAAACwiZT/CbaRFnuYfU9PT4p7AgAAAAAYC2L555n8cbUxl6T7/X5JUkVFRYp7AgAAAAAYS/x+v3w+3ymPGXN/J90wDB0+fFgej0cOhyPV3Tmlnp4eVVRUqK2tjb/pjrRHPGO0IJYxmhDPGE2IZ9iZaZry+/0qKyuT03nqu87HXCXd6XSqvLw81d04K16vlxMNRg3iGaMFsYzRhHjGaEI8w65OV0GP4cFxAAAAAADYBEk6AAAAAAA2QZJuY1lZWVqxYoWysrJS3RXgnBHPGC2IZYwmxDNGE+IZo8WYe3AcAAAAAAB2RSUdAAAAAACbIEkHAAAAAMAmSNIBAAAAALAJknQAAAAAAGyCJN2m1qxZowsvvFDZ2dmaNWuWdu3aleouAaf13nvv6eabb1ZZWZkcDoe2bt2atN80TS1fvlylpaXKyclRTU2Nvvrqq9R0FjiNxsZGXXXVVfJ4PJowYYLmzZun/fv3Jx3T19en+vp6jR8/Xvn5+brlllvU0dGRoh4DJ/f8889r6tSp8nq98nq9qq6u1ltvvWXtJ5aRrlauXCmHw6ElS5ZYbcQz0h1Jug1t2rRJDQ0NWrFihfbs2aNp06aptrZWnZ2dqe4acEqBQEDTpk3TmjVrhtz/5JNPavXq1Vq7dq0++eQT5eXlqba2Vn19fSPcU+D0mpubVV9fr48//ljbt29XKBTSnDlzFAgErGMefvhh/etf/9Jrr72m5uZmHT58WPPnz09hr4GhlZeXa+XKlWppadGnn36q66+/XnPnztW+ffskEctIT7t379YLL7ygqVOnJrUTz0h7Jmxn5syZZn19vbUdDofNsrIys7GxMYW9As6OJHPLli3WtmEYZklJifnUU09ZbV1dXWZWVpb597//PQU9BM5OZ2enKclsbm42TTMSv5mZmeZrr71mHfPll1+aksydO3emqpvAGRs3bpz54osvEstIS36/37z44ovN7du3m7NnzzYfeugh0zQ5N2N0oJJuM8FgUC0tLaqpqbHanE6nampqtHPnzhT2DDg3Bw8eVHt7e1Js+3w+zZo1i9hGWuju7pYkFRYWSpJaWloUCoWSYvqSSy5RZWUlMQ1bC4fDevXVVxUIBFRdXU0sIy3V19frpptuSopbiXMzRoeMVHcAyY4dO6ZwOKzi4uKk9uLiYv3nP/9JUa+Ac9fe3i5JQ8Z2bB9gV4ZhaMmSJfrFL36hyy67TFIkpt1utwoKCpKOJaZhV3v37lV1dbX6+vqUn5+vLVu2aMqUKWptbSWWkVZeffVV7dmzR7t37x60j3MzRgOSdAAATqO+vl6ff/65Pvjgg1R3BfifTZ48Wa2treru7tbmzZtVV1en5ubmVHcLOCttbW166KGHtH37dmVnZ6e6O8CwYLq7zRQVFcnlcg16AmVHR4dKSkpS1Cvg3MXil9hGunnwwQf15ptv6t///rfKy8ut9pKSEgWDQXV1dSUdT0zDrtxuty666CJVVVWpsbFR06ZN0zPPPEMsI620tLSos7NTV155pTIyMpSRkaHm5matXr1aGRkZKi4uJp6R9kjSbcbtdquqqkpNTU1Wm2EYampqUnV1dQp7BpybSZMmqaSkJCm2e3p69MknnxDbsCXTNPXggw9qy5YtevfddzVp0qSk/VVVVcrMzEyK6f379+vQoUPENNKCYRjq7+8nlpFWbrjhBu3du1etra3WMmPGDN1xxx3WOvGMdMd0dxtqaGhQXV2dZsyYoZkzZ+rpp59WIBDQokWLUt014JR6e3t14MABa/vgwYNqbW1VYWGhKisrtWTJEj3++OO6+OKLNWnSJD366KMqKyvTvHnzUtdp4CTq6+v1yiuv6I033pDH47HuZfT5fMrJyZHP59PixYvV0NCgwsJCeb1e/fGPf1R1dbWuvvrqFPceSLZs2TLdeOONqqyslN/v1yuvvKIdO3bo7bffJpaRVjwej/VskJi8vDyNHz/eaieeke5I0m3otttu09GjR7V8+XK1t7dr+vTp2rZt26AHbgF28+mnn+q6666zthsaGiRJdXV1Wr9+vZYuXapAIKA//OEP6urq0rXXXqtt27ZxTxls6fnnn5ck/epXv0pqX7dunRYuXChJ+utf/yqn06lbbrlF/f39qq2t1XPPPTfCPQVOr7OzUwsWLNCRI0fk8/k0depUvf322/r1r38tiVjG6EI8I905TNM0U90JAAAAAADAPekAAAAAANgGSToAAAAAADZBkg4AAAAAgE2QpAMAAAAAYBMk6QAAAAAA2ARJOgAAAAAANkGSDgAAAACATZCkAwAAAABgEyTpAAAAAADYBEk6AABjwMKFCzVv3rxUdwMAAJwGSToAABhxwWAw1V0AAMCWSNIBABjjVq1apcsvv1x5eXmqqKjQAw88oN7eXklSIBCQ1+vV5s2bk96zdetW5eXlye/3S5La2tr0u9/9TgUFBSosLNTcuXP17bffWsfHKvl//vOfVVZWpsmTJ4/Y5wMAIJ2QpAMAMMY5nU6tXr1a+/bt08svv6x3331XS5culSTl5eXp97//vdatW5f0nnXr1unWW2+Vx+NRKBRSbW2tPB6P3n//fX344YfKz8/Xb37zm6SKeVNTk/bv36/t27frzTffHNHPCABAunCYpmmmuhMAAGB4LVy4UF1dXdq6detpj928ebPuv/9+HTt2TJK0a9cuXXPNNWpra1Npaak6Ozs1ceJEvfPOO5o9e7Y2btyoxx9/XF9++aUcDoekyHT2goICbd26VXPmzNHChQu1bds2HTp0SG63ezg/KgAAaY1KOgAAY9w777yjG264QRMnTpTH49Fdd92lH374QcePH5ckzZw5U5deeqlefvllSdLGjRt1wQUX6Je//KUk6bPPPtOBAwfk8XiUn5+v/Px8FRYWqq+vT19//bX1fS6//HISdAAAToMkHQCAMezbb7/Vb3/7W02dOlWvv/66WlpatGbNGknJD3e75557tH79ekmRqe6LFi2yqua9vb2qqqpSa2tr0vLf//5Xt99+u/U18vLyRu6DAQCQpjJS3QEAAJA6LS0tMgxDf/nLX+R0Rsbu//GPfww67s4779TSpUu1evVqffHFF6qrq7P2XXnlldq0aZMmTJggr9c7Yn0HAGA0opIOAMAY0d3dPajaXVRUpFAopGeffVbffPONNmzYoLVr1w5677hx4zR//nw98sgjmjNnjsrLy619d9xxh4qKijR37ly9//77OnjwoHbs2KE//elP+v7770fyIwIAkPZI0gEAGCN27NihK664ImnZsGGDVq1apSeeeEKXXXaZ/va3v6mxsXHI9y9evFjBYFB33313Untubq7ee+89VVZWav78+fr5z3+uxYsXq6+vj8o6AABniae7AwCAM7JhwwY9/PDDOnz4MA+AAwBgmHBPOgAAOKXjx4/ryJEjWrlype677z4SdAAAhhHT3QEAwCk9+eSTuuSSS1RSUqJly5alujsAAIxqTHcHAAAAAMAmqKQDAAAAAGATJOkAAAAAANgESToAAAAAADZBkg4AAAAAgE2QpAMAAAAAYBMk6QAAAAAA2ARJOgAAAAAANkGSDgAAAACATfw/01WRZf+rRvAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# DataFrame을 구성할 데이터를 저장할 리스트\n",
    "data = []\n",
    "\n",
    "for i in range(48):\n",
    "    lora_A = model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.lora_A.default.weight.T\n",
    "    lora_A_norm = torch.norm(lora_A[:, 0]).item()\n",
    "    q_original_weight = model.deberta.encoder.layer[i].attention.self.query_proj.weight.data.T\n",
    "    q_proj_u, q_proj_s, q_proj_v = torch.linalg.svd(q_original_weight)\n",
    "    new_lora_A = q_proj_u[:, :4] @ torch.diag(q_proj_s[:4]).sqrt()\n",
    "    new_lora_A_norm = torch.norm(new_lora_A[:, 0]).item()\n",
    "    # 각 layer의 lora_A 및 new_lora_A의 norm 값을 데이터에 추가\n",
    "    data.append({\"Layer\": i, \"lora_A Norm\": lora_A_norm, \"new_lora_A Norm\": new_lora_A_norm})\n",
    "\n",
    "# 리스트를 DataFrame으로 변환\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame을 출력\n",
    "print(df)\n",
    "\n",
    "# Layer에 따른 lora_A Norm과 new_lora_A Norm의 분포를 표시하는 lineplot 생성\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x=\"Layer\", y=\"lora_A Norm\", data=df, label=\"lora_A Norm\")\n",
    "sns.lineplot(x=\"Layer\", y=\"new_lora_A Norm\", data=df, label=\"new_lora_A Norm\")\n",
    "plt.title(\"Norm Distribution by Layer for lora_A and new_lora_A\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([tensor(0.5674, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5857, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5796, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5761, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5753, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5796, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5784, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5738, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5734, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5654, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5826, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5692, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5683, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5814, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5670, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5818, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5741, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5749, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5754, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5869, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5786, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5709, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5776, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5739, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5794, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5844, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5689, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5763, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5770, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5744, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5760, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5835, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5823, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5711, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5769, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5761, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5847, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5749, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5732, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5854, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5775, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5685, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5741, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5814, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5759, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5720, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5738, grad_fn=<LinalgVectorNormBackward0>),\n",
       "  tensor(0.5704, grad_fn=<LinalgVectorNormBackward0>)],\n",
       " [tensor(3.2754),\n",
       "  tensor(2.7274),\n",
       "  tensor(2.9403),\n",
       "  tensor(3.0741),\n",
       "  tensor(2.8918),\n",
       "  tensor(3.0407),\n",
       "  tensor(2.9064),\n",
       "  tensor(3.1607),\n",
       "  tensor(3.0960),\n",
       "  tensor(3.0624),\n",
       "  tensor(3.0136),\n",
       "  tensor(2.9946),\n",
       "  tensor(3.0471),\n",
       "  tensor(2.6912),\n",
       "  tensor(3.0725),\n",
       "  tensor(3.1196),\n",
       "  tensor(2.9832),\n",
       "  tensor(3.0535),\n",
       "  tensor(2.8972),\n",
       "  tensor(3.1522),\n",
       "  tensor(3.0936),\n",
       "  tensor(3.0776),\n",
       "  tensor(3.0492),\n",
       "  tensor(2.9450),\n",
       "  tensor(3.0412),\n",
       "  tensor(2.6832),\n",
       "  tensor(3.0625),\n",
       "  tensor(3.1114),\n",
       "  tensor(2.9791),\n",
       "  tensor(3.0429),\n",
       "  tensor(2.8912),\n",
       "  tensor(3.1349),\n",
       "  tensor(3.0654),\n",
       "  tensor(3.0342),\n",
       "  tensor(3.0312),\n",
       "  tensor(2.9505),\n",
       "  tensor(3.0318),\n",
       "  tensor(2.6869),\n",
       "  tensor(3.0530),\n",
       "  tensor(3.1329),\n",
       "  tensor(2.9930),\n",
       "  tensor(3.0552),\n",
       "  tensor(2.9097),\n",
       "  tensor(3.1697),\n",
       "  tensor(3.1089),\n",
       "  tensor(3.0483),\n",
       "  tensor(3.0182),\n",
       "  tensor(4.0768)])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_A_norms = []\n",
    "new_lora_A_norms = []\n",
    "for i in range(48):\n",
    "    lora_A = model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.lora_A.default.weight.T\n",
    "    lora_A_norms.append(torch.norm(lora_A[:, 0]))\n",
    "\n",
    "    q_original_weight = model.deberta.encoder.layer[i].attention.self.query_proj.weight.data.T\n",
    "    q_proj_u, q_proj_s, q_proj_v = torch.linalg.svd(q_original_weight)\n",
    "    new_lora_A = q_proj_u[:, :4] @ torch.diag(q_proj_s[:4]).sqrt()\n",
    "    new_lora_A_norms.append(torch.norm(new_lora_A[:, 0]))\n",
    "\n",
    "lora_A_norms, new_lora_A_norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.2754)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lora_A_norms[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(48):\n",
    "    scale = lora_A_norms[i] / new_lora_A_norms[i]\n",
    "    scaled_lora = model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.lora_A.default.weight.T * scale\n",
    "    scaled_norm = torch.norm(scaled_lora[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head-Wise Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0번째 Weight의 Head-Wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([24, 1536, 64]), torch.Size([24, 1536, 64]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_loraA_head_list = []\n",
    "v_loraA_head_list = []\n",
    "q_weight = model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.weight.T\n",
    "v_weight = model.base_model.model.deberta.encoder.layer[0].attention.self.value_proj.weight.T\n",
    "all_q_head = copy.deepcopy(q_weight)\n",
    "all_q_head = all_q_head.reshape(24, 1536, 64)\n",
    "all_v_head = copy.deepcopy(v_weight)\n",
    "all_v_head = all_v_head.reshape(24, 1536, 64)\n",
    "all_q_head.shape, all_v_head.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0712,  0.0211, -0.0921,  ..., -0.0111,  0.0479,  0.0247],\n",
       "        [-0.0326,  0.0605, -0.1219,  ...,  0.0445, -0.0812, -0.0392],\n",
       "        [ 0.0090,  0.0134,  0.0068,  ...,  0.0231, -0.0630,  0.0749],\n",
       "        ...,\n",
       "        [ 0.0343,  0.0161, -0.0853,  ..., -0.0633, -0.0336,  0.0295],\n",
       "        [ 0.1448, -0.0611, -0.0674,  ..., -0.0059,  0.0809, -0.0400],\n",
       "        [-0.0075,  0.0023, -0.0746,  ..., -0.0186, -0.0690, -0.0521]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th head norm tensor(18.0516)\n",
      "1th head norm tensor(18.1587)\n",
      "2th head norm tensor(17.7440)\n",
      "3th head norm tensor(19.0205)\n",
      "4th head norm tensor(18.2110)\n",
      "5th head norm tensor(18.1781)\n",
      "6th head norm tensor(18.0326)\n",
      "7th head norm tensor(17.7316)\n",
      "8th head norm tensor(17.9980)\n",
      "9th head norm tensor(17.7017)\n",
      "10th head norm tensor(17.9464)\n",
      "11th head norm tensor(18.2455)\n",
      "12th head norm tensor(18.5894)\n",
      "13th head norm tensor(18.3362)\n",
      "14th head norm tensor(17.9002)\n",
      "15th head norm tensor(18.0703)\n",
      "16th head norm tensor(18.5292)\n",
      "17th head norm tensor(17.9256)\n",
      "18th head norm tensor(18.5164)\n",
      "19th head norm tensor(18.2631)\n",
      "20th head norm tensor(17.7721)\n",
      "21th head norm tensor(17.8273)\n",
      "22th head norm tensor(17.8645)\n",
      "23th head norm tensor(17.7883)\n"
     ]
    }
   ],
   "source": [
    "norm_list = []\n",
    "for i in range(24):\n",
    "    norm_of_head = torch.norm(all_q_head[i])\n",
    "    norm_list.append(norm_of_head)\n",
    "    print(f\"{i}th head norm\", norm_of_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0712,  0.0211, -0.0921,  ..., -0.0527,  0.0543,  0.0396],\n",
       "         [-0.0436,  0.0284,  0.0367,  ...,  0.0268,  0.0274, -0.0667],\n",
       "         [-0.0261, -0.0560,  0.0159,  ...,  0.0166,  0.0348, -0.0400],\n",
       "         ...,\n",
       "         [-0.0388, -0.0961,  0.1036,  ...,  0.1256,  0.0967, -0.0703],\n",
       "         [-0.0631,  0.0809, -0.0771,  ...,  0.0237, -0.0681,  0.0747],\n",
       "         [ 0.0491,  0.1465, -0.0120,  ...,  0.1124,  0.0781, -0.0861]]),\n",
       " tensor(18.0516))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_q_head[0], torch.norm(all_q_head[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 1536]), torch.Size([64]), torch.Size([64, 64]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_head = all_q_head[0]\n",
    "q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "q_u.shape, q_s.shape, q_vt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.3308, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 2.7497, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 2.7257,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.7899, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.7736, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.7647]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(q_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8251, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 1.6582, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 1.6510,  ..., 0.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 1.3379, 0.0000, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 1.3318, 0.0000],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 1.3284]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.diag(q_s).sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0153],\n",
       "        [ 0.0038],\n",
       "        [-0.0069],\n",
       "        ...,\n",
       "        [ 0.0242],\n",
       "        [ 0.0127],\n",
       "        [-0.0069]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_u[:, :1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1536, 1]), tensor(3.3308))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "q_loraA_head.shape, torch.norm(q_loraA_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th head norm tensor(3.3308)\n",
      "1th head norm tensor(2.8173)\n",
      "2th head norm tensor(2.6986)\n",
      "3th head norm tensor(3.2101)\n",
      "4th head norm tensor(2.8012)\n",
      "5th head norm tensor(2.7725)\n",
      "6th head norm tensor(2.8254)\n",
      "7th head norm tensor(2.6568)\n",
      "8th head norm tensor(2.7803)\n",
      "9th head norm tensor(2.6874)\n",
      "10th head norm tensor(2.7354)\n",
      "11th head norm tensor(2.7827)\n",
      "12th head norm tensor(3.5680)\n",
      "13th head norm tensor(2.7754)\n",
      "14th head norm tensor(2.7380)\n",
      "15th head norm tensor(2.7538)\n",
      "16th head norm tensor(2.9089)\n",
      "17th head norm tensor(2.7351)\n",
      "18th head norm tensor(2.9385)\n",
      "19th head norm tensor(2.7841)\n",
      "20th head norm tensor(2.6948)\n",
      "21th head norm tensor(2.7090)\n",
      "22th head norm tensor(2.6923)\n",
      "23th head norm tensor(2.7195)\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    q_head = all_q_head[i]\n",
    "    q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "    q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "    print(f\"{i}th head norm\", torch.norm(q_loraA_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####0th layer#####\n",
      "0th Layer 0th head norm tensor(3.3308)\n",
      "0th Layer 1th head norm tensor(2.8173)\n",
      "0th Layer 2th head norm tensor(2.6986)\n",
      "0th Layer 3th head norm tensor(3.2101)\n",
      "0th Layer 4th head norm tensor(2.8012)\n",
      "0th Layer 5th head norm tensor(2.7725)\n",
      "0th Layer 6th head norm tensor(2.8254)\n",
      "0th Layer 7th head norm tensor(2.6568)\n",
      "0th Layer 8th head norm tensor(2.7803)\n",
      "0th Layer 9th head norm tensor(2.6874)\n",
      "0th Layer 10th head norm tensor(2.7354)\n",
      "0th Layer 11th head norm tensor(2.7827)\n",
      "0th Layer 12th head norm tensor(3.5680)\n",
      "0th Layer 13th head norm tensor(2.7754)\n",
      "0th Layer 14th head norm tensor(2.7380)\n",
      "0th Layer 15th head norm tensor(2.7538)\n",
      "0th Layer 16th head norm tensor(2.9089)\n",
      "0th Layer 17th head norm tensor(2.7351)\n",
      "0th Layer 18th head norm tensor(2.9385)\n",
      "0th Layer 19th head norm tensor(2.7841)\n",
      "0th Layer 20th head norm tensor(2.6948)\n",
      "0th Layer 21th head norm tensor(2.7090)\n",
      "0th Layer 22th head norm tensor(2.6923)\n",
      "0th Layer 23th head norm tensor(2.7195)\n",
      "#####1th layer#####\n",
      "1th Layer 0th head norm tensor(1.7169)\n",
      "1th Layer 1th head norm tensor(1.7428)\n",
      "1th Layer 2th head norm tensor(1.6574)\n",
      "1th Layer 3th head norm tensor(1.6936)\n",
      "1th Layer 4th head norm tensor(1.7722)\n",
      "1th Layer 5th head norm tensor(1.7189)\n",
      "1th Layer 6th head norm tensor(1.7011)\n",
      "1th Layer 7th head norm tensor(1.7096)\n",
      "1th Layer 8th head norm tensor(1.8017)\n",
      "1th Layer 9th head norm tensor(1.7361)\n",
      "1th Layer 10th head norm tensor(1.7080)\n",
      "1th Layer 11th head norm tensor(1.7022)\n",
      "1th Layer 12th head norm tensor(1.7149)\n",
      "1th Layer 13th head norm tensor(1.6745)\n",
      "1th Layer 14th head norm tensor(1.6967)\n",
      "1th Layer 15th head norm tensor(1.7444)\n",
      "1th Layer 16th head norm tensor(1.8129)\n",
      "1th Layer 17th head norm tensor(1.7347)\n",
      "1th Layer 18th head norm tensor(1.6948)\n",
      "1th Layer 19th head norm tensor(1.7157)\n",
      "1th Layer 20th head norm tensor(1.7349)\n",
      "1th Layer 21th head norm tensor(1.7390)\n",
      "1th Layer 22th head norm tensor(1.6566)\n",
      "1th Layer 23th head norm tensor(1.6768)\n",
      "#####2th layer#####\n",
      "2th Layer 0th head norm tensor(1.7454)\n",
      "2th Layer 1th head norm tensor(1.7140)\n",
      "2th Layer 2th head norm tensor(1.7137)\n",
      "2th Layer 3th head norm tensor(1.7183)\n",
      "2th Layer 4th head norm tensor(1.7721)\n",
      "2th Layer 5th head norm tensor(1.7332)\n",
      "2th Layer 6th head norm tensor(1.7228)\n",
      "2th Layer 7th head norm tensor(1.6978)\n",
      "2th Layer 8th head norm tensor(1.7339)\n",
      "2th Layer 9th head norm tensor(1.6837)\n",
      "2th Layer 10th head norm tensor(1.7117)\n",
      "2th Layer 11th head norm tensor(1.7033)\n",
      "2th Layer 12th head norm tensor(1.7271)\n",
      "2th Layer 13th head norm tensor(1.6973)\n",
      "2th Layer 14th head norm tensor(1.7129)\n",
      "2th Layer 15th head norm tensor(1.7355)\n",
      "2th Layer 16th head norm tensor(1.7092)\n",
      "2th Layer 17th head norm tensor(1.7210)\n",
      "2th Layer 18th head norm tensor(1.7192)\n",
      "2th Layer 19th head norm tensor(1.7359)\n",
      "2th Layer 20th head norm tensor(1.7101)\n",
      "2th Layer 21th head norm tensor(1.7286)\n",
      "2th Layer 22th head norm tensor(1.7304)\n",
      "2th Layer 23th head norm tensor(1.7156)\n",
      "#####3th layer#####\n",
      "3th Layer 0th head norm tensor(1.7453)\n",
      "3th Layer 1th head norm tensor(1.7250)\n",
      "3th Layer 2th head norm tensor(1.6748)\n",
      "3th Layer 3th head norm tensor(1.7080)\n",
      "3th Layer 4th head norm tensor(1.7255)\n",
      "3th Layer 5th head norm tensor(1.7493)\n",
      "3th Layer 6th head norm tensor(1.7097)\n",
      "3th Layer 7th head norm tensor(1.7026)\n",
      "3th Layer 8th head norm tensor(1.7078)\n",
      "3th Layer 9th head norm tensor(1.6798)\n",
      "3th Layer 10th head norm tensor(1.7402)\n",
      "3th Layer 11th head norm tensor(1.7119)\n",
      "3th Layer 12th head norm tensor(1.7098)\n",
      "3th Layer 13th head norm tensor(1.7277)\n",
      "3th Layer 14th head norm tensor(1.6829)\n",
      "3th Layer 15th head norm tensor(1.7375)\n",
      "3th Layer 16th head norm tensor(1.7336)\n",
      "3th Layer 17th head norm tensor(1.7443)\n",
      "3th Layer 18th head norm tensor(1.7052)\n",
      "3th Layer 19th head norm tensor(1.7156)\n",
      "3th Layer 20th head norm tensor(1.6956)\n",
      "3th Layer 21th head norm tensor(1.7568)\n",
      "3th Layer 22th head norm tensor(1.7476)\n",
      "3th Layer 23th head norm tensor(1.7029)\n",
      "#####4th layer#####\n",
      "4th Layer 0th head norm tensor(1.7631)\n",
      "4th Layer 1th head norm tensor(1.8126)\n",
      "4th Layer 2th head norm tensor(1.7533)\n",
      "4th Layer 3th head norm tensor(1.7733)\n",
      "4th Layer 4th head norm tensor(1.7949)\n",
      "4th Layer 5th head norm tensor(1.8185)\n",
      "4th Layer 6th head norm tensor(1.7680)\n",
      "4th Layer 7th head norm tensor(1.7218)\n",
      "4th Layer 8th head norm tensor(1.7631)\n",
      "4th Layer 9th head norm tensor(1.7507)\n",
      "4th Layer 10th head norm tensor(1.7929)\n",
      "4th Layer 11th head norm tensor(1.7643)\n",
      "4th Layer 12th head norm tensor(1.8515)\n",
      "4th Layer 13th head norm tensor(1.7425)\n",
      "4th Layer 14th head norm tensor(1.7213)\n",
      "4th Layer 15th head norm tensor(1.7271)\n",
      "4th Layer 16th head norm tensor(1.7386)\n",
      "4th Layer 17th head norm tensor(1.7669)\n",
      "4th Layer 18th head norm tensor(1.7394)\n",
      "4th Layer 19th head norm tensor(1.7435)\n",
      "4th Layer 20th head norm tensor(1.7440)\n",
      "4th Layer 21th head norm tensor(1.7797)\n",
      "4th Layer 22th head norm tensor(1.7360)\n",
      "4th Layer 23th head norm tensor(1.7695)\n",
      "#####5th layer#####\n",
      "5th Layer 0th head norm tensor(1.7678)\n",
      "5th Layer 1th head norm tensor(1.7798)\n",
      "5th Layer 2th head norm tensor(1.7321)\n",
      "5th Layer 3th head norm tensor(1.7342)\n",
      "5th Layer 4th head norm tensor(1.7677)\n",
      "5th Layer 5th head norm tensor(1.7455)\n",
      "5th Layer 6th head norm tensor(1.7620)\n",
      "5th Layer 7th head norm tensor(1.7296)\n",
      "5th Layer 8th head norm tensor(1.7376)\n",
      "5th Layer 9th head norm tensor(1.7322)\n",
      "5th Layer 10th head norm tensor(1.7657)\n",
      "5th Layer 11th head norm tensor(1.7381)\n",
      "5th Layer 12th head norm tensor(1.9738)\n",
      "5th Layer 13th head norm tensor(1.7325)\n",
      "5th Layer 14th head norm tensor(1.7365)\n",
      "5th Layer 15th head norm tensor(1.7285)\n",
      "5th Layer 16th head norm tensor(1.7531)\n",
      "5th Layer 17th head norm tensor(1.7474)\n",
      "5th Layer 18th head norm tensor(1.7381)\n",
      "5th Layer 19th head norm tensor(1.7349)\n",
      "5th Layer 20th head norm tensor(1.7315)\n",
      "5th Layer 21th head norm tensor(1.7507)\n",
      "5th Layer 22th head norm tensor(1.7153)\n",
      "5th Layer 23th head norm tensor(1.7383)\n",
      "#####6th layer#####\n",
      "6th Layer 0th head norm tensor(1.8079)\n",
      "6th Layer 1th head norm tensor(1.9059)\n",
      "6th Layer 2th head norm tensor(1.7865)\n",
      "6th Layer 3th head norm tensor(1.7543)\n",
      "6th Layer 4th head norm tensor(1.7782)\n",
      "6th Layer 5th head norm tensor(1.7953)\n",
      "6th Layer 6th head norm tensor(1.7846)\n",
      "6th Layer 7th head norm tensor(1.8006)\n",
      "6th Layer 8th head norm tensor(1.7760)\n",
      "6th Layer 9th head norm tensor(1.8101)\n",
      "6th Layer 10th head norm tensor(1.7818)\n",
      "6th Layer 11th head norm tensor(1.7912)\n",
      "6th Layer 12th head norm tensor(1.8720)\n",
      "6th Layer 13th head norm tensor(1.7672)\n",
      "6th Layer 14th head norm tensor(1.7721)\n",
      "6th Layer 15th head norm tensor(1.7503)\n",
      "6th Layer 16th head norm tensor(1.7672)\n",
      "6th Layer 17th head norm tensor(1.8367)\n",
      "6th Layer 18th head norm tensor(1.7537)\n",
      "6th Layer 19th head norm tensor(1.7587)\n",
      "6th Layer 20th head norm tensor(1.7896)\n",
      "6th Layer 21th head norm tensor(1.8132)\n",
      "6th Layer 22th head norm tensor(1.7627)\n",
      "6th Layer 23th head norm tensor(1.7900)\n",
      "#####7th layer#####\n",
      "7th Layer 0th head norm tensor(1.8499)\n",
      "7th Layer 1th head norm tensor(1.7917)\n",
      "7th Layer 2th head norm tensor(1.7667)\n",
      "7th Layer 3th head norm tensor(1.7794)\n",
      "7th Layer 4th head norm tensor(2.0228)\n",
      "7th Layer 5th head norm tensor(1.8580)\n",
      "7th Layer 6th head norm tensor(1.7810)\n",
      "7th Layer 7th head norm tensor(1.7692)\n",
      "7th Layer 8th head norm tensor(1.7895)\n",
      "7th Layer 9th head norm tensor(1.7779)\n",
      "7th Layer 10th head norm tensor(1.8111)\n",
      "7th Layer 11th head norm tensor(1.7936)\n",
      "7th Layer 12th head norm tensor(1.7535)\n",
      "7th Layer 13th head norm tensor(1.7336)\n",
      "7th Layer 14th head norm tensor(1.7845)\n",
      "7th Layer 15th head norm tensor(1.7765)\n",
      "7th Layer 16th head norm tensor(1.7918)\n",
      "7th Layer 17th head norm tensor(1.7847)\n",
      "7th Layer 18th head norm tensor(1.8101)\n",
      "7th Layer 19th head norm tensor(1.7932)\n",
      "7th Layer 20th head norm tensor(1.8103)\n",
      "7th Layer 21th head norm tensor(1.8003)\n",
      "7th Layer 22th head norm tensor(1.7698)\n",
      "7th Layer 23th head norm tensor(1.7828)\n",
      "#####8th layer#####\n",
      "8th Layer 0th head norm tensor(1.8902)\n",
      "8th Layer 1th head norm tensor(1.7859)\n",
      "8th Layer 2th head norm tensor(2.0634)\n",
      "8th Layer 3th head norm tensor(1.7849)\n",
      "8th Layer 4th head norm tensor(1.7523)\n",
      "8th Layer 5th head norm tensor(1.7820)\n",
      "8th Layer 6th head norm tensor(1.8133)\n",
      "8th Layer 7th head norm tensor(1.7965)\n",
      "8th Layer 8th head norm tensor(1.9236)\n",
      "8th Layer 9th head norm tensor(1.8143)\n",
      "8th Layer 10th head norm tensor(1.8099)\n",
      "8th Layer 11th head norm tensor(1.8114)\n",
      "8th Layer 12th head norm tensor(1.7828)\n",
      "8th Layer 13th head norm tensor(1.7728)\n",
      "8th Layer 14th head norm tensor(1.8014)\n",
      "8th Layer 15th head norm tensor(1.8045)\n",
      "8th Layer 16th head norm tensor(1.7740)\n",
      "8th Layer 17th head norm tensor(1.8263)\n",
      "8th Layer 18th head norm tensor(1.8181)\n",
      "8th Layer 19th head norm tensor(1.8059)\n",
      "8th Layer 20th head norm tensor(1.7836)\n",
      "8th Layer 21th head norm tensor(1.8339)\n",
      "8th Layer 22th head norm tensor(1.7880)\n",
      "8th Layer 23th head norm tensor(1.8104)\n",
      "#####9th layer#####\n",
      "9th Layer 0th head norm tensor(1.8298)\n",
      "9th Layer 1th head norm tensor(1.7912)\n",
      "9th Layer 2th head norm tensor(2.1151)\n",
      "9th Layer 3th head norm tensor(1.8048)\n",
      "9th Layer 4th head norm tensor(1.7586)\n",
      "9th Layer 5th head norm tensor(1.8097)\n",
      "9th Layer 6th head norm tensor(1.8029)\n",
      "9th Layer 7th head norm tensor(1.7968)\n",
      "9th Layer 8th head norm tensor(1.9907)\n",
      "9th Layer 9th head norm tensor(1.8013)\n",
      "9th Layer 10th head norm tensor(1.8124)\n",
      "9th Layer 11th head norm tensor(1.8448)\n",
      "9th Layer 12th head norm tensor(1.7893)\n",
      "9th Layer 13th head norm tensor(1.7917)\n",
      "9th Layer 14th head norm tensor(1.7914)\n",
      "9th Layer 15th head norm tensor(1.8043)\n",
      "9th Layer 16th head norm tensor(1.7932)\n",
      "9th Layer 17th head norm tensor(1.8059)\n",
      "9th Layer 18th head norm tensor(1.7676)\n",
      "9th Layer 19th head norm tensor(1.7886)\n",
      "9th Layer 20th head norm tensor(1.7813)\n",
      "9th Layer 21th head norm tensor(1.8098)\n",
      "9th Layer 22th head norm tensor(1.7730)\n",
      "9th Layer 23th head norm tensor(1.8902)\n",
      "#####10th layer#####\n",
      "10th Layer 0th head norm tensor(1.8109)\n",
      "10th Layer 1th head norm tensor(1.7612)\n",
      "10th Layer 2th head norm tensor(1.7935)\n",
      "10th Layer 3th head norm tensor(1.7547)\n",
      "10th Layer 4th head norm tensor(1.7551)\n",
      "10th Layer 5th head norm tensor(1.7796)\n",
      "10th Layer 6th head norm tensor(1.7579)\n",
      "10th Layer 7th head norm tensor(1.7478)\n",
      "10th Layer 8th head norm tensor(1.7593)\n",
      "10th Layer 9th head norm tensor(2.3368)\n",
      "10th Layer 10th head norm tensor(1.7814)\n",
      "10th Layer 11th head norm tensor(1.8045)\n",
      "10th Layer 12th head norm tensor(1.7740)\n",
      "10th Layer 13th head norm tensor(1.7396)\n",
      "10th Layer 14th head norm tensor(1.7813)\n",
      "10th Layer 15th head norm tensor(1.7702)\n",
      "10th Layer 16th head norm tensor(1.7646)\n",
      "10th Layer 17th head norm tensor(1.7600)\n",
      "10th Layer 18th head norm tensor(1.7367)\n",
      "10th Layer 19th head norm tensor(1.7913)\n",
      "10th Layer 20th head norm tensor(1.7761)\n",
      "10th Layer 21th head norm tensor(1.7931)\n",
      "10th Layer 22th head norm tensor(1.8260)\n",
      "10th Layer 23th head norm tensor(1.7499)\n",
      "#####11th layer#####\n",
      "11th Layer 0th head norm tensor(1.7367)\n",
      "11th Layer 1th head norm tensor(1.7121)\n",
      "11th Layer 2th head norm tensor(1.6878)\n",
      "11th Layer 3th head norm tensor(1.9178)\n",
      "11th Layer 4th head norm tensor(1.9090)\n",
      "11th Layer 5th head norm tensor(1.6826)\n",
      "11th Layer 6th head norm tensor(2.8532)\n",
      "11th Layer 7th head norm tensor(1.8163)\n",
      "11th Layer 8th head norm tensor(3.5363)\n",
      "11th Layer 9th head norm tensor(2.8237)\n",
      "11th Layer 10th head norm tensor(2.3182)\n",
      "11th Layer 11th head norm tensor(2.0448)\n",
      "11th Layer 12th head norm tensor(2.6254)\n",
      "11th Layer 13th head norm tensor(1.9571)\n",
      "11th Layer 14th head norm tensor(2.6674)\n",
      "11th Layer 15th head norm tensor(1.7203)\n",
      "11th Layer 16th head norm tensor(1.7929)\n",
      "11th Layer 17th head norm tensor(1.7100)\n",
      "11th Layer 18th head norm tensor(1.6825)\n",
      "11th Layer 19th head norm tensor(3.2473)\n",
      "11th Layer 20th head norm tensor(1.8095)\n",
      "11th Layer 21th head norm tensor(1.7237)\n",
      "11th Layer 22th head norm tensor(1.6910)\n",
      "11th Layer 23th head norm tensor(1.7275)\n",
      "#####12th layer#####\n",
      "12th Layer 0th head norm tensor(2.8627)\n",
      "12th Layer 1th head norm tensor(2.3866)\n",
      "12th Layer 2th head norm tensor(2.2649)\n",
      "12th Layer 3th head norm tensor(2.6544)\n",
      "12th Layer 4th head norm tensor(2.3524)\n",
      "12th Layer 5th head norm tensor(2.3239)\n",
      "12th Layer 6th head norm tensor(2.3573)\n",
      "12th Layer 7th head norm tensor(2.2534)\n",
      "12th Layer 8th head norm tensor(2.3397)\n",
      "12th Layer 9th head norm tensor(2.2584)\n",
      "12th Layer 10th head norm tensor(2.3027)\n",
      "12th Layer 11th head norm tensor(2.3537)\n",
      "12th Layer 12th head norm tensor(3.1575)\n",
      "12th Layer 13th head norm tensor(2.3452)\n",
      "12th Layer 14th head norm tensor(2.3025)\n",
      "12th Layer 15th head norm tensor(2.3343)\n",
      "12th Layer 16th head norm tensor(2.5000)\n",
      "12th Layer 17th head norm tensor(2.3012)\n",
      "12th Layer 18th head norm tensor(2.5419)\n",
      "12th Layer 19th head norm tensor(2.3594)\n",
      "12th Layer 20th head norm tensor(2.2588)\n",
      "12th Layer 21th head norm tensor(2.3120)\n",
      "12th Layer 22th head norm tensor(2.2504)\n",
      "12th Layer 23th head norm tensor(2.2741)\n",
      "#####13th layer#####\n",
      "13th Layer 0th head norm tensor(1.5367)\n",
      "13th Layer 1th head norm tensor(1.5588)\n",
      "13th Layer 2th head norm tensor(1.4776)\n",
      "13th Layer 3th head norm tensor(1.5024)\n",
      "13th Layer 4th head norm tensor(1.5657)\n",
      "13th Layer 5th head norm tensor(1.5228)\n",
      "13th Layer 6th head norm tensor(1.5341)\n",
      "13th Layer 7th head norm tensor(1.4986)\n",
      "13th Layer 8th head norm tensor(1.5309)\n",
      "13th Layer 9th head norm tensor(1.5411)\n",
      "13th Layer 10th head norm tensor(1.5228)\n",
      "13th Layer 11th head norm tensor(1.5202)\n",
      "13th Layer 12th head norm tensor(1.5322)\n",
      "13th Layer 13th head norm tensor(1.4910)\n",
      "13th Layer 14th head norm tensor(1.5209)\n",
      "13th Layer 15th head norm tensor(1.5433)\n",
      "13th Layer 16th head norm tensor(1.6032)\n",
      "13th Layer 17th head norm tensor(1.5350)\n",
      "13th Layer 18th head norm tensor(1.5407)\n",
      "13th Layer 19th head norm tensor(1.5417)\n",
      "13th Layer 20th head norm tensor(1.5269)\n",
      "13th Layer 21th head norm tensor(1.5372)\n",
      "13th Layer 22th head norm tensor(1.4894)\n",
      "13th Layer 23th head norm tensor(1.5167)\n",
      "#####14th layer#####\n",
      "14th Layer 0th head norm tensor(1.7176)\n",
      "14th Layer 1th head norm tensor(1.6915)\n",
      "14th Layer 2th head norm tensor(1.6690)\n",
      "14th Layer 3th head norm tensor(1.7082)\n",
      "14th Layer 4th head norm tensor(1.7159)\n",
      "14th Layer 5th head norm tensor(1.6951)\n",
      "14th Layer 6th head norm tensor(1.6817)\n",
      "14th Layer 7th head norm tensor(1.6711)\n",
      "14th Layer 8th head norm tensor(1.6651)\n",
      "14th Layer 9th head norm tensor(1.6854)\n",
      "14th Layer 10th head norm tensor(1.7131)\n",
      "14th Layer 11th head norm tensor(1.6757)\n",
      "14th Layer 12th head norm tensor(1.6936)\n",
      "14th Layer 13th head norm tensor(1.6608)\n",
      "14th Layer 14th head norm tensor(1.7104)\n",
      "14th Layer 15th head norm tensor(1.7278)\n",
      "14th Layer 16th head norm tensor(1.6845)\n",
      "14th Layer 17th head norm tensor(1.6916)\n",
      "14th Layer 18th head norm tensor(1.6880)\n",
      "14th Layer 19th head norm tensor(1.7020)\n",
      "14th Layer 20th head norm tensor(1.6729)\n",
      "14th Layer 21th head norm tensor(1.7117)\n",
      "14th Layer 22th head norm tensor(1.6794)\n",
      "14th Layer 23th head norm tensor(1.6786)\n",
      "#####15th layer#####\n",
      "15th Layer 0th head norm tensor(1.7833)\n",
      "15th Layer 1th head norm tensor(1.7279)\n",
      "15th Layer 2th head norm tensor(1.6627)\n",
      "15th Layer 3th head norm tensor(1.6850)\n",
      "15th Layer 4th head norm tensor(1.7145)\n",
      "15th Layer 5th head norm tensor(1.7769)\n",
      "15th Layer 6th head norm tensor(1.7121)\n",
      "15th Layer 7th head norm tensor(1.6884)\n",
      "15th Layer 8th head norm tensor(1.6870)\n",
      "15th Layer 9th head norm tensor(1.6926)\n",
      "15th Layer 10th head norm tensor(1.7104)\n",
      "15th Layer 11th head norm tensor(1.7099)\n",
      "15th Layer 12th head norm tensor(1.6907)\n",
      "15th Layer 13th head norm tensor(1.7184)\n",
      "15th Layer 14th head norm tensor(1.6805)\n",
      "15th Layer 15th head norm tensor(1.7297)\n",
      "15th Layer 16th head norm tensor(1.7527)\n",
      "15th Layer 17th head norm tensor(1.7242)\n",
      "15th Layer 18th head norm tensor(1.7259)\n",
      "15th Layer 19th head norm tensor(1.7200)\n",
      "15th Layer 20th head norm tensor(1.6686)\n",
      "15th Layer 21th head norm tensor(1.7736)\n",
      "15th Layer 22th head norm tensor(1.6966)\n",
      "15th Layer 23th head norm tensor(1.7051)\n",
      "#####16th layer#####\n",
      "16th Layer 0th head norm tensor(1.7712)\n",
      "16th Layer 1th head norm tensor(1.8529)\n",
      "16th Layer 2th head norm tensor(1.7756)\n",
      "16th Layer 3th head norm tensor(1.7610)\n",
      "16th Layer 4th head norm tensor(1.8061)\n",
      "16th Layer 5th head norm tensor(1.8031)\n",
      "16th Layer 6th head norm tensor(1.7997)\n",
      "16th Layer 7th head norm tensor(1.7483)\n",
      "16th Layer 8th head norm tensor(1.7706)\n",
      "16th Layer 9th head norm tensor(1.7752)\n",
      "16th Layer 10th head norm tensor(1.8004)\n",
      "16th Layer 11th head norm tensor(1.7955)\n",
      "16th Layer 12th head norm tensor(1.9134)\n",
      "16th Layer 13th head norm tensor(1.7572)\n",
      "16th Layer 14th head norm tensor(1.7460)\n",
      "16th Layer 15th head norm tensor(1.7656)\n",
      "16th Layer 16th head norm tensor(1.7668)\n",
      "16th Layer 17th head norm tensor(1.7890)\n",
      "16th Layer 18th head norm tensor(1.7810)\n",
      "16th Layer 19th head norm tensor(1.7677)\n",
      "16th Layer 20th head norm tensor(1.7518)\n",
      "16th Layer 21th head norm tensor(1.7927)\n",
      "16th Layer 22th head norm tensor(1.7816)\n",
      "16th Layer 23th head norm tensor(1.7985)\n",
      "#####17th layer#####\n",
      "17th Layer 0th head norm tensor(1.7853)\n",
      "17th Layer 1th head norm tensor(1.7824)\n",
      "17th Layer 2th head norm tensor(1.7479)\n",
      "17th Layer 3th head norm tensor(1.7411)\n",
      "17th Layer 4th head norm tensor(1.7747)\n",
      "17th Layer 5th head norm tensor(1.7611)\n",
      "17th Layer 6th head norm tensor(1.7691)\n",
      "17th Layer 7th head norm tensor(1.7297)\n",
      "17th Layer 8th head norm tensor(1.7790)\n",
      "17th Layer 9th head norm tensor(1.7654)\n",
      "17th Layer 10th head norm tensor(1.7660)\n",
      "17th Layer 11th head norm tensor(1.7635)\n",
      "17th Layer 12th head norm tensor(2.2333)\n",
      "17th Layer 13th head norm tensor(1.7464)\n",
      "17th Layer 14th head norm tensor(1.7568)\n",
      "17th Layer 15th head norm tensor(1.7783)\n",
      "17th Layer 16th head norm tensor(1.7487)\n",
      "17th Layer 17th head norm tensor(1.7772)\n",
      "17th Layer 18th head norm tensor(1.7623)\n",
      "17th Layer 19th head norm tensor(1.7829)\n",
      "17th Layer 20th head norm tensor(1.8951)\n",
      "17th Layer 21th head norm tensor(1.8942)\n",
      "17th Layer 22th head norm tensor(1.7468)\n",
      "17th Layer 23th head norm tensor(1.7646)\n",
      "#####18th layer#####\n",
      "18th Layer 0th head norm tensor(1.8425)\n",
      "18th Layer 1th head norm tensor(1.9266)\n",
      "18th Layer 2th head norm tensor(1.8068)\n",
      "18th Layer 3th head norm tensor(1.7774)\n",
      "18th Layer 4th head norm tensor(1.8274)\n",
      "18th Layer 5th head norm tensor(1.8271)\n",
      "18th Layer 6th head norm tensor(1.8222)\n",
      "18th Layer 7th head norm tensor(1.8300)\n",
      "18th Layer 8th head norm tensor(1.8051)\n",
      "18th Layer 9th head norm tensor(1.8818)\n",
      "18th Layer 10th head norm tensor(1.7936)\n",
      "18th Layer 11th head norm tensor(1.8020)\n",
      "18th Layer 12th head norm tensor(1.9222)\n",
      "18th Layer 13th head norm tensor(1.7898)\n",
      "18th Layer 14th head norm tensor(1.7869)\n",
      "18th Layer 15th head norm tensor(1.7702)\n",
      "18th Layer 16th head norm tensor(1.7823)\n",
      "18th Layer 17th head norm tensor(1.8523)\n",
      "18th Layer 18th head norm tensor(1.7829)\n",
      "18th Layer 19th head norm tensor(1.8048)\n",
      "18th Layer 20th head norm tensor(1.8083)\n",
      "18th Layer 21th head norm tensor(1.8313)\n",
      "18th Layer 22th head norm tensor(1.7974)\n",
      "18th Layer 23th head norm tensor(1.7948)\n",
      "#####19th layer#####\n",
      "19th Layer 0th head norm tensor(1.8812)\n",
      "19th Layer 1th head norm tensor(1.8197)\n",
      "19th Layer 2th head norm tensor(1.7771)\n",
      "19th Layer 3th head norm tensor(1.8012)\n",
      "19th Layer 4th head norm tensor(2.0659)\n",
      "19th Layer 5th head norm tensor(1.8785)\n",
      "19th Layer 6th head norm tensor(1.8091)\n",
      "19th Layer 7th head norm tensor(1.7880)\n",
      "19th Layer 8th head norm tensor(1.8266)\n",
      "19th Layer 9th head norm tensor(1.8063)\n",
      "19th Layer 10th head norm tensor(1.8566)\n",
      "19th Layer 11th head norm tensor(1.8116)\n",
      "19th Layer 12th head norm tensor(1.7610)\n",
      "19th Layer 13th head norm tensor(1.7666)\n",
      "19th Layer 14th head norm tensor(1.8060)\n",
      "19th Layer 15th head norm tensor(1.8164)\n",
      "19th Layer 16th head norm tensor(1.8297)\n",
      "19th Layer 17th head norm tensor(1.8291)\n",
      "19th Layer 18th head norm tensor(1.8226)\n",
      "19th Layer 19th head norm tensor(1.8229)\n",
      "19th Layer 20th head norm tensor(1.8327)\n",
      "19th Layer 21th head norm tensor(1.8288)\n",
      "19th Layer 22th head norm tensor(1.7777)\n",
      "19th Layer 23th head norm tensor(1.8296)\n",
      "#####20th layer#####\n",
      "20th Layer 0th head norm tensor(1.8811)\n",
      "20th Layer 1th head norm tensor(1.8168)\n",
      "20th Layer 2th head norm tensor(2.1060)\n",
      "20th Layer 3th head norm tensor(1.7998)\n",
      "20th Layer 4th head norm tensor(1.7822)\n",
      "20th Layer 5th head norm tensor(1.8014)\n",
      "20th Layer 6th head norm tensor(1.8348)\n",
      "20th Layer 7th head norm tensor(1.8210)\n",
      "20th Layer 8th head norm tensor(1.9530)\n",
      "20th Layer 9th head norm tensor(1.8236)\n",
      "20th Layer 10th head norm tensor(1.8395)\n",
      "20th Layer 11th head norm tensor(1.8484)\n",
      "20th Layer 12th head norm tensor(1.8056)\n",
      "20th Layer 13th head norm tensor(1.8096)\n",
      "20th Layer 14th head norm tensor(1.8237)\n",
      "20th Layer 15th head norm tensor(1.8249)\n",
      "20th Layer 16th head norm tensor(1.8194)\n",
      "20th Layer 17th head norm tensor(1.8641)\n",
      "20th Layer 18th head norm tensor(1.8541)\n",
      "20th Layer 19th head norm tensor(1.8348)\n",
      "20th Layer 20th head norm tensor(1.8093)\n",
      "20th Layer 21th head norm tensor(1.8520)\n",
      "20th Layer 22th head norm tensor(1.7992)\n",
      "20th Layer 23th head norm tensor(1.8481)\n",
      "#####21th layer#####\n",
      "21th Layer 0th head norm tensor(1.8684)\n",
      "21th Layer 1th head norm tensor(1.8375)\n",
      "21th Layer 2th head norm tensor(2.1087)\n",
      "21th Layer 3th head norm tensor(1.8342)\n",
      "21th Layer 4th head norm tensor(1.8333)\n",
      "21th Layer 5th head norm tensor(1.8294)\n",
      "21th Layer 6th head norm tensor(1.8238)\n",
      "21th Layer 7th head norm tensor(1.8505)\n",
      "21th Layer 8th head norm tensor(2.0292)\n",
      "21th Layer 9th head norm tensor(1.8273)\n",
      "21th Layer 10th head norm tensor(1.8382)\n",
      "21th Layer 11th head norm tensor(1.8558)\n",
      "21th Layer 12th head norm tensor(1.8197)\n",
      "21th Layer 13th head norm tensor(1.8214)\n",
      "21th Layer 14th head norm tensor(1.8292)\n",
      "21th Layer 15th head norm tensor(1.8532)\n",
      "21th Layer 16th head norm tensor(1.8265)\n",
      "21th Layer 17th head norm tensor(1.8560)\n",
      "21th Layer 18th head norm tensor(1.7996)\n",
      "21th Layer 19th head norm tensor(1.8198)\n",
      "21th Layer 20th head norm tensor(1.8283)\n",
      "21th Layer 21th head norm tensor(1.8614)\n",
      "21th Layer 22th head norm tensor(1.8366)\n",
      "21th Layer 23th head norm tensor(1.9356)\n",
      "#####22th layer#####\n",
      "22th Layer 0th head norm tensor(1.8461)\n",
      "22th Layer 1th head norm tensor(1.8195)\n",
      "22th Layer 2th head norm tensor(1.8636)\n",
      "22th Layer 3th head norm tensor(1.7899)\n",
      "22th Layer 4th head norm tensor(1.8132)\n",
      "22th Layer 5th head norm tensor(1.8310)\n",
      "22th Layer 6th head norm tensor(1.8344)\n",
      "22th Layer 7th head norm tensor(1.8091)\n",
      "22th Layer 8th head norm tensor(1.8275)\n",
      "22th Layer 9th head norm tensor(2.4262)\n",
      "22th Layer 10th head norm tensor(1.8133)\n",
      "22th Layer 11th head norm tensor(1.8296)\n",
      "22th Layer 12th head norm tensor(1.8156)\n",
      "22th Layer 13th head norm tensor(1.7880)\n",
      "22th Layer 14th head norm tensor(1.8350)\n",
      "22th Layer 15th head norm tensor(1.8292)\n",
      "22th Layer 16th head norm tensor(1.7906)\n",
      "22th Layer 17th head norm tensor(1.8141)\n",
      "22th Layer 18th head norm tensor(1.8042)\n",
      "22th Layer 19th head norm tensor(1.8345)\n",
      "22th Layer 20th head norm tensor(1.8084)\n",
      "22th Layer 21th head norm tensor(1.8546)\n",
      "22th Layer 22th head norm tensor(1.8953)\n",
      "22th Layer 23th head norm tensor(1.8068)\n",
      "#####23th layer#####\n",
      "23th Layer 0th head norm tensor(1.7483)\n",
      "23th Layer 1th head norm tensor(1.7272)\n",
      "23th Layer 2th head norm tensor(1.7060)\n",
      "23th Layer 3th head norm tensor(1.9310)\n",
      "23th Layer 4th head norm tensor(1.9259)\n",
      "23th Layer 5th head norm tensor(1.6987)\n",
      "23th Layer 6th head norm tensor(2.8750)\n",
      "23th Layer 7th head norm tensor(1.8171)\n",
      "23th Layer 8th head norm tensor(3.5320)\n",
      "23th Layer 9th head norm tensor(2.8441)\n",
      "23th Layer 10th head norm tensor(2.3275)\n",
      "23th Layer 11th head norm tensor(2.0624)\n",
      "23th Layer 12th head norm tensor(2.6374)\n",
      "23th Layer 13th head norm tensor(1.9616)\n",
      "23th Layer 14th head norm tensor(2.6928)\n",
      "23th Layer 15th head norm tensor(1.7324)\n",
      "23th Layer 16th head norm tensor(1.7969)\n",
      "23th Layer 17th head norm tensor(1.7372)\n",
      "23th Layer 18th head norm tensor(1.7024)\n",
      "23th Layer 19th head norm tensor(3.2437)\n",
      "23th Layer 20th head norm tensor(1.8219)\n",
      "23th Layer 21th head norm tensor(1.7311)\n",
      "23th Layer 22th head norm tensor(1.7080)\n",
      "23th Layer 23th head norm tensor(1.7386)\n",
      "#####24th layer#####\n",
      "24th Layer 0th head norm tensor(2.8693)\n",
      "24th Layer 1th head norm tensor(2.3810)\n",
      "24th Layer 2th head norm tensor(2.2601)\n",
      "24th Layer 3th head norm tensor(2.6559)\n",
      "24th Layer 4th head norm tensor(2.3511)\n",
      "24th Layer 5th head norm tensor(2.3245)\n",
      "24th Layer 6th head norm tensor(2.3608)\n",
      "24th Layer 7th head norm tensor(2.2527)\n",
      "24th Layer 8th head norm tensor(2.3417)\n",
      "24th Layer 9th head norm tensor(2.2541)\n",
      "24th Layer 10th head norm tensor(2.3009)\n",
      "24th Layer 11th head norm tensor(2.3492)\n",
      "24th Layer 12th head norm tensor(3.1543)\n",
      "24th Layer 13th head norm tensor(2.3481)\n",
      "24th Layer 14th head norm tensor(2.3049)\n",
      "24th Layer 15th head norm tensor(2.3316)\n",
      "24th Layer 16th head norm tensor(2.4978)\n",
      "24th Layer 17th head norm tensor(2.3010)\n",
      "24th Layer 18th head norm tensor(2.5411)\n",
      "24th Layer 19th head norm tensor(2.3751)\n",
      "24th Layer 20th head norm tensor(2.2598)\n",
      "24th Layer 21th head norm tensor(2.3111)\n",
      "24th Layer 22th head norm tensor(2.2486)\n",
      "24th Layer 23th head norm tensor(2.2732)\n",
      "#####25th layer#####\n",
      "25th Layer 0th head norm tensor(1.5256)\n",
      "25th Layer 1th head norm tensor(1.5610)\n",
      "25th Layer 2th head norm tensor(1.4758)\n",
      "25th Layer 3th head norm tensor(1.5050)\n",
      "25th Layer 4th head norm tensor(1.5619)\n",
      "25th Layer 5th head norm tensor(1.5248)\n",
      "25th Layer 6th head norm tensor(1.5250)\n",
      "25th Layer 7th head norm tensor(1.5074)\n",
      "25th Layer 8th head norm tensor(1.5284)\n",
      "25th Layer 9th head norm tensor(1.5336)\n",
      "25th Layer 10th head norm tensor(1.5140)\n",
      "25th Layer 11th head norm tensor(1.5219)\n",
      "25th Layer 12th head norm tensor(1.5326)\n",
      "25th Layer 13th head norm tensor(1.4906)\n",
      "25th Layer 14th head norm tensor(1.5153)\n",
      "25th Layer 15th head norm tensor(1.5389)\n",
      "25th Layer 16th head norm tensor(1.6031)\n",
      "25th Layer 17th head norm tensor(1.5358)\n",
      "25th Layer 18th head norm tensor(1.5315)\n",
      "25th Layer 19th head norm tensor(1.5359)\n",
      "25th Layer 20th head norm tensor(1.5189)\n",
      "25th Layer 21th head norm tensor(1.5390)\n",
      "25th Layer 22th head norm tensor(1.4851)\n",
      "25th Layer 23th head norm tensor(1.5077)\n",
      "#####26th layer#####\n",
      "26th Layer 0th head norm tensor(1.7216)\n",
      "26th Layer 1th head norm tensor(1.6793)\n",
      "26th Layer 2th head norm tensor(1.6666)\n",
      "26th Layer 3th head norm tensor(1.7067)\n",
      "26th Layer 4th head norm tensor(1.7387)\n",
      "26th Layer 5th head norm tensor(1.6881)\n",
      "26th Layer 6th head norm tensor(1.6893)\n",
      "26th Layer 7th head norm tensor(1.6741)\n",
      "26th Layer 8th head norm tensor(1.6646)\n",
      "26th Layer 9th head norm tensor(1.6809)\n",
      "26th Layer 10th head norm tensor(1.7124)\n",
      "26th Layer 11th head norm tensor(1.6799)\n",
      "26th Layer 12th head norm tensor(1.6941)\n",
      "26th Layer 13th head norm tensor(1.6654)\n",
      "26th Layer 14th head norm tensor(1.7186)\n",
      "26th Layer 15th head norm tensor(1.7189)\n",
      "26th Layer 16th head norm tensor(1.6866)\n",
      "26th Layer 17th head norm tensor(1.6938)\n",
      "26th Layer 18th head norm tensor(1.6918)\n",
      "26th Layer 19th head norm tensor(1.7032)\n",
      "26th Layer 20th head norm tensor(1.6615)\n",
      "26th Layer 21th head norm tensor(1.7180)\n",
      "26th Layer 22th head norm tensor(1.6832)\n",
      "26th Layer 23th head norm tensor(1.6863)\n",
      "#####27th layer#####\n",
      "27th Layer 0th head norm tensor(1.7782)\n",
      "27th Layer 1th head norm tensor(1.7405)\n",
      "27th Layer 2th head norm tensor(1.6794)\n",
      "27th Layer 3th head norm tensor(1.6803)\n",
      "27th Layer 4th head norm tensor(1.7194)\n",
      "27th Layer 5th head norm tensor(1.7859)\n",
      "27th Layer 6th head norm tensor(1.7284)\n",
      "27th Layer 7th head norm tensor(1.7038)\n",
      "27th Layer 8th head norm tensor(1.6847)\n",
      "27th Layer 9th head norm tensor(1.7033)\n",
      "27th Layer 10th head norm tensor(1.7162)\n",
      "27th Layer 11th head norm tensor(1.7246)\n",
      "27th Layer 12th head norm tensor(1.6980)\n",
      "27th Layer 13th head norm tensor(1.7218)\n",
      "27th Layer 14th head norm tensor(1.6866)\n",
      "27th Layer 15th head norm tensor(1.7312)\n",
      "27th Layer 16th head norm tensor(1.7635)\n",
      "27th Layer 17th head norm tensor(1.7376)\n",
      "27th Layer 18th head norm tensor(1.7374)\n",
      "27th Layer 19th head norm tensor(1.7136)\n",
      "27th Layer 20th head norm tensor(1.6871)\n",
      "27th Layer 21th head norm tensor(1.7885)\n",
      "27th Layer 22th head norm tensor(1.7061)\n",
      "27th Layer 23th head norm tensor(1.7169)\n",
      "#####28th layer#####\n",
      "28th Layer 0th head norm tensor(1.7802)\n",
      "28th Layer 1th head norm tensor(1.8742)\n",
      "28th Layer 2th head norm tensor(1.7814)\n",
      "28th Layer 3th head norm tensor(1.7638)\n",
      "28th Layer 4th head norm tensor(1.8164)\n",
      "28th Layer 5th head norm tensor(1.8171)\n",
      "28th Layer 6th head norm tensor(1.8086)\n",
      "28th Layer 7th head norm tensor(1.7580)\n",
      "28th Layer 8th head norm tensor(1.7799)\n",
      "28th Layer 9th head norm tensor(1.7834)\n",
      "28th Layer 10th head norm tensor(1.7962)\n",
      "28th Layer 11th head norm tensor(1.7950)\n",
      "28th Layer 12th head norm tensor(1.9283)\n",
      "28th Layer 13th head norm tensor(1.7608)\n",
      "28th Layer 14th head norm tensor(1.7463)\n",
      "28th Layer 15th head norm tensor(1.7718)\n",
      "28th Layer 16th head norm tensor(1.7785)\n",
      "28th Layer 17th head norm tensor(1.7987)\n",
      "28th Layer 18th head norm tensor(1.7872)\n",
      "28th Layer 19th head norm tensor(1.7731)\n",
      "28th Layer 20th head norm tensor(1.7667)\n",
      "28th Layer 21th head norm tensor(1.8000)\n",
      "28th Layer 22th head norm tensor(1.7900)\n",
      "28th Layer 23th head norm tensor(1.8078)\n",
      "#####29th layer#####\n",
      "29th Layer 0th head norm tensor(1.8006)\n",
      "29th Layer 1th head norm tensor(1.7937)\n",
      "29th Layer 2th head norm tensor(1.7664)\n",
      "29th Layer 3th head norm tensor(1.7496)\n",
      "29th Layer 4th head norm tensor(1.7723)\n",
      "29th Layer 5th head norm tensor(1.7751)\n",
      "29th Layer 6th head norm tensor(1.7808)\n",
      "29th Layer 7th head norm tensor(1.7531)\n",
      "29th Layer 8th head norm tensor(1.7735)\n",
      "29th Layer 9th head norm tensor(1.7763)\n",
      "29th Layer 10th head norm tensor(1.7797)\n",
      "29th Layer 11th head norm tensor(1.7720)\n",
      "29th Layer 12th head norm tensor(2.2334)\n",
      "29th Layer 13th head norm tensor(1.7487)\n",
      "29th Layer 14th head norm tensor(1.7589)\n",
      "29th Layer 15th head norm tensor(1.7846)\n",
      "29th Layer 16th head norm tensor(1.7572)\n",
      "29th Layer 17th head norm tensor(1.7941)\n",
      "29th Layer 18th head norm tensor(1.7807)\n",
      "29th Layer 19th head norm tensor(1.7885)\n",
      "29th Layer 20th head norm tensor(1.9066)\n",
      "29th Layer 21th head norm tensor(1.9031)\n",
      "29th Layer 22th head norm tensor(1.7628)\n",
      "29th Layer 23th head norm tensor(1.7806)\n",
      "#####30th layer#####\n",
      "30th Layer 0th head norm tensor(1.8459)\n",
      "30th Layer 1th head norm tensor(1.9519)\n",
      "30th Layer 2th head norm tensor(1.8131)\n",
      "30th Layer 3th head norm tensor(1.7877)\n",
      "30th Layer 4th head norm tensor(1.8361)\n",
      "30th Layer 5th head norm tensor(1.8358)\n",
      "30th Layer 6th head norm tensor(1.8400)\n",
      "30th Layer 7th head norm tensor(1.8478)\n",
      "30th Layer 8th head norm tensor(1.8082)\n",
      "30th Layer 9th head norm tensor(1.8865)\n",
      "30th Layer 10th head norm tensor(1.8003)\n",
      "30th Layer 11th head norm tensor(1.8139)\n",
      "30th Layer 12th head norm tensor(1.9245)\n",
      "30th Layer 13th head norm tensor(1.7949)\n",
      "30th Layer 14th head norm tensor(1.7986)\n",
      "30th Layer 15th head norm tensor(1.7801)\n",
      "30th Layer 16th head norm tensor(1.7843)\n",
      "30th Layer 17th head norm tensor(1.8585)\n",
      "30th Layer 18th head norm tensor(1.8004)\n",
      "30th Layer 19th head norm tensor(1.8217)\n",
      "30th Layer 20th head norm tensor(1.8084)\n",
      "30th Layer 21th head norm tensor(1.8378)\n",
      "30th Layer 22th head norm tensor(1.8057)\n",
      "30th Layer 23th head norm tensor(1.8013)\n",
      "#####31th layer#####\n",
      "31th Layer 0th head norm tensor(1.8843)\n",
      "31th Layer 1th head norm tensor(1.8437)\n",
      "31th Layer 2th head norm tensor(1.7854)\n",
      "31th Layer 3th head norm tensor(1.7937)\n",
      "31th Layer 4th head norm tensor(2.0845)\n",
      "31th Layer 5th head norm tensor(1.8835)\n",
      "31th Layer 6th head norm tensor(1.8211)\n",
      "31th Layer 7th head norm tensor(1.8086)\n",
      "31th Layer 8th head norm tensor(1.8354)\n",
      "31th Layer 9th head norm tensor(1.8110)\n",
      "31th Layer 10th head norm tensor(1.8639)\n",
      "31th Layer 11th head norm tensor(1.8172)\n",
      "31th Layer 12th head norm tensor(1.7715)\n",
      "31th Layer 13th head norm tensor(1.7742)\n",
      "31th Layer 14th head norm tensor(1.8182)\n",
      "31th Layer 15th head norm tensor(1.8261)\n",
      "31th Layer 16th head norm tensor(1.8354)\n",
      "31th Layer 17th head norm tensor(1.8460)\n",
      "31th Layer 18th head norm tensor(1.8337)\n",
      "31th Layer 19th head norm tensor(1.8246)\n",
      "31th Layer 20th head norm tensor(1.8374)\n",
      "31th Layer 21th head norm tensor(1.8374)\n",
      "31th Layer 22th head norm tensor(1.7975)\n",
      "31th Layer 23th head norm tensor(1.8473)\n",
      "#####32th layer#####\n",
      "32th Layer 0th head norm tensor(1.9045)\n",
      "32th Layer 1th head norm tensor(1.8266)\n",
      "32th Layer 2th head norm tensor(2.1046)\n",
      "32th Layer 3th head norm tensor(1.8169)\n",
      "32th Layer 4th head norm tensor(1.8033)\n",
      "32th Layer 5th head norm tensor(1.8224)\n",
      "32th Layer 6th head norm tensor(1.8511)\n",
      "32th Layer 7th head norm tensor(1.8349)\n",
      "32th Layer 8th head norm tensor(1.9721)\n",
      "32th Layer 9th head norm tensor(1.8355)\n",
      "32th Layer 10th head norm tensor(1.8589)\n",
      "32th Layer 11th head norm tensor(1.8591)\n",
      "32th Layer 12th head norm tensor(1.8077)\n",
      "32th Layer 13th head norm tensor(1.8217)\n",
      "32th Layer 14th head norm tensor(1.8409)\n",
      "32th Layer 15th head norm tensor(1.8411)\n",
      "32th Layer 16th head norm tensor(1.8339)\n",
      "32th Layer 17th head norm tensor(1.8799)\n",
      "32th Layer 18th head norm tensor(1.8696)\n",
      "32th Layer 19th head norm tensor(1.8404)\n",
      "32th Layer 20th head norm tensor(1.8220)\n",
      "32th Layer 21th head norm tensor(1.8569)\n",
      "32th Layer 22th head norm tensor(1.8223)\n",
      "32th Layer 23th head norm tensor(1.8651)\n",
      "#####33th layer#####\n",
      "33th Layer 0th head norm tensor(1.8793)\n",
      "33th Layer 1th head norm tensor(1.8576)\n",
      "33th Layer 2th head norm tensor(2.1466)\n",
      "33th Layer 3th head norm tensor(1.8545)\n",
      "33th Layer 4th head norm tensor(1.8390)\n",
      "33th Layer 5th head norm tensor(1.8447)\n",
      "33th Layer 6th head norm tensor(1.8505)\n",
      "33th Layer 7th head norm tensor(1.8712)\n",
      "33th Layer 8th head norm tensor(2.0169)\n",
      "33th Layer 9th head norm tensor(1.8468)\n",
      "33th Layer 10th head norm tensor(1.8607)\n",
      "33th Layer 11th head norm tensor(1.8636)\n",
      "33th Layer 12th head norm tensor(1.8283)\n",
      "33th Layer 13th head norm tensor(1.8466)\n",
      "33th Layer 14th head norm tensor(1.8412)\n",
      "33th Layer 15th head norm tensor(1.8642)\n",
      "33th Layer 16th head norm tensor(1.8467)\n",
      "33th Layer 17th head norm tensor(1.8659)\n",
      "33th Layer 18th head norm tensor(1.8072)\n",
      "33th Layer 19th head norm tensor(1.8606)\n",
      "33th Layer 20th head norm tensor(1.8297)\n",
      "33th Layer 21th head norm tensor(1.8788)\n",
      "33th Layer 22th head norm tensor(1.8568)\n",
      "33th Layer 23th head norm tensor(1.9258)\n",
      "#####34th layer#####\n",
      "34th Layer 0th head norm tensor(1.8517)\n",
      "34th Layer 1th head norm tensor(1.8568)\n",
      "34th Layer 2th head norm tensor(1.9033)\n",
      "34th Layer 3th head norm tensor(1.8099)\n",
      "34th Layer 4th head norm tensor(1.8281)\n",
      "34th Layer 5th head norm tensor(1.8660)\n",
      "34th Layer 6th head norm tensor(1.8652)\n",
      "34th Layer 7th head norm tensor(1.8264)\n",
      "34th Layer 8th head norm tensor(1.8507)\n",
      "34th Layer 9th head norm tensor(2.4994)\n",
      "34th Layer 10th head norm tensor(1.8339)\n",
      "34th Layer 11th head norm tensor(1.8387)\n",
      "34th Layer 12th head norm tensor(1.8315)\n",
      "34th Layer 13th head norm tensor(1.8165)\n",
      "34th Layer 14th head norm tensor(1.8533)\n",
      "34th Layer 15th head norm tensor(1.8371)\n",
      "34th Layer 16th head norm tensor(1.8160)\n",
      "34th Layer 17th head norm tensor(1.8379)\n",
      "34th Layer 18th head norm tensor(1.8160)\n",
      "34th Layer 19th head norm tensor(1.8582)\n",
      "34th Layer 20th head norm tensor(1.8372)\n",
      "34th Layer 21th head norm tensor(1.8666)\n",
      "34th Layer 22th head norm tensor(1.8900)\n",
      "34th Layer 23th head norm tensor(1.8248)\n",
      "#####35th layer#####\n",
      "35th Layer 0th head norm tensor(1.7578)\n",
      "35th Layer 1th head norm tensor(1.7353)\n",
      "35th Layer 2th head norm tensor(1.7261)\n",
      "35th Layer 3th head norm tensor(1.9586)\n",
      "35th Layer 4th head norm tensor(1.9363)\n",
      "35th Layer 5th head norm tensor(1.7201)\n",
      "35th Layer 6th head norm tensor(2.9038)\n",
      "35th Layer 7th head norm tensor(1.8285)\n",
      "35th Layer 8th head norm tensor(3.5653)\n",
      "35th Layer 9th head norm tensor(2.9033)\n",
      "35th Layer 10th head norm tensor(2.3619)\n",
      "35th Layer 11th head norm tensor(2.0860)\n",
      "35th Layer 12th head norm tensor(2.6694)\n",
      "35th Layer 13th head norm tensor(1.9967)\n",
      "35th Layer 14th head norm tensor(2.7535)\n",
      "35th Layer 15th head norm tensor(1.7365)\n",
      "35th Layer 16th head norm tensor(1.8123)\n",
      "35th Layer 17th head norm tensor(1.7408)\n",
      "35th Layer 18th head norm tensor(1.7220)\n",
      "35th Layer 19th head norm tensor(3.3029)\n",
      "35th Layer 20th head norm tensor(1.8292)\n",
      "35th Layer 21th head norm tensor(1.7581)\n",
      "35th Layer 22th head norm tensor(1.7243)\n",
      "35th Layer 23th head norm tensor(1.7601)\n",
      "#####36th layer#####\n",
      "36th Layer 0th head norm tensor(2.8587)\n",
      "36th Layer 1th head norm tensor(2.3749)\n",
      "36th Layer 2th head norm tensor(2.2634)\n",
      "36th Layer 3th head norm tensor(2.6536)\n",
      "36th Layer 4th head norm tensor(2.3474)\n",
      "36th Layer 5th head norm tensor(2.3240)\n",
      "36th Layer 6th head norm tensor(2.3535)\n",
      "36th Layer 7th head norm tensor(2.2492)\n",
      "36th Layer 8th head norm tensor(2.3402)\n",
      "36th Layer 9th head norm tensor(2.2546)\n",
      "36th Layer 10th head norm tensor(2.3017)\n",
      "36th Layer 11th head norm tensor(2.3484)\n",
      "36th Layer 12th head norm tensor(3.1535)\n",
      "36th Layer 13th head norm tensor(2.3442)\n",
      "36th Layer 14th head norm tensor(2.3004)\n",
      "36th Layer 15th head norm tensor(2.3270)\n",
      "36th Layer 16th head norm tensor(2.4977)\n",
      "36th Layer 17th head norm tensor(2.3004)\n",
      "36th Layer 18th head norm tensor(2.5389)\n",
      "36th Layer 19th head norm tensor(2.3976)\n",
      "36th Layer 20th head norm tensor(2.2590)\n",
      "36th Layer 21th head norm tensor(2.3076)\n",
      "36th Layer 22th head norm tensor(2.2478)\n",
      "36th Layer 23th head norm tensor(2.2718)\n",
      "#####37th layer#####\n",
      "37th Layer 0th head norm tensor(1.5161)\n",
      "37th Layer 1th head norm tensor(1.5705)\n",
      "37th Layer 2th head norm tensor(1.4727)\n",
      "37th Layer 3th head norm tensor(1.4973)\n",
      "37th Layer 4th head norm tensor(1.5558)\n",
      "37th Layer 5th head norm tensor(1.5257)\n",
      "37th Layer 6th head norm tensor(1.5269)\n",
      "37th Layer 7th head norm tensor(1.4972)\n",
      "37th Layer 8th head norm tensor(1.5277)\n",
      "37th Layer 9th head norm tensor(1.5285)\n",
      "37th Layer 10th head norm tensor(1.5206)\n",
      "37th Layer 11th head norm tensor(1.5214)\n",
      "37th Layer 12th head norm tensor(1.5275)\n",
      "37th Layer 13th head norm tensor(1.4868)\n",
      "37th Layer 14th head norm tensor(1.5074)\n",
      "37th Layer 15th head norm tensor(1.5500)\n",
      "37th Layer 16th head norm tensor(1.5896)\n",
      "37th Layer 17th head norm tensor(1.5362)\n",
      "37th Layer 18th head norm tensor(1.5366)\n",
      "37th Layer 19th head norm tensor(1.5343)\n",
      "37th Layer 20th head norm tensor(1.5175)\n",
      "37th Layer 21th head norm tensor(1.5416)\n",
      "37th Layer 22th head norm tensor(1.4804)\n",
      "37th Layer 23th head norm tensor(1.5146)\n",
      "#####38th layer#####\n",
      "38th Layer 0th head norm tensor(1.7173)\n",
      "38th Layer 1th head norm tensor(1.6957)\n",
      "38th Layer 2th head norm tensor(1.6748)\n",
      "38th Layer 3th head norm tensor(1.7178)\n",
      "38th Layer 4th head norm tensor(1.7253)\n",
      "38th Layer 5th head norm tensor(1.7002)\n",
      "38th Layer 6th head norm tensor(1.6966)\n",
      "38th Layer 7th head norm tensor(1.6804)\n",
      "38th Layer 8th head norm tensor(1.6714)\n",
      "38th Layer 9th head norm tensor(1.6884)\n",
      "38th Layer 10th head norm tensor(1.7250)\n",
      "38th Layer 11th head norm tensor(1.6823)\n",
      "38th Layer 12th head norm tensor(1.6973)\n",
      "38th Layer 13th head norm tensor(1.6626)\n",
      "38th Layer 14th head norm tensor(1.7068)\n",
      "38th Layer 15th head norm tensor(1.7369)\n",
      "38th Layer 16th head norm tensor(1.6843)\n",
      "38th Layer 17th head norm tensor(1.6949)\n",
      "38th Layer 18th head norm tensor(1.6964)\n",
      "38th Layer 19th head norm tensor(1.7047)\n",
      "38th Layer 20th head norm tensor(1.6755)\n",
      "38th Layer 21th head norm tensor(1.7165)\n",
      "38th Layer 22th head norm tensor(1.6862)\n",
      "38th Layer 23th head norm tensor(1.6957)\n",
      "#####39th layer#####\n",
      "39th Layer 0th head norm tensor(1.7834)\n",
      "39th Layer 1th head norm tensor(1.7436)\n",
      "39th Layer 2th head norm tensor(1.6899)\n",
      "39th Layer 3th head norm tensor(1.6828)\n",
      "39th Layer 4th head norm tensor(1.7259)\n",
      "39th Layer 5th head norm tensor(1.7916)\n",
      "39th Layer 6th head norm tensor(1.7269)\n",
      "39th Layer 7th head norm tensor(1.6962)\n",
      "39th Layer 8th head norm tensor(1.6870)\n",
      "39th Layer 9th head norm tensor(1.6998)\n",
      "39th Layer 10th head norm tensor(1.7313)\n",
      "39th Layer 11th head norm tensor(1.7353)\n",
      "39th Layer 12th head norm tensor(1.7134)\n",
      "39th Layer 13th head norm tensor(1.7252)\n",
      "39th Layer 14th head norm tensor(1.6972)\n",
      "39th Layer 15th head norm tensor(1.7429)\n",
      "39th Layer 16th head norm tensor(1.7646)\n",
      "39th Layer 17th head norm tensor(1.7443)\n",
      "39th Layer 18th head norm tensor(1.7212)\n",
      "39th Layer 19th head norm tensor(1.7217)\n",
      "39th Layer 20th head norm tensor(1.6948)\n",
      "39th Layer 21th head norm tensor(1.8019)\n",
      "39th Layer 22th head norm tensor(1.7134)\n",
      "39th Layer 23th head norm tensor(1.7188)\n",
      "#####40th layer#####\n",
      "40th Layer 0th head norm tensor(1.7695)\n",
      "40th Layer 1th head norm tensor(1.8857)\n",
      "40th Layer 2th head norm tensor(1.7915)\n",
      "40th Layer 3th head norm tensor(1.7536)\n",
      "40th Layer 4th head norm tensor(1.8158)\n",
      "40th Layer 5th head norm tensor(1.8086)\n",
      "40th Layer 6th head norm tensor(1.8115)\n",
      "40th Layer 7th head norm tensor(1.7573)\n",
      "40th Layer 8th head norm tensor(1.7838)\n",
      "40th Layer 9th head norm tensor(1.7809)\n",
      "40th Layer 10th head norm tensor(1.7886)\n",
      "40th Layer 11th head norm tensor(1.7858)\n",
      "40th Layer 12th head norm tensor(1.9218)\n",
      "40th Layer 13th head norm tensor(1.7731)\n",
      "40th Layer 14th head norm tensor(1.7404)\n",
      "40th Layer 15th head norm tensor(1.7686)\n",
      "40th Layer 16th head norm tensor(1.7842)\n",
      "40th Layer 17th head norm tensor(1.7899)\n",
      "40th Layer 18th head norm tensor(1.7974)\n",
      "40th Layer 19th head norm tensor(1.7771)\n",
      "40th Layer 20th head norm tensor(1.7683)\n",
      "40th Layer 21th head norm tensor(1.7846)\n",
      "40th Layer 22th head norm tensor(1.7742)\n",
      "40th Layer 23th head norm tensor(1.7977)\n",
      "#####41th layer#####\n",
      "41th Layer 0th head norm tensor(1.7999)\n",
      "41th Layer 1th head norm tensor(1.7831)\n",
      "41th Layer 2th head norm tensor(1.7683)\n",
      "41th Layer 3th head norm tensor(1.7628)\n",
      "41th Layer 4th head norm tensor(1.7754)\n",
      "41th Layer 5th head norm tensor(1.7699)\n",
      "41th Layer 6th head norm tensor(1.7829)\n",
      "41th Layer 7th head norm tensor(1.7474)\n",
      "41th Layer 8th head norm tensor(1.7827)\n",
      "41th Layer 9th head norm tensor(1.7860)\n",
      "41th Layer 10th head norm tensor(1.7810)\n",
      "41th Layer 11th head norm tensor(1.7741)\n",
      "41th Layer 12th head norm tensor(2.2528)\n",
      "41th Layer 13th head norm tensor(1.7607)\n",
      "41th Layer 14th head norm tensor(1.7732)\n",
      "41th Layer 15th head norm tensor(1.8113)\n",
      "41th Layer 16th head norm tensor(1.7558)\n",
      "41th Layer 17th head norm tensor(1.7965)\n",
      "41th Layer 18th head norm tensor(1.7744)\n",
      "41th Layer 19th head norm tensor(1.7980)\n",
      "41th Layer 20th head norm tensor(1.9131)\n",
      "41th Layer 21th head norm tensor(1.9281)\n",
      "41th Layer 22th head norm tensor(1.7577)\n",
      "41th Layer 23th head norm tensor(1.7819)\n",
      "#####42th layer#####\n",
      "42th Layer 0th head norm tensor(1.8526)\n",
      "42th Layer 1th head norm tensor(1.9490)\n",
      "42th Layer 2th head norm tensor(1.8176)\n",
      "42th Layer 3th head norm tensor(1.7950)\n",
      "42th Layer 4th head norm tensor(1.8504)\n",
      "42th Layer 5th head norm tensor(1.8332)\n",
      "42th Layer 6th head norm tensor(1.8353)\n",
      "42th Layer 7th head norm tensor(1.8418)\n",
      "42th Layer 8th head norm tensor(1.8082)\n",
      "42th Layer 9th head norm tensor(1.8983)\n",
      "42th Layer 10th head norm tensor(1.8174)\n",
      "42th Layer 11th head norm tensor(1.8122)\n",
      "42th Layer 12th head norm tensor(1.9732)\n",
      "42th Layer 13th head norm tensor(1.7903)\n",
      "42th Layer 14th head norm tensor(1.7963)\n",
      "42th Layer 15th head norm tensor(1.7859)\n",
      "42th Layer 16th head norm tensor(1.7918)\n",
      "42th Layer 17th head norm tensor(1.8803)\n",
      "42th Layer 18th head norm tensor(1.7878)\n",
      "42th Layer 19th head norm tensor(1.8222)\n",
      "42th Layer 20th head norm tensor(1.8177)\n",
      "42th Layer 21th head norm tensor(1.8393)\n",
      "42th Layer 22th head norm tensor(1.8078)\n",
      "42th Layer 23th head norm tensor(1.8063)\n",
      "#####43th layer#####\n",
      "43th Layer 0th head norm tensor(1.8766)\n",
      "43th Layer 1th head norm tensor(1.8626)\n",
      "43th Layer 2th head norm tensor(1.8015)\n",
      "43th Layer 3th head norm tensor(1.8150)\n",
      "43th Layer 4th head norm tensor(2.0781)\n",
      "43th Layer 5th head norm tensor(1.8960)\n",
      "43th Layer 6th head norm tensor(1.8233)\n",
      "43th Layer 7th head norm tensor(1.8055)\n",
      "43th Layer 8th head norm tensor(1.8490)\n",
      "43th Layer 9th head norm tensor(1.8379)\n",
      "43th Layer 10th head norm tensor(1.8679)\n",
      "43th Layer 11th head norm tensor(1.8266)\n",
      "43th Layer 12th head norm tensor(1.7788)\n",
      "43th Layer 13th head norm tensor(1.7667)\n",
      "43th Layer 14th head norm tensor(1.8289)\n",
      "43th Layer 15th head norm tensor(1.8295)\n",
      "43th Layer 16th head norm tensor(1.8448)\n",
      "43th Layer 17th head norm tensor(1.8374)\n",
      "43th Layer 18th head norm tensor(1.8469)\n",
      "43th Layer 19th head norm tensor(1.8471)\n",
      "43th Layer 20th head norm tensor(1.8471)\n",
      "43th Layer 21th head norm tensor(1.8535)\n",
      "43th Layer 22th head norm tensor(1.8071)\n",
      "43th Layer 23th head norm tensor(1.8500)\n",
      "#####44th layer#####\n",
      "44th Layer 0th head norm tensor(1.9146)\n",
      "44th Layer 1th head norm tensor(1.8483)\n",
      "44th Layer 2th head norm tensor(2.1371)\n",
      "44th Layer 3th head norm tensor(1.8394)\n",
      "44th Layer 4th head norm tensor(1.8229)\n",
      "44th Layer 5th head norm tensor(1.8452)\n",
      "44th Layer 6th head norm tensor(1.8699)\n",
      "44th Layer 7th head norm tensor(1.8512)\n",
      "44th Layer 8th head norm tensor(1.9916)\n",
      "44th Layer 9th head norm tensor(1.8455)\n",
      "44th Layer 10th head norm tensor(1.8645)\n",
      "44th Layer 11th head norm tensor(1.8607)\n",
      "44th Layer 12th head norm tensor(1.8119)\n",
      "44th Layer 13th head norm tensor(1.8237)\n",
      "44th Layer 14th head norm tensor(1.8408)\n",
      "44th Layer 15th head norm tensor(1.8552)\n",
      "44th Layer 16th head norm tensor(1.8515)\n",
      "44th Layer 17th head norm tensor(1.8822)\n",
      "44th Layer 18th head norm tensor(1.8890)\n",
      "44th Layer 19th head norm tensor(1.8789)\n",
      "44th Layer 20th head norm tensor(1.8478)\n",
      "44th Layer 21th head norm tensor(1.8578)\n",
      "44th Layer 22th head norm tensor(1.8237)\n",
      "44th Layer 23th head norm tensor(1.8730)\n",
      "#####45th layer#####\n",
      "45th Layer 0th head norm tensor(1.8748)\n",
      "45th Layer 1th head norm tensor(1.8726)\n",
      "45th Layer 2th head norm tensor(2.1506)\n",
      "45th Layer 3th head norm tensor(1.8710)\n",
      "45th Layer 4th head norm tensor(1.8595)\n",
      "45th Layer 5th head norm tensor(1.8601)\n",
      "45th Layer 6th head norm tensor(1.8546)\n",
      "45th Layer 7th head norm tensor(1.8776)\n",
      "45th Layer 8th head norm tensor(2.0214)\n",
      "45th Layer 9th head norm tensor(1.8592)\n",
      "45th Layer 10th head norm tensor(1.8635)\n",
      "45th Layer 11th head norm tensor(1.8878)\n",
      "45th Layer 12th head norm tensor(1.8358)\n",
      "45th Layer 13th head norm tensor(1.8449)\n",
      "45th Layer 14th head norm tensor(1.8844)\n",
      "45th Layer 15th head norm tensor(1.8767)\n",
      "45th Layer 16th head norm tensor(1.8423)\n",
      "45th Layer 17th head norm tensor(1.8846)\n",
      "45th Layer 18th head norm tensor(1.8288)\n",
      "45th Layer 19th head norm tensor(1.8555)\n",
      "45th Layer 20th head norm tensor(1.8476)\n",
      "45th Layer 21th head norm tensor(1.8813)\n",
      "45th Layer 22th head norm tensor(1.8729)\n",
      "45th Layer 23th head norm tensor(1.9458)\n",
      "#####46th layer#####\n",
      "46th Layer 0th head norm tensor(1.8901)\n",
      "46th Layer 1th head norm tensor(1.8786)\n",
      "46th Layer 2th head norm tensor(1.9309)\n",
      "46th Layer 3th head norm tensor(1.8316)\n",
      "46th Layer 4th head norm tensor(1.8578)\n",
      "46th Layer 5th head norm tensor(1.8753)\n",
      "46th Layer 6th head norm tensor(1.8768)\n",
      "46th Layer 7th head norm tensor(1.8649)\n",
      "46th Layer 8th head norm tensor(1.8795)\n",
      "46th Layer 9th head norm tensor(2.5917)\n",
      "46th Layer 10th head norm tensor(1.8556)\n",
      "46th Layer 11th head norm tensor(1.8835)\n",
      "46th Layer 12th head norm tensor(1.8431)\n",
      "46th Layer 13th head norm tensor(1.8553)\n",
      "46th Layer 14th head norm tensor(1.8885)\n",
      "46th Layer 15th head norm tensor(1.8700)\n",
      "46th Layer 16th head norm tensor(1.8402)\n",
      "46th Layer 17th head norm tensor(1.8504)\n",
      "46th Layer 18th head norm tensor(1.8571)\n",
      "46th Layer 19th head norm tensor(1.8703)\n",
      "46th Layer 20th head norm tensor(1.8664)\n",
      "46th Layer 21th head norm tensor(1.8999)\n",
      "46th Layer 22th head norm tensor(1.9390)\n",
      "46th Layer 23th head norm tensor(1.8557)\n",
      "#####47th layer#####\n",
      "47th Layer 0th head norm tensor(1.9745)\n",
      "47th Layer 1th head norm tensor(1.8151)\n",
      "47th Layer 2th head norm tensor(1.8524)\n",
      "47th Layer 3th head norm tensor(3.0701)\n",
      "47th Layer 4th head norm tensor(3.5427)\n",
      "47th Layer 5th head norm tensor(1.7751)\n",
      "47th Layer 6th head norm tensor(6.0277)\n",
      "47th Layer 7th head norm tensor(2.6473)\n",
      "47th Layer 8th head norm tensor(6.9775)\n",
      "47th Layer 9th head norm tensor(5.4226)\n",
      "47th Layer 10th head norm tensor(3.9573)\n",
      "47th Layer 11th head norm tensor(3.4608)\n",
      "47th Layer 12th head norm tensor(3.7087)\n",
      "47th Layer 13th head norm tensor(3.5536)\n",
      "47th Layer 14th head norm tensor(4.6022)\n",
      "47th Layer 15th head norm tensor(2.1804)\n",
      "47th Layer 16th head norm tensor(2.6788)\n",
      "47th Layer 17th head norm tensor(1.8791)\n",
      "47th Layer 18th head norm tensor(1.7811)\n",
      "47th Layer 19th head norm tensor(7.0563)\n",
      "47th Layer 20th head norm tensor(3.0673)\n",
      "47th Layer 21th head norm tensor(1.9371)\n",
      "47th Layer 22th head norm tensor(1.8069)\n",
      "47th Layer 23th head norm tensor(2.2780)\n"
     ]
    }
   ],
   "source": [
    "for i in range(48):\n",
    "    q_loraA_head_list = []\n",
    "    v_loraA_head_list = []\n",
    "    q_weight = model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.weight.T\n",
    "    v_weight = model.base_model.model.deberta.encoder.layer[i].attention.self.value_proj.weight.T\n",
    "    all_q_head = copy.deepcopy(q_weight)\n",
    "    all_q_head = all_q_head.reshape(24, 1536, 64)\n",
    "    all_v_head = copy.deepcopy(v_weight)\n",
    "    all_v_head = all_v_head.reshape(24, 1536, 64)\n",
    "    print(f\"#####{i}th layer#####\")\n",
    "    for j in range(24):\n",
    "        q_head = all_q_head[j]\n",
    "        q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "        q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "        print(f\"{i}th Layer {j}th head norm\", torch.norm(q_loraA_head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "head_data = []\n",
    "\n",
    "for i in range(48):\n",
    "    q_loraA_head_list = []\n",
    "    v_loraA_head_list = []\n",
    "    q_weight = model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.weight.T\n",
    "    v_weight = model.base_model.model.deberta.encoder.layer[i].attention.self.value_proj.weight.T\n",
    "    all_q_head = copy.deepcopy(q_weight)\n",
    "    all_q_head = all_q_head.reshape(24, 1536, 64)\n",
    "    all_v_head = copy.deepcopy(v_weight)\n",
    "    all_v_head = all_v_head.reshape(24, 1536, 64)\n",
    "    for j in range(24):\n",
    "        q_head = all_q_head[j]\n",
    "        q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "        q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "        norm = torch.norm(q_loraA_head).item()\n",
    "        # 각 layer 및 head의 norm 값을 데이터에 추가\n",
    "        head_data.append({\"Layer\": i, \"Head\": j, \"Norm\": norm})\n",
    "\n",
    "df = pd.DataFrame(head_data)\n",
    "print(df)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=\"Layer\", y=\"Norm\", data=df)\n",
    "plt.title(\"Norm Distribution by Layer and Head\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deberta_init_dW_A_with_svd_us_by_head_scalng(model, approx_rank):\n",
    "    len_of_layers = len(model.base_model.model.deberta.encoder.layer)\n",
    "    for layer_idx in range(len_of_layers):\n",
    "        print(f\"{layer_idx}th Layer \")\n",
    "        q_loraA_head_list = []\n",
    "        v_loraA_head_list = []\n",
    "\n",
    "        q_weight = model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.weight.T\n",
    "        v_weight = model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.weight.T\n",
    "\n",
    "        q_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.query_proj.lora_A.default.weight.T\n",
    "        v_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.value_proj.lora_A.default.weight.T\n",
    "\n",
    "        all_q_head = copy.deepcopy(q_weight)\n",
    "        all_q_head = all_q_head.reshape(approx_rank, 1536, 1536 // approx_rank)\n",
    "\n",
    "        all_v_head = copy.deepcopy(v_weight)\n",
    "        all_v_head = all_v_head.reshape(approx_rank, 1536, 1536 // approx_rank)\n",
    "\n",
    "        for j in range(approx_rank):\n",
    "            q_head = all_q_head[j]\n",
    "            q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "            q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "\n",
    "            v_head = all_v_head[j]\n",
    "            v_u, v_s, v_vt = torch.linalg.svd(v_head)\n",
    "            v_loraA_head = v_u[:, :1] @ torch.diag(v_s[:1])\n",
    "            print(\"-\" * 50)\n",
    "            print(\n",
    "                f\"{j}th head norm Before Scaling \\n q_loraA_head : {torch.norm(q_loraA_head)} v_loraA_head : {torch.norm(v_loraA_head)}\"\n",
    "            )\n",
    "            q_og_lora_A_icol_norm = torch.norm(q_og_lora_A[:, 1])\n",
    "            q_new_lora_A_icol_norm = torch.norm(q_new_lora_A[:, 1])\n",
    "            v_og_lora_A_icol_norm = torch.norm(v_og_lora_A[:, 1])\n",
    "            v_new_lora_A_icol_norm = torch.norm(v_new_lora_A[:, 1])\n",
    "\n",
    "            q_scale = q_og_lora_A_icol_norm / q_new_lora_A_icol_norm\n",
    "            v_scale = v_og_lora_A_icol_norm / v_new_lora_A_icol_norm\n",
    "\n",
    "            q_loraA_head = q_loraA_head * q_scale\n",
    "            v_loraA_head = v_loraA_head * v_scale\n",
    "\n",
    "            print(\n",
    "                f\"{j}th head norm After Scaling \\n q_loraA_head : {torch.norm(q_loraA_head)} v_loraA_head : {torch.norm(v_loraA_head)}\"\n",
    "            )\n",
    "            print(\"-\" * 50)\n",
    "            q_loraA_head_list.append(q_loraA_head)\n",
    "            v_loraA_head_list.append(v_loraA_head)\n",
    "\n",
    "        q_loraA = torch.cat(q_loraA_head_list, dim=1)\n",
    "        v_loraA = torch.cat(v_loraA_head_list, dim=1)\n",
    "\n",
    "        model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.query_proj.lora_A.default.weight.data = q_loraA.T.contiguous()\n",
    "        model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.value_proj.lora_A.default.weight.data = v_loraA.T.contiguous()\n",
    "\n",
    "        model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.query_proj.lora_B.default.weight.data = torch.zeros_like(\n",
    "            model.base_model.model.deberta.encoder.layer[\n",
    "                layer_idx\n",
    "            ].attention.self.query_proj.lora_B.default.weight.data\n",
    "        )\n",
    "        model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.value_proj.lora_B.default.weight.data = torch.zeros_like(\n",
    "            model.base_model.model.deberta.encoder.layer[\n",
    "                layer_idx\n",
    "            ].attention.self.value_proj.lora_B.default.weight.data\n",
    "        )\n",
    "\n",
    "        print(f\"Complete init LoraA! layer: {layer_idx}, q_loraA: {model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.query_proj.lora_A.default.weight.shape}, v_loraA: {model.base_model.model.deberta.encoder.layer[\n",
    "            layer_idx\n",
    "        ].attention.self.value_proj.lora_A.default.weight.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "approx_rank = 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v2-xxlarge and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 7080962 || all params: 1573994500 || trainable%: 0.4498720929456869\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/deberta-v2-xxlarge\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=2)\n",
    "config = LoraConfig(r=24, lora_alpha=24, target_modules=[\"query_proj\", \"value_proj\"], task_type=\"SEQ_CLS\")\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536, 24])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_w = model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A.default.weight.T\n",
    "tmp_w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8253, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(tmp_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1536])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_w[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0199, -0.0079, -0.0229,  ...,  0.0175,  0.0226,  0.0201],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_w[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0199],\n",
       "        [-0.0079],\n",
       "        [-0.0229],\n",
       "        ...,\n",
       "        [ 0.0175],\n",
       "        [ 0.0226],\n",
       "        [ 0.0201]], grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_w[:, 0 : 0 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5786, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5835, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5679, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5776, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5689, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5709, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5822, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5736, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5832, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5787, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5741, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5794, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5715, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5742, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5777, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5629, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5905, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5812, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5836, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5801, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5703, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5734, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5844, grad_fn=<LinalgVectorNormBackward0>)\n",
      "tensor(0.5717, grad_fn=<LinalgVectorNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(24):\n",
    "    print(torch.norm(tmp_w[:, i:i+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Merge Head! len of q_loraA_head_list: 24,len of v_loraA_head_list: 24\n",
      "Complete! Norm of q_loraA: 13.944880485534668, Norm of v_loraA: 7.784212112426758\n",
      "Complete Merge Head! len of q_loraA_head_list: 24,len of v_loraA_head_list: 24\n",
      "Complete! Norm of q_loraA: 8.423471450805664, Norm of v_loraA: 6.587897777557373\n",
      "Complete Merge Head! len of q_loraA_head_list: 24,len of v_loraA_head_list: 24\n",
      "Complete! Norm of q_loraA: 8.42933177947998, Norm of v_loraA: 7.465761661529541\n",
      "Complete Merge Head! len of q_loraA_head_list: 24,len of v_loraA_head_list: 24\n",
      "Complete! Norm of q_loraA: 8.418700218200684, Norm of v_loraA: 7.929988384246826\n",
      "Complete Merge Head! len of q_loraA_head_list: 24,len of v_loraA_head_list: 24\n",
      "Complete! Norm of q_loraA: 8.643241882324219, Norm of v_loraA: 7.301017761230469\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m v_weight \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdeberta\u001b[39m.\u001b[39mencoder\u001b[39m.\u001b[39mlayer[i]\u001b[39m.\u001b[39mattention\u001b[39m.\u001b[39mself\u001b[39m.\u001b[39mvalue_proj\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mT\n\u001b[1;32m      7\u001b[0m all_q_head \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(q_weight)\n\u001b[0;32m----> 8\u001b[0m all_q_head \u001b[39m=\u001b[39m all_q_head\u001b[39m.\u001b[39;49mreshape(approx_rank, \u001b[39m1536\u001b[39;49m, \u001b[39m1536\u001b[39;49m \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m approx_rank)\n\u001b[1;32m      9\u001b[0m all_v_head \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(v_weight)\n\u001b[1;32m     10\u001b[0m all_v_head \u001b[39m=\u001b[39m all_v_head\u001b[39m.\u001b[39mreshape(approx_rank, \u001b[39m1536\u001b[39m, \u001b[39m1536\u001b[39m \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m approx_rank)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len_of_layers = len(model.base_model.model.deberta.encoder.layer)\n",
    "for i in range(len_of_layers):\n",
    "    q_loraA_head_list = []\n",
    "    v_loraA_head_list = []\n",
    "    q_weight = model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.weight.T\n",
    "    v_weight = model.base_model.model.deberta.encoder.layer[i].attention.self.value_proj.weight.T\n",
    "    all_q_head = copy.deepcopy(q_weight)\n",
    "    all_q_head = all_q_head.reshape(approx_rank, 1536, 1536 // approx_rank)\n",
    "    all_v_head = copy.deepcopy(v_weight)\n",
    "    all_v_head = all_v_head.reshape(approx_rank, 1536, 1536 // approx_rank)\n",
    "    for j in range(approx_rank):\n",
    "        q_head = all_q_head[j]\n",
    "        q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "        q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "        v_head = all_v_head[j]\n",
    "        v_u, v_s, v_vt = torch.linalg.svd(v_head)\n",
    "        v_loraA_head = v_u[:, :1] @ torch.diag(v_s[:1])\n",
    "        q_loraA_head_list.append(q_loraA_head)\n",
    "        v_loraA_head_list.append(v_loraA_head)\n",
    "    print(\n",
    "        f\"Complete Merge Head! len of q_loraA_head_list: {len(q_loraA_head_list)},len of v_loraA_head_list: {len(v_loraA_head_list)}\"\n",
    "    )\n",
    "    q_loraA = torch.cat(q_loraA_head_list, dim=1)\n",
    "    v_loraA = torch.cat(v_loraA_head_list, dim=1)\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        i\n",
    "    ].attention.self.query_proj.lora_A.default.weight.data = q_loraA.T.contiguous()\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        i\n",
    "    ].attention.self.value_proj.lora_A.default.weight.data = v_loraA.T.contiguous()\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        i\n",
    "    ].attention.self.query_proj.lora_B.default.weight.data = torch.zeros_like(\n",
    "        model.base_model.model.deberta.encoder.layer[i].attention.self.query_proj.lora_B.default.weight.data\n",
    "    )\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        i\n",
    "    ].attention.self.value_proj.lora_B.default.weight.data = torch.zeros_like(\n",
    "        model.base_model.model.deberta.encoder.layer[i].attention.self.value_proj.lora_B.default.weight.data\n",
    "    )\n",
    "    print(f\"Complete! Norm of q_loraA: {torch.norm(q_loraA)}, Norm of v_loraA: {torch.norm(v_loraA)}\")\n",
    "    #print(f\"Complete init LoraA! layer: {i}, q_loraA: {q_loraA.shape}, v_loraA: {v_loraA.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 3.3308427333831787 v_loraA_head : 1.5449029207229614\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5812019109725952 v_loraA_head : 0.5815222263336182\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 2.817272663116455 v_loraA_head : 1.5823298692703247\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5801612138748169 v_loraA_head : 0.5802536010742188\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 2.6986498832702637 v_loraA_head : 1.5997166633605957\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5838015079498291 v_loraA_head : 0.5741603970527649\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 3.2101049423217773 v_loraA_head : 1.5608558654785156\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.577570378780365 v_loraA_head : 0.5810839533805847\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 2.801222562789917 v_loraA_head : 1.5585856437683105\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5670623779296875 v_loraA_head : 0.582253634929657\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 2.772526502609253 v_loraA_head : 1.5920060873031616\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.5833855271339417 v_loraA_head : 0.5798369646072388\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 2.8253629207611084 v_loraA_head : 1.6143772602081299\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5785372853279114 v_loraA_head : 0.5803302526473999\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 2.656848907470703 v_loraA_head : 1.5693186521530151\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5709751844406128 v_loraA_head : 0.5808721780776978\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 2.7803072929382324 v_loraA_head : 1.5483015775680542\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5754052996635437 v_loraA_head : 0.5705822110176086\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 2.687443733215332 v_loraA_head : 1.5544096231460571\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5738977789878845 v_loraA_head : 0.5857831835746765\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 2.7353806495666504 v_loraA_head : 1.6225032806396484\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5909292101860046 v_loraA_head : 0.5658062100410461\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 2.7826991081237793 v_loraA_head : 1.6244324445724487\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5847077965736389 v_loraA_head : 0.5771334171295166\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 3.5679898262023926 v_loraA_head : 1.6400225162506104\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5691646337509155 v_loraA_head : 0.5808940529823303\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 2.775360107421875 v_loraA_head : 1.6220325231552124\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5956133008003235 v_loraA_head : 0.5788415670394897\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 2.738048553466797 v_loraA_head : 1.5448638200759888\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5766405463218689 v_loraA_head : 0.5680311918258667\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 2.753819227218628 v_loraA_head : 1.5793147087097168\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5741186738014221 v_loraA_head : 0.5792297124862671\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 2.9088656902313232 v_loraA_head : 1.5647175312042236\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5800416469573975 v_loraA_head : 0.573779821395874\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 2.7350974082946777 v_loraA_head : 1.6816041469573975\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5793200731277466 v_loraA_head : 0.5833224654197693\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 2.938549041748047 v_loraA_head : 1.600702166557312\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5845130681991577 v_loraA_head : 0.5787774324417114\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 2.7840583324432373 v_loraA_head : 1.6077877283096313\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5808286666870117 v_loraA_head : 0.5741638541221619\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 2.6948115825653076 v_loraA_head : 1.5855158567428589\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5827451348304749 v_loraA_head : 0.57257080078125\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 2.709029197692871 v_loraA_head : 1.6001962423324585\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5733863115310669 v_loraA_head : 0.5775147080421448\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 2.6923019886016846 v_loraA_head : 1.5739718675613403\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5665082335472107 v_loraA_head : 0.5644036531448364\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 2.7195448875427246 v_loraA_head : 1.5536954402923584\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5810524225234985 v_loraA_head : 0.568573534488678\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 0, q_loraA: 2.835801601409912, v_loraA: 2.8251583576202393\n",
      "1th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.7169445753097534 v_loraA_head : 1.3428406715393066\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5828343033790588 v_loraA_head : 0.5810438394546509\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7427977323532104 v_loraA_head : 1.3274272680282593\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5848501920700073 v_loraA_head : 0.5756689310073853\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.6573524475097656 v_loraA_head : 1.3193453550338745\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5695322751998901 v_loraA_head : 0.5723848342895508\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.6935691833496094 v_loraA_head : 1.3269315958023071\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5761529207229614 v_loraA_head : 0.5784251689910889\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7721688747406006 v_loraA_head : 1.3376086950302124\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5813745856285095 v_loraA_head : 0.5777136087417603\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.7189191579818726 v_loraA_head : 1.373767614364624\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.5848847031593323 v_loraA_head : 0.5796998143196106\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.7010504007339478 v_loraA_head : 1.360376238822937\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5745324492454529 v_loraA_head : 0.5721817016601562\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.7095527648925781 v_loraA_head : 1.3454169034957886\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5788136720657349 v_loraA_head : 0.5729442834854126\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.801688313484192 v_loraA_head : 1.318018913269043\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.572641134262085 v_loraA_head : 0.5754226446151733\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.7360931634902954 v_loraA_head : 1.3332254886627197\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5861284732818604 v_loraA_head : 0.5791400074958801\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.7079733610153198 v_loraA_head : 1.366186261177063\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5898487567901611 v_loraA_head : 0.5853568315505981\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.7021900415420532 v_loraA_head : 1.3792675733566284\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5832564830780029 v_loraA_head : 0.5828955173492432\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.7148622274398804 v_loraA_head : 1.3602004051208496\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5773555040359497 v_loraA_head : 0.5785545110702515\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.6744948625564575 v_loraA_head : 1.337390661239624\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5763383507728577 v_loraA_head : 0.5759817361831665\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.696730136871338 v_loraA_head : 1.3276437520980835\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5822610855102539 v_loraA_head : 0.5789661407470703\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7443580627441406 v_loraA_head : 1.3464514017105103\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5702734589576721 v_loraA_head : 0.5680945515632629\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.8129081726074219 v_loraA_head : 1.3622393608093262\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5634990334510803 v_loraA_head : 0.5757312774658203\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.7347441911697388 v_loraA_head : 1.358751654624939\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5765907168388367 v_loraA_head : 0.5742921233177185\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.6948301792144775 v_loraA_head : 1.3143984079360962\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5759692192077637 v_loraA_head : 0.5766315460205078\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.7157310247421265 v_loraA_head : 1.3370640277862549\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5705657005310059 v_loraA_head : 0.5712077617645264\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.7348774671554565 v_loraA_head : 1.3522711992263794\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5753619074821472 v_loraA_head : 0.5717718005180359\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.7389864921569824 v_loraA_head : 1.3466055393218994\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5809103846549988 v_loraA_head : 0.5739702582359314\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.656597375869751 v_loraA_head : 1.3399100303649902\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5847915410995483 v_loraA_head : 0.5886726379394531\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.6768368482589722 v_loraA_head : 1.3579477071762085\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5725995302200317 v_loraA_head : 0.5762807726860046\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 1, q_loraA: 2.8316428661346436, v_loraA: 2.8257851600646973\n",
      "2th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.7454280853271484 v_loraA_head : 1.528914213180542\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5775073170661926 v_loraA_head : 0.5814418196678162\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7140141725540161 v_loraA_head : 1.483948826789856\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5696613192558289 v_loraA_head : 0.5716218948364258\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.7137342691421509 v_loraA_head : 1.4848538637161255\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5752866268157959 v_loraA_head : 0.5797261595726013\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.7183338403701782 v_loraA_head : 1.5166345834732056\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5907351970672607 v_loraA_head : 0.5645803809165955\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7720755338668823 v_loraA_head : 1.5367693901062012\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5832977890968323 v_loraA_head : 0.5758516788482666\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.7331756353378296 v_loraA_head : 1.5409191846847534\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.5819584727287292 v_loraA_head : 0.5672339200973511\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.7228453159332275 v_loraA_head : 1.5536096096038818\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.584385871887207 v_loraA_head : 0.5693665146827698\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.6978174448013306 v_loraA_head : 1.4908390045166016\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5732908844947815 v_loraA_head : 0.5939391255378723\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.7338989973068237 v_loraA_head : 1.4730346202850342\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5807343125343323 v_loraA_head : 0.5770887732505798\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.6836832761764526 v_loraA_head : 1.4957221746444702\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5729514956474304 v_loraA_head : 0.5891890525817871\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.7117414474487305 v_loraA_head : 1.5700536966323853\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5786511898040771 v_loraA_head : 0.5766858458518982\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.7033370733261108 v_loraA_head : 1.5546268224716187\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5862091183662415 v_loraA_head : 0.574221134185791\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.7270888090133667 v_loraA_head : 1.5235072374343872\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5804591178894043 v_loraA_head : 0.5728468298912048\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.6973047256469727 v_loraA_head : 1.5154715776443481\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5795169472694397 v_loraA_head : 0.5787188410758972\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.712948203086853 v_loraA_head : 1.5086207389831543\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5730347037315369 v_loraA_head : 0.5695070028305054\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7355022430419922 v_loraA_head : 1.537758469581604\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5739253163337708 v_loraA_head : 0.5750652551651001\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.7091708183288574 v_loraA_head : 1.5566868782043457\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5679203867912292 v_loraA_head : 0.5682379007339478\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.721001386642456 v_loraA_head : 1.5298117399215698\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5841483473777771 v_loraA_head : 0.5807942748069763\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.71915602684021 v_loraA_head : 1.5111583471298218\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5799562335014343 v_loraA_head : 0.5704243183135986\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.7359468936920166 v_loraA_head : 1.5315378904342651\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5788750648498535 v_loraA_head : 0.5732380151748657\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.7101246118545532 v_loraA_head : 1.5048695802688599\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5730552077293396 v_loraA_head : 0.5865750312805176\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.7285586595535278 v_loraA_head : 1.5665440559387207\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5648287534713745 v_loraA_head : 0.5813562273979187\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.7304259538650513 v_loraA_head : 1.5280755758285522\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5876014828681946 v_loraA_head : 0.5647954344749451\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.7156305313110352 v_loraA_head : 1.525364875793457\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5747510194778442 v_loraA_head : 0.5861443877220154\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 2, q_loraA: 2.8319265842437744, v_loraA: 2.8230032920837402\n",
      "3th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.7453182935714722 v_loraA_head : 1.6256500482559204\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5687105655670166 v_loraA_head : 0.5695631504058838\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7250032424926758 v_loraA_head : 1.6090222597122192\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.57156902551651 v_loraA_head : 0.5732185244560242\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.6747843027114868 v_loraA_head : 1.59685218334198\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5613985657691956 v_loraA_head : 0.565173327922821\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.7079830169677734 v_loraA_head : 1.5980126857757568\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5793179273605347 v_loraA_head : 0.5775858163833618\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7255496978759766 v_loraA_head : 1.6024378538131714\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5768800973892212 v_loraA_head : 0.5867896676063538\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.749327301979065 v_loraA_head : 1.6459224224090576\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.570411741733551 v_loraA_head : 0.5872594714164734\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.7097374200820923 v_loraA_head : 1.643916368484497\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5802229642868042 v_loraA_head : 0.5818326473236084\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.7025901079177856 v_loraA_head : 1.6104260683059692\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5713486075401306 v_loraA_head : 0.5968093872070312\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.7078007459640503 v_loraA_head : 1.5972565412521362\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5737295150756836 v_loraA_head : 0.5702764987945557\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.6797629594802856 v_loraA_head : 1.602920651435852\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5806898474693298 v_loraA_head : 0.5686138272285461\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.7402174472808838 v_loraA_head : 1.635825276374817\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5711749196052551 v_loraA_head : 0.5647758841514587\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.7119176387786865 v_loraA_head : 1.6644198894500732\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5707395076751709 v_loraA_head : 0.5758630037307739\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.7098135948181152 v_loraA_head : 1.5927815437316895\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5772624015808105 v_loraA_head : 0.5740931630134583\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.7277085781097412 v_loraA_head : 1.6185487508773804\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.57896888256073 v_loraA_head : 0.585216760635376\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.6829472780227661 v_loraA_head : 1.5961661338806152\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5815876126289368 v_loraA_head : 0.5816690921783447\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7374763488769531 v_loraA_head : 1.6403416395187378\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5723395347595215 v_loraA_head : 0.5723878145217896\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.7336384057998657 v_loraA_head : 1.652463674545288\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5776880979537964 v_loraA_head : 0.5735459923744202\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.7442882061004639 v_loraA_head : 1.643332839012146\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5841478705406189 v_loraA_head : 0.5833951830863953\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.7051551342010498 v_loraA_head : 1.584708333015442\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5684381723403931 v_loraA_head : 0.5773610472679138\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.7156178951263428 v_loraA_head : 1.6127479076385498\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5740721821784973 v_loraA_head : 0.5889467597007751\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.6956249475479126 v_loraA_head : 1.604001760482788\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5722180008888245 v_loraA_head : 0.577652096748352\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.7567909955978394 v_loraA_head : 1.6307967901229858\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5813974142074585 v_loraA_head : 0.5810295939445496\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.7475776672363281 v_loraA_head : 1.6368117332458496\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5713061094284058 v_loraA_head : 0.5746976137161255\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.702858328819275 v_loraA_head : 1.5999467372894287\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5709725022315979 v_loraA_head : 0.5740844011306763\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 3, q_loraA: 2.8142940998077393, v_loraA: 2.8297886848449707\n",
      "4th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.763051986694336 v_loraA_head : 1.506270170211792\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5712127089500427 v_loraA_head : 0.5724522471427917\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.8125593662261963 v_loraA_head : 1.4977412223815918\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.569574236869812 v_loraA_head : 0.5750604867935181\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.7532955408096313 v_loraA_head : 1.452851414680481\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5806747078895569 v_loraA_head : 0.5707730054855347\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.7732679843902588 v_loraA_head : 1.491568684577942\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5752615332603455 v_loraA_head : 0.575515627861023\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7948660850524902 v_loraA_head : 1.502766489982605\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5763192772865295 v_loraA_head : 0.5848268866539001\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.8185231685638428 v_loraA_head : 1.5298717021942139\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.5743805766105652 v_loraA_head : 0.5673023462295532\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.768027663230896 v_loraA_head : 1.5101264715194702\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5753960609436035 v_loraA_head : 0.585817813873291\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.7217934131622314 v_loraA_head : 1.487065315246582\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.575785219669342 v_loraA_head : 0.5820131897926331\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.7631136178970337 v_loraA_head : 1.4416145086288452\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5701155662536621 v_loraA_head : 0.5777046084403992\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.750695824623108 v_loraA_head : 1.4957091808319092\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5802381038665771 v_loraA_head : 0.5844952464103699\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.7928615808486938 v_loraA_head : 1.5057569742202759\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5769827365875244 v_loraA_head : 0.5694218277931213\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.7642841339111328 v_loraA_head : 1.5093134641647339\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5741407871246338 v_loraA_head : 0.5661481022834778\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.8515129089355469 v_loraA_head : 1.4565287828445435\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5820889472961426 v_loraA_head : 0.5724229216575623\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.7425134181976318 v_loraA_head : 1.4783231019973755\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5632217526435852 v_loraA_head : 0.5675976872444153\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.721272349357605 v_loraA_head : 1.4568670988082886\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.580244243144989 v_loraA_head : 0.5848180651664734\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7271337509155273 v_loraA_head : 1.5090229511260986\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5821274518966675 v_loraA_head : 0.5776277184486389\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.738578200340271 v_loraA_head : 1.5121499300003052\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.574944257736206 v_loraA_head : 0.5840967297554016\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.7669278383255005 v_loraA_head : 1.5118604898452759\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5771072506904602 v_loraA_head : 0.5749796032905579\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.739387035369873 v_loraA_head : 1.4616490602493286\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5823973417282104 v_loraA_head : 0.5760628581047058\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.743469476699829 v_loraA_head : 1.473920226097107\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5827732682228088 v_loraA_head : 0.5792523622512817\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.7440292835235596 v_loraA_head : 1.4586031436920166\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5884172320365906 v_loraA_head : 0.5702333450317383\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.779660940170288 v_loraA_head : 1.508088231086731\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5796141624450684 v_loraA_head : 0.5858496427536011\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.7359718084335327 v_loraA_head : 1.4924649000167847\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5767920613288879 v_loraA_head : 0.5781068801879883\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.769544005393982 v_loraA_head : 1.5128945112228394\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5741624236106873 v_loraA_head : 0.576252818107605\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 4, q_loraA: 2.826002359390259, v_loraA: 2.825000524520874\n",
      "5th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.767831563949585 v_loraA_head : 1.5775859355926514\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5776825547218323 v_loraA_head : 0.579875648021698\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7798314094543457 v_loraA_head : 1.5523037910461426\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5774505138397217 v_loraA_head : 0.5708103775978088\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.732061505317688 v_loraA_head : 1.5260165929794312\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5733082890510559 v_loraA_head : 0.5751645565032959\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.7341707944869995 v_loraA_head : 1.5557690858840942\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5758843421936035 v_loraA_head : 0.5766215324401855\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7676942348480225 v_loraA_head : 1.5559903383255005\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5875106453895569 v_loraA_head : 0.5792195796966553\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.745546817779541 v_loraA_head : 1.5940946340560913\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.5702583193778992 v_loraA_head : 0.5803766846656799\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.7620489597320557 v_loraA_head : 1.5780755281448364\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5769094228744507 v_loraA_head : 0.5702916979789734\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.7296167612075806 v_loraA_head : 1.5400158166885376\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5898820757865906 v_loraA_head : 0.5600284934043884\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.7376060485839844 v_loraA_head : 1.5789786577224731\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5730164051055908 v_loraA_head : 0.5675868391990662\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.732212781906128 v_loraA_head : 1.534303069114685\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5711712837219238 v_loraA_head : 0.5739620923995972\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.7657307386398315 v_loraA_head : 1.5944818258285522\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5731585025787354 v_loraA_head : 0.5820974111557007\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.738065242767334 v_loraA_head : 1.554082989692688\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5652921199798584 v_loraA_head : 0.5880919694900513\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.973758339881897 v_loraA_head : 1.503627896308899\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5831146836280823 v_loraA_head : 0.5653823018074036\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.732452154159546 v_loraA_head : 1.532454490661621\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5814946889877319 v_loraA_head : 0.5775938034057617\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.7364717721939087 v_loraA_head : 1.5440740585327148\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5839776992797852 v_loraA_head : 0.5745177865028381\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7284760475158691 v_loraA_head : 1.5942769050598145\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5761627554893494 v_loraA_head : 0.5792778134346008\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.7531442642211914 v_loraA_head : 1.5732232332229614\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5762763023376465 v_loraA_head : 0.5723435282707214\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.7474161386489868 v_loraA_head : 1.5839482545852661\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.573725700378418 v_loraA_head : 0.5792600512504578\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.7381021976470947 v_loraA_head : 1.5426496267318726\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5803506970405579 v_loraA_head : 0.593016505241394\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.7349196672439575 v_loraA_head : 1.5571529865264893\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5645158290863037 v_loraA_head : 0.5791285037994385\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.7314660549163818 v_loraA_head : 1.5100809335708618\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5731750130653381 v_loraA_head : 0.5843714475631714\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.7507026195526123 v_loraA_head : 1.5726428031921387\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.583957850933075 v_loraA_head : 0.5717360973358154\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.715286135673523 v_loraA_head : 1.5662509202957153\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5781993865966797 v_loraA_head : 0.5761720538139343\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.7383437156677246 v_loraA_head : 1.565108299255371\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5890780091285706 v_loraA_head : 0.5676167011260986\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 5, q_loraA: 2.8284358978271484, v_loraA: 2.82214093208313\n",
      "6th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.8079330921173096 v_loraA_head : 1.5399855375289917\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.575233519077301 v_loraA_head : 0.5705366134643555\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.9059498310089111 v_loraA_head : 1.5102801322937012\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5693178772926331 v_loraA_head : 0.5672532320022583\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.7864563465118408 v_loraA_head : 1.4791252613067627\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5771880745887756 v_loraA_head : 0.5736927390098572\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.754314661026001 v_loraA_head : 1.460734248161316\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5651692748069763 v_loraA_head : 0.574293851852417\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7781522274017334 v_loraA_head : 1.504127860069275\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.566545307636261 v_loraA_head : 0.5703170299530029\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.7953362464904785 v_loraA_head : 1.4971377849578857\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.582033634185791 v_loraA_head : 0.5837438702583313\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.7846131324768066 v_loraA_head : 1.5082255601882935\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5809078216552734 v_loraA_head : 0.5823706388473511\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.8005722761154175 v_loraA_head : 1.4734441041946411\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5832089781761169 v_loraA_head : 0.5735252499580383\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.776004672050476 v_loraA_head : 1.500622272491455\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5732851028442383 v_loraA_head : 0.5845799446105957\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.8100959062576294 v_loraA_head : 1.4729301929473877\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5772538781166077 v_loraA_head : 0.5950511693954468\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.7818490266799927 v_loraA_head : 1.5186392068862915\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5748739838600159 v_loraA_head : 0.5825523734092712\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.7912129163742065 v_loraA_head : 1.49278724193573\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5661267638206482 v_loraA_head : 0.5663694143295288\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.8720341920852661 v_loraA_head : 1.4584439992904663\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5692611932754517 v_loraA_head : 0.5681242346763611\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.7672330141067505 v_loraA_head : 1.4782923460006714\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.578336238861084 v_loraA_head : 0.5868110656738281\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.7721076011657715 v_loraA_head : 1.4651081562042236\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5794472098350525 v_loraA_head : 0.574795126914978\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7503091096878052 v_loraA_head : 1.5073853731155396\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.57004714012146 v_loraA_head : 0.5698769688606262\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.7672120332717896 v_loraA_head : 1.4993808269500732\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5842522978782654 v_loraA_head : 0.5783172249794006\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.836746096611023 v_loraA_head : 1.541735291481018\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5809211730957031 v_loraA_head : 0.5725143551826477\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.7537002563476562 v_loraA_head : 1.4556604623794556\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5855515599250793 v_loraA_head : 0.575860321521759\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.758739709854126 v_loraA_head : 1.5045084953308105\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.586365282535553 v_loraA_head : 0.5796604156494141\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.7895545959472656 v_loraA_head : 1.4470361471176147\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5888448357582092 v_loraA_head : 0.5816062092781067\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.813163161277771 v_loraA_head : 1.5164573192596436\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5789103507995605 v_loraA_head : 0.5805792212486267\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.7627251148223877 v_loraA_head : 1.5014281272888184\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.576223611831665 v_loraA_head : 0.5802515149116516\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.7899569272994995 v_loraA_head : 1.4905030727386475\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5756995677947998 v_loraA_head : 0.5741543173789978\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 6, q_loraA: 2.8262791633605957, v_loraA: 2.8266730308532715\n",
      "7th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.849898099899292 v_loraA_head : 1.5603604316711426\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5731213092803955 v_loraA_head : 0.56490558385849\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7917406558990479 v_loraA_head : 1.5413076877593994\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5827061533927917 v_loraA_head : 0.5764783620834351\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 1.766709804534912 v_loraA_head : 1.5394346714019775\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.587144672870636 v_loraA_head : 0.5865546464920044\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.779369592666626 v_loraA_head : 1.5424842834472656\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5825218558311462 v_loraA_head : 0.591609537601471\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 2.0227513313293457 v_loraA_head : 1.5371135473251343\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.576326847076416 v_loraA_head : 0.5742169618606567\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.8579856157302856 v_loraA_head : 1.570770502090454\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.572136402130127 v_loraA_head : 0.5866060853004456\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.781020998954773 v_loraA_head : 1.5535030364990234\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5595041513442993 v_loraA_head : 0.5729943513870239\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.7691959142684937 v_loraA_head : 1.5220352411270142\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5746984481811523 v_loraA_head : 0.5880985260009766\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.7894701957702637 v_loraA_head : 1.5096968412399292\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5798907279968262 v_loraA_head : 0.5692650079727173\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.7779126167297363 v_loraA_head : 1.5291306972503662\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.5854921936988831 v_loraA_head : 0.5816267132759094\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.8111215829849243 v_loraA_head : 1.5706878900527954\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5749635100364685 v_loraA_head : 0.5763530135154724\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.7935799360275269 v_loraA_head : 1.52229642868042\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5691329836845398 v_loraA_head : 0.575493335723877\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.7535059452056885 v_loraA_head : 1.51004958152771\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.5708216428756714 v_loraA_head : 0.5680679678916931\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.7335829734802246 v_loraA_head : 1.522545576095581\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5772517323493958 v_loraA_head : 0.582011342048645\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.7845114469528198 v_loraA_head : 1.5149928331375122\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5820931196212769 v_loraA_head : 0.5790740251541138\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.7764718532562256 v_loraA_head : 1.540692925453186\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5867226719856262 v_loraA_head : 0.5928463339805603\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.7918119430541992 v_loraA_head : 1.5309906005859375\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5856499671936035 v_loraA_head : 0.5839343667030334\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.7846537828445435 v_loraA_head : 1.5805211067199707\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5783363580703735 v_loraA_head : 0.5754632949829102\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.8100526332855225 v_loraA_head : 1.4762907028198242\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.593507707118988 v_loraA_head : 0.5871713757514954\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.7931774854660034 v_loraA_head : 1.5509207248687744\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5788322687149048 v_loraA_head : 0.5814616680145264\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.8102636337280273 v_loraA_head : 1.4985159635543823\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5803185701370239 v_loraA_head : 0.5739067196846008\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.8003053665161133 v_loraA_head : 1.5751450061798096\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.5675756335258484 v_loraA_head : 0.5724772810935974\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.7698416709899902 v_loraA_head : 1.5487562417984009\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.5805662870407104 v_loraA_head : 0.5772156119346619\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.782823085784912 v_loraA_head : 1.5562907457351685\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5590838193893433 v_loraA_head : 0.5765475630760193\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 7, q_loraA: 2.8291172981262207, v_loraA: 2.8363988399505615\n",
      "8th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.8901865482330322 v_loraA_head : 1.6190130710601807\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5828338861465454 v_loraA_head : 0.5745853781700134\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7859492301940918 v_loraA_head : 1.585720419883728\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5745067000389099 v_loraA_head : 0.5731796622276306\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "2th head norm Before Scaling \n",
      " q_loraA_head : 2.06343936920166 v_loraA_head : 1.5811196565628052\n",
      "2th head norm After Scaling \n",
      " q_loraA_head : 0.5791006088256836 v_loraA_head : 0.583298921585083\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "3th head norm Before Scaling \n",
      " q_loraA_head : 1.7848504781723022 v_loraA_head : 1.571279764175415\n",
      "3th head norm After Scaling \n",
      " q_loraA_head : 0.5704934000968933 v_loraA_head : 0.5725594162940979\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "4th head norm Before Scaling \n",
      " q_loraA_head : 1.7523194551467896 v_loraA_head : 1.6041709184646606\n",
      "4th head norm After Scaling \n",
      " q_loraA_head : 0.5901382565498352 v_loraA_head : 0.5774272680282593\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "5th head norm Before Scaling \n",
      " q_loraA_head : 1.7819772958755493 v_loraA_head : 1.6190985441207886\n",
      "5th head norm After Scaling \n",
      " q_loraA_head : 0.5780037641525269 v_loraA_head : 0.5879780054092407\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "6th head norm Before Scaling \n",
      " q_loraA_head : 1.813329815864563 v_loraA_head : 1.5865237712860107\n",
      "6th head norm After Scaling \n",
      " q_loraA_head : 0.5765899419784546 v_loraA_head : 0.5818126201629639\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "7th head norm Before Scaling \n",
      " q_loraA_head : 1.79649817943573 v_loraA_head : 1.5932730436325073\n",
      "7th head norm After Scaling \n",
      " q_loraA_head : 0.5896158218383789 v_loraA_head : 0.577678382396698\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "8th head norm Before Scaling \n",
      " q_loraA_head : 1.923586368560791 v_loraA_head : 1.591588020324707\n",
      "8th head norm After Scaling \n",
      " q_loraA_head : 0.5780074000358582 v_loraA_head : 0.5838586091995239\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "9th head norm Before Scaling \n",
      " q_loraA_head : 1.8143117427825928 v_loraA_head : 1.5676723718643188\n",
      "9th head norm After Scaling \n",
      " q_loraA_head : 0.574523389339447 v_loraA_head : 0.5746673345565796\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "10th head norm Before Scaling \n",
      " q_loraA_head : 1.8098572492599487 v_loraA_head : 1.6344013214111328\n",
      "10th head norm After Scaling \n",
      " q_loraA_head : 0.5658954977989197 v_loraA_head : 0.5683095455169678\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "11th head norm Before Scaling \n",
      " q_loraA_head : 1.8114237785339355 v_loraA_head : 1.579306721687317\n",
      "11th head norm After Scaling \n",
      " q_loraA_head : 0.5937678217887878 v_loraA_head : 0.5715122818946838\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "12th head norm Before Scaling \n",
      " q_loraA_head : 1.7827879190444946 v_loraA_head : 1.5753296613693237\n",
      "12th head norm After Scaling \n",
      " q_loraA_head : 0.585884153842926 v_loraA_head : 0.5736616849899292\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "13th head norm Before Scaling \n",
      " q_loraA_head : 1.7728357315063477 v_loraA_head : 1.5893057584762573\n",
      "13th head norm After Scaling \n",
      " q_loraA_head : 0.5807315707206726 v_loraA_head : 0.5646921396255493\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "14th head norm Before Scaling \n",
      " q_loraA_head : 1.8014272451400757 v_loraA_head : 1.5635930299758911\n",
      "14th head norm After Scaling \n",
      " q_loraA_head : 0.5768747329711914 v_loraA_head : 0.5807358026504517\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "15th head norm Before Scaling \n",
      " q_loraA_head : 1.8045179843902588 v_loraA_head : 1.6012300252914429\n",
      "15th head norm After Scaling \n",
      " q_loraA_head : 0.5707871913909912 v_loraA_head : 0.5763216614723206\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "16th head norm Before Scaling \n",
      " q_loraA_head : 1.7740381956100464 v_loraA_head : 1.624569058418274\n",
      "16th head norm After Scaling \n",
      " q_loraA_head : 0.5815744996070862 v_loraA_head : 0.5732480883598328\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "17th head norm Before Scaling \n",
      " q_loraA_head : 1.8262826204299927 v_loraA_head : 1.6229088306427002\n",
      "17th head norm After Scaling \n",
      " q_loraA_head : 0.5779114961624146 v_loraA_head : 0.5728752613067627\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "18th head norm Before Scaling \n",
      " q_loraA_head : 1.8180822134017944 v_loraA_head : 1.553934931755066\n",
      "18th head norm After Scaling \n",
      " q_loraA_head : 0.5718247294425964 v_loraA_head : 0.5698984265327454\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "19th head norm Before Scaling \n",
      " q_loraA_head : 1.80585777759552 v_loraA_head : 1.5708351135253906\n",
      "19th head norm After Scaling \n",
      " q_loraA_head : 0.5672471523284912 v_loraA_head : 0.5814617872238159\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "20th head norm Before Scaling \n",
      " q_loraA_head : 1.7836345434188843 v_loraA_head : 1.547348976135254\n",
      "20th head norm After Scaling \n",
      " q_loraA_head : 0.5764868259429932 v_loraA_head : 0.5844849348068237\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "21th head norm Before Scaling \n",
      " q_loraA_head : 1.8338913917541504 v_loraA_head : 1.607978343963623\n",
      "21th head norm After Scaling \n",
      " q_loraA_head : 0.579158365726471 v_loraA_head : 0.5806792974472046\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "22th head norm Before Scaling \n",
      " q_loraA_head : 1.7880451679229736 v_loraA_head : 1.6122989654541016\n",
      "22th head norm After Scaling \n",
      " q_loraA_head : 0.583771824836731 v_loraA_head : 0.5768778920173645\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "23th head norm Before Scaling \n",
      " q_loraA_head : 1.8103538751602173 v_loraA_head : 1.5918457508087158\n",
      "23th head norm After Scaling \n",
      " q_loraA_head : 0.5848016738891602 v_loraA_head : 0.5806430578231812\n",
      "--------------------------------------------------\n",
      "Complete init LoraA! layer: 8, q_loraA: 2.8355937004089355, v_loraA: 2.8257083892822266\n",
      "9th Layer \n",
      "--------------------------------------------------\n",
      "0th head norm Before Scaling \n",
      " q_loraA_head : 1.8298372030258179 v_loraA_head : 1.5330572128295898\n",
      "0th head norm After Scaling \n",
      " q_loraA_head : 0.5768884420394897 v_loraA_head : 0.5899977684020996\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "1th head norm Before Scaling \n",
      " q_loraA_head : 1.7911548614501953 v_loraA_head : 1.5269657373428345\n",
      "1th head norm After Scaling \n",
      " q_loraA_head : 0.5752916932106018 v_loraA_head : 0.5784064531326294\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m q_loraA_head \u001b[39m=\u001b[39m q_u[:, :\u001b[39m1\u001b[39m] \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mdiag(q_s[:\u001b[39m1\u001b[39m])\n\u001b[1;32m     25\u001b[0m v_head \u001b[39m=\u001b[39m all_v_head[j]\n\u001b[0;32m---> 26\u001b[0m v_u, v_s, v_vt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msvd(v_head)\n\u001b[1;32m     27\u001b[0m v_loraA_head \u001b[39m=\u001b[39m v_u[:, :\u001b[39m1\u001b[39m] \u001b[39m@\u001b[39m torch\u001b[39m.\u001b[39mdiag(v_s[:\u001b[39m1\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m50\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "len_of_layers = len(model.base_model.model.deberta.encoder.layer)\n",
    "for layer_idx in range(len_of_layers):\n",
    "    print(f\"{layer_idx}th Layer \")\n",
    "    q_loraA_head_list = []\n",
    "    v_loraA_head_list = []\n",
    "    q_weight = model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.weight.T\n",
    "    v_weight = model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.weight.T\n",
    "\n",
    "    q_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.query_proj.lora_A.default.weight.T\n",
    "    v_og_lora_A = model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.value_proj.lora_A.default.weight.T\n",
    "\n",
    "    all_q_head = copy.deepcopy(q_weight)\n",
    "    all_q_head = all_q_head.reshape(approx_rank, 1536, 1536 // approx_rank)\n",
    "    all_v_head = copy.deepcopy(v_weight)\n",
    "    all_v_head = all_v_head.reshape(approx_rank, 1536, 1536 // approx_rank)\n",
    "\n",
    "    for j in range(approx_rank):\n",
    "        q_head = all_q_head[j]\n",
    "        q_u, q_s, q_vt = torch.linalg.svd(q_head)\n",
    "        q_loraA_head = q_u[:, :1] @ torch.diag(q_s[:1])\n",
    "        v_head = all_v_head[j]\n",
    "        v_u, v_s, v_vt = torch.linalg.svd(v_head)\n",
    "        v_loraA_head = v_u[:, :1] @ torch.diag(v_s[:1])\n",
    "        print(\"-\" * 50)\n",
    "        print(\n",
    "            f\"{j}th head norm Before Scaling \\n q_loraA_head : {torch.norm(q_loraA_head)} v_loraA_head : {torch.norm(v_loraA_head)}\"\n",
    "        )\n",
    "        q_og_lora_A_icol_norm = torch.norm(q_og_lora_A[:, j : j + 1])\n",
    "        v_og_lora_A_icol_norm = torch.norm(v_og_lora_A[:, j : j + 1])\n",
    "\n",
    "        q_scale = q_og_lora_A_icol_norm / torch.norm(q_loraA_head)\n",
    "        v_scale = v_og_lora_A_icol_norm / torch.norm(v_loraA_head)\n",
    "\n",
    "        q_loraA_head = q_loraA_head * q_scale\n",
    "        v_loraA_head = v_loraA_head * v_scale\n",
    "        print(\n",
    "            f\"{j}th head norm After Scaling \\n q_loraA_head : {torch.norm(q_loraA_head)} v_loraA_head : {torch.norm(v_loraA_head)}\"\n",
    "        )\n",
    "        print(\"-\" * 50)\n",
    "        q_loraA_head_list.append(q_loraA_head)\n",
    "        v_loraA_head_list.append(v_loraA_head)\n",
    "    q_loraA = torch.cat(q_loraA_head_list, dim=1)\n",
    "    v_loraA = torch.cat(v_loraA_head_list, dim=1)\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.query_proj.lora_A.default.weight.data = q_loraA.T.contiguous()\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.value_proj.lora_A.default.weight.data = v_loraA.T.contiguous()\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.query_proj.lora_B.default.weight.data = torch.zeros_like(\n",
    "        model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.lora_B.default.weight.data\n",
    "    )\n",
    "    model.base_model.model.deberta.encoder.layer[\n",
    "        layer_idx\n",
    "    ].attention.self.value_proj.lora_B.default.weight.data = torch.zeros_like(\n",
    "        model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.lora_B.default.weight.data\n",
    "    )\n",
    "    print(\n",
    "        f\"Complete init LoraA! layer: {layer_idx}, q_loraA: {torch.norm(model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.query_proj.lora_A.default.weight)}, v_loraA: {torch.norm(model.base_model.model.deberta.encoder.layer[layer_idx].attention.self.value_proj.lora_A.default.weight)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8770, grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A.default.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=1536, out_features=4, bias=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.deberta.encoder.layer[0].attention.self.query_proj.lora_A.default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qlora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
